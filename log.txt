================================================================================
[2025-05-09 15:25:42] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 15:25:42] [INFO] K-Fold 분할 시작
================================================================================
[2025-05-09 15:25:42] [INFO] 데이터 로드 중...
[2025-05-09 15:25:42] [INFO] 총 3000개 샘플 로드 완료
[2025-05-09 15:25:42] [INFO] 데이터 검증 결과:
[2025-05-09 15:25:42] [INFO] - 총 행 수: 3000
[2025-05-09 15:25:42] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 15:25:42] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 15:25:42] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 15:25:42] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 15:25:42] [INFO] KFold 비활성화 상태 - 전체 데이터 저장
================================================================================
[2025-05-09 15:25:42] [INFO] train_folds.csv 저장 완료: data\train_folds.csv
================================================================================
================================================================================
[2025-05-09 15:26:01] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 15:26:01] [INFO] TF-IDF + LightGBM 학습 시작
================================================================================
[2025-05-09 15:26:01] [INFO] 데이터 로드 중...
[2025-05-09 15:26:01] [INFO] 데이터 검증 결과:
[2025-05-09 15:26:01] [INFO] - 총 행 수: 3000
[2025-05-09 15:26:01] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 15:26:01] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 15:26:01] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 15:26:01] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 15:26:01] [INFO] 데이터 분할 중...
[2025-05-09 15:26:01] [INFO] 학습 샘플: 2400개, 검증 샘플: 600개
[2025-05-09 15:26:01] [INFO] TF-IDF 피처 추출 중 (ngrams=[1, 5], max_features=150000)...
[2025-05-09 15:26:08] [INFO] TF-IDF 피처 추출 완료: 150000 피처 (소요시간: 6초)
[2025-05-09 15:26:08] [INFO] LightGBM 모델 학습 중 (n_estimators=500, lr=0.05)...
================================================================================
[2025-05-09 15:26:09] [INFO] 에러 발생: LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'
================================================================================
================================================================================
[2025-05-09 15:26:30] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 15:26:30] [INFO] TF-IDF + LightGBM 학습 시작
================================================================================
[2025-05-09 15:26:30] [INFO] 데이터 로드 중...
[2025-05-09 15:26:30] [INFO] 데이터 검증 결과:
[2025-05-09 15:26:30] [INFO] - 총 행 수: 3000
[2025-05-09 15:26:30] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 15:26:30] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 15:26:30] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 15:26:30] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 15:26:30] [INFO] 데이터 분할 중...
[2025-05-09 15:26:30] [INFO] 학습 샘플: 2400개, 검증 샘플: 600개
[2025-05-09 15:26:30] [INFO] TF-IDF 피처 추출 중 (ngrams=[1, 5], max_features=150000)...
[2025-05-09 15:26:37] [INFO] TF-IDF 피처 추출 완료: 150000 피처 (소요시간: 6초)
[2025-05-09 15:26:37] [INFO] LightGBM 모델 학습 중 (n_estimators=500, lr=0.05)...
================================================================================
[2025-05-09 15:26:37] [INFO] 에러 발생: LGBMClassifier.fit() got an unexpected keyword argument 'verbose'
================================================================================
================================================================================
[2025-05-09 15:26:53] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 15:26:53] [INFO] TF-IDF + LightGBM 학습 시작
================================================================================
[2025-05-09 15:26:53] [INFO] 데이터 로드 중...
[2025-05-09 15:26:53] [INFO] 데이터 검증 결과:
[2025-05-09 15:26:53] [INFO] - 총 행 수: 3000
[2025-05-09 15:26:53] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 15:26:53] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 15:26:53] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 15:26:53] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 15:26:53] [INFO] 데이터 분할 중...
[2025-05-09 15:26:53] [INFO] 학습 샘플: 2400개, 검증 샘플: 600개
[2025-05-09 15:26:53] [INFO] TF-IDF 피처 추출 중 (ngrams=[1, 5], max_features=150000)...
[2025-05-09 15:27:00] [INFO] TF-IDF 피처 추출 완료: 150000 피처 (소요시간: 6초)
[2025-05-09 15:27:00] [INFO] LightGBM 모델 학습 중 (n_estimators=500, lr=0.05)...
[2025-05-09 15:27:03] [INFO] LightGBM 학습 완료 (소요시간: 3초)
[2025-05-09 15:27:04] [INFO] 검증 정확도: 97.50%
[2025-05-09 15:27:04] [INFO] 모델 저장 중...
================================================================================
[2025-05-09 15:27:05] [INFO] TF-IDF 및 LGBM 모델 저장 완료
================================================================================
================================================================================
[2025-05-09 15:27:05] [INFO] 전체 TF-IDF + LGBM 과정 완료 (총 소요시간: 11초)
================================================================================
================================================================================
[2025-05-09 15:27:12] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 15:27:12] [INFO] KenLM 3-gram 모델 구축 시작
================================================================================
================================================================================
[2025-05-09 15:27:12] [WARNING] 
KenLM 모듈이 설치되어 있지 않습니다. KenLM 설치 방법:

1. Linux:
   pip install https://github.com/kpu/kenlm/archive/master.zip

2. Windows (복잡):
   a. MSVC 컴파일러와 CMake 설치 필요
   b. https://github.com/kpu/kenlm 에서 소스코드 다운로드 후 빌드
   c. 또는 WSL을 통해 Linux 환경에서 설치

지금은 KenLM 기능을 건너뛰고 계속 진행합니다.
후속 단계에서는 이 기능 없이도 전체 파이프라인이 동작하도록 처리되어 있습니다.

================================================================================
================================================================================
[2025-05-09 15:27:12] [WARNING] Windows 환경에서는 KenLM 설치가 복잡할 수 있습니다. WSL 사용을 권장합니다.
================================================================================
[2025-05-09 15:27:12] [INFO] 더미 KenLM 파일을 생성했습니다: checkpoint\data3.binary
================================================================================
[2025-05-09 15:27:12] [INFO] 나머지 파이프라인은 정상적으로 진행됩니다.
================================================================================
================================================================================
[2025-05-09 15:27:12] [INFO] 전체 KenLM 구축 과정 완료 (총 소요시간: 0초)
================================================================================
================================================================================
[2025-05-09 15:27:23] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 15:27:23] [INFO] 스태킹 메타 모델 학습 시작
================================================================================
================================================================================
[2025-05-09 15:27:23] [INFO] 스태킹 학습 시작 (총 5 단계)
================================================================================
[2025-05-09 15:27:23] [INFO] 검증 데이터 로드 중...
[2025-05-09 15:27:23] [INFO] 데이터 검증 결과:
[2025-05-09 15:27:23] [INFO] - 총 행 수: 900
[2025-05-09 15:27:23] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 15:27:23] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 15:27:23] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 15:27:23] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 15:27:23] [INFO] 검증 데이터 로드 완료: 900개 샘플
[2025-05-09 15:27:23] [INFO] 스태킹 학습 진행 중: 1/5 (20.0%) - 데이터 로드 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 15:27:23] [INFO] Mistral-7B 추론 중...
[2025-05-09 15:27:23] [INFO] Mistral-7B 모델 로드 중...
================================================================================
[2025-05-09 15:27:23] [ERROR] 에러 발생: mistralai/Mistral-7B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
================================================================================
[2025-05-09 15:27:23] [ERROR] 상세 오류: Traceback (most recent call last):
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\utils\_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\requests\models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\utils\hub.py", line 424, in cached_files
    hf_hub_download(
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py", line 961, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py", line 1068, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py", line 1596, in _raise_on_head_call_error
    raise head_call_error
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py", line 1484, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py", line 1401, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py", line 285, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py", line 309, in _request_wrapper
    hf_raise_for_status(response)
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\utils\_http.py", line 459, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-681da048-2902ca4a0e4a481f6fb538c9;56d29f15-9251-4d57-bcea-20662fcd8cf3)

Repository Not Found for url: https://huggingface.co/mistralai/Mistral-7B-Instruct/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\yujin\내 드라이브(dbsdosdb1@gmail.com)\Hanbat\AI_Detect\fake-detect\src\stack.py", line 171, in main
    train_stacking(cfg)
  File "C:\Users\yujin\내 드라이브(dbsdosdb1@gmail.com)\Hanbat\AI_Detect\fake-detect\src\stack.py", line 36, in train_stacking
    mistral_prob = probs_model1(val_df['text'])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yujin\내 드라이브(dbsdosdb1@gmail.com)\Hanbat\AI_Detect\fake-detect\src\train_lora.py", line 175, in probs_model1
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\models\auto\tokenization_auto.py", line 946, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\models\auto\tokenization_auto.py", line 778, in get_tokenizer_config
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\utils\hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yujin\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\utils\hub.py", line 456, in cached_files
    raise OSError(
OSError: mistralai/Mistral-7B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

================================================================================
[2025-05-09 15:27:34] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 15:27:34] [INFO] 최종 추론 시작
================================================================================
================================================================================
[2025-05-09 15:27:34] [INFO] 추론 파이프라인 시작 (총 6 단계)
================================================================================
[2025-05-09 15:27:34] [INFO] 테스트 데이터 로드 중...
[2025-05-09 15:27:34] [INFO] 추론 파이프라인 진행 중: 1/6 (16.7%) - 데이터 로드 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 15:27:34] [INFO] 데이터 검증 및 정제 중...
[2025-05-09 15:27:34] [INFO] 데이터 검증 결과:
[2025-05-09 15:27:34] [INFO] - 총 행 수: 1200
[2025-05-09 15:27:34] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 15:27:34] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 15:27:34] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 15:27:34] [INFO] 추론 파이프라인 진행 중: 2/6 (33.3%) - 데이터 검증 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 15:27:34] [WARNING] 경고: 다음 모델 파일이 없습니다: meta.pkl
[2025-05-09 15:27:34] [WARNING] 일부 모델 없이도 계속 진행합니다. 결과가 부정확할 수 있습니다.
[2025-05-09 15:27:34] [INFO] TF-IDF 모델 로드 중...
[2025-05-09 15:27:34] [INFO] TF-IDF 및 LGBM 모델 로드 완료 (소요시간: 0초)
[2025-05-09 15:27:34] [INFO] TF-IDF 예측 중...
[2025-05-09 15:27:35] [INFO] TF-IDF 예측 완료 (소요시간: 0초)
[2025-05-09 15:27:35] [INFO] 추론 파이프라인 진행 중: 3/6 (50.0%) - TF-IDF 예측 완료 - 경과: 1초, 예상 남은 시간: 1초
[2025-05-09 15:27:35] [INFO] Perplexity 계산 중...
[2025-05-09 15:27:35] [WARNING] KenLM 모듈이 설치되어 있지 않습니다. 랜덤 Perplexity 값을 사용합니다.
[2025-05-09 15:27:35] [INFO] Perplexity 계산 완료 (소요시간: 0초)
[2025-05-09 15:27:35] [INFO] 추론 파이프라인 진행 중: 4/6 (66.7%) - Perplexity 계산 완료 - 경과: 1초, 예상 남은 시간: 0초
[2025-05-09 15:27:35] [INFO] Mistral 모델 예측 중...
[2025-05-09 15:27:35] [INFO] Mistral-7B 모델 로드 중...
[2025-05-09 15:27:35] [ERROR] Mistral 모델 예측 중 오류 발생: mistralai/Mistral-7B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
[2025-05-09 15:27:35] [INFO] 랜덤 확률값을 사용합니다.
[2025-05-09 15:27:35] [INFO] 추론 파이프라인 진행 중: 5/6 (83.3%) - Mistral 예측 완료 - 경과: 1초, 예상 남은 시간: 0초
[2025-05-09 15:27:35] [INFO] DeBERTa 모델 예측 중...
[2025-05-09 15:27:35] [INFO] DeBERTa-v3-Large 모델 로드 중...
[2025-05-09 15:28:56] [ERROR] DeBERTa 모델 예측 중 오류 발생: 체크포인트 파일이 없습니다: checkpoint\deberta.pt
[2025-05-09 15:28:56] [INFO] 랜덤 확률값을 사용합니다.
[2025-05-09 15:28:56] [INFO] 추론 파이프라인 진행 중: 6/6 (100.0%) - DeBERTa 예측 완료 - 경과: 1분 22초, 예상 남은 시간: 0초
[2025-05-09 15:28:56] [INFO] 메타 모델 로드 중...
[2025-05-09 15:28:56] [ERROR] 메타 모델 로드 중 오류 발생: [Errno 2] No such file or directory: 'checkpoint\\meta.pkl'
[2025-05-09 15:28:56] [WARNING] 간단한 투표 모델로 대체합니다.
[2025-05-09 15:28:56] [INFO] 메타 모델로 앙상블 중...
[2025-05-09 15:28:56] [ERROR] 메타 모델 예측 중 오류 발생: 'Series' object has no attribute 'reshape'
[2025-05-09 15:28:56] [WARNING] 평균값으로 대체합니다.
[2025-05-09 15:28:56] [INFO] 예측 클래스 분포: 0(진짜)=588개, 1(가짜)=612개
[2025-05-09 15:28:56] [INFO] 결과 통계: 총 1200개, 가짜 텍스트 비율: 51.00%
================================================================================
[2025-05-09 15:28:56] [INFO] 추론 파이프라인 완료 - submission.csv 저장 완료: submission.csv (소요시간: 1분 22초)
================================================================================
================================================================================
[2025-05-09 11:01:09] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:01:10] [INFO] K-Fold 분할 시작
================================================================================
[2025-05-09 11:01:10] [INFO] 데이터 로드 중...
[2025-05-09 11:01:10] [INFO] 총 3000개 샘플 로드 완료
[2025-05-09 11:01:10] [INFO] 데이터 검증 결과:
[2025-05-09 11:01:10] [INFO] - 총 행 수: 3000
[2025-05-09 11:01:10] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:01:10] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:01:10] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:01:10] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:01:10] [INFO] KFold 비활성화 상태 - 전체 데이터 저장
================================================================================
[2025-05-09 11:01:10] [INFO] train_folds.csv 저장 완료: data/train_folds.csv
================================================================================
================================================================================
[2025-05-09 11:01:14] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:01:14] [INFO] TF-IDF + LightGBM 학습 시작
================================================================================
[2025-05-09 11:01:14] [INFO] 데이터 로드 중...
[2025-05-09 11:01:14] [INFO] 데이터 검증 결과:
[2025-05-09 11:01:14] [INFO] - 총 행 수: 3000
[2025-05-09 11:01:14] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:01:14] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:01:14] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:01:14] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:01:14] [INFO] 데이터 분할 중...
[2025-05-09 11:01:14] [INFO] 학습 샘플: 2400개, 검증 샘플: 600개
[2025-05-09 11:01:14] [INFO] TF-IDF 피처 추출 중 (ngrams=[1, 5], max_features=150000)...
[2025-05-09 11:01:21] [INFO] TF-IDF 피처 추출 완료: 150000 피처 (소요시간: 6초)
[2025-05-09 11:01:21] [INFO] LightGBM 모델 학습 중 (n_estimators=500, lr=0.05)...
[2025-05-09 11:01:28] [INFO] LightGBM 학습 완료 (소요시간: 6초)
[2025-05-09 11:01:29] [INFO] 검증 정확도: 97.50%
[2025-05-09 11:01:29] [INFO] 모델 저장 중...
================================================================================
[2025-05-09 11:01:32] [INFO] TF-IDF 및 LGBM 모델 저장 완료
================================================================================
================================================================================
[2025-05-09 11:01:32] [INFO] 전체 TF-IDF + LGBM 과정 완료 (총 소요시간: 18초)
================================================================================
================================================================================
[2025-05-09 11:01:35] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:01:35] [INFO] KenLM 3-gram 모델 구축 시작
================================================================================
================================================================================
[2025-05-09 11:01:35] [INFO] 이미 KenLM 모델이 존재합니다. 재사용합니다.
================================================================================
================================================================================
[2025-05-09 11:01:35] [INFO] 전체 KenLM 구축 과정 완료 (총 소요시간: 0초)
================================================================================
================================================================================
[2025-05-09 11:01:42] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:01:42] [INFO] Mistral-7B QLoRA 학습 시작
================================================================================
[2025-05-09 11:01:42] [INFO] 데이터 로드 중...
[2025-05-09 11:01:42] [INFO] 데이터 검증 결과:
[2025-05-09 11:01:42] [INFO] - 총 행 수: 3000
[2025-05-09 11:01:42] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:01:42] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:01:42] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:01:42] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:01:42] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:01:42] [INFO] 모델 및 토크나이저 로드 중...
[2025-05-09 11:01:47] [INFO] Mistral-7B 모델 로드 중 (4비트 양자화)...
[2025-05-09 11:03:22] [INFO] LoRA 어댑터 초기화 중...
================================================================================
[2025-05-09 11:03:22] [ERROR] 에러 발생: Please specify `target_modules` in `peft_config`
================================================================================
[2025-05-09 11:03:22] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 229, in main
    train_lora(cfg,
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 62, in train_lora
    model = get_peft_model(model, LoraConfig(task_type=TaskType.SEQ_CLS, r=cfg['mistral']['lora_r'], lora_alpha=cfg['mistral']['lora_alpha']))
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/peft/mapping_func.py", line 123, in get_peft_model
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/peft/peft_model.py", line 1486, in __init__
    super().__init__(model, peft_config, adapter_name, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/peft/peft_model.py", line 132, in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/peft/tuners/lora/model.py", line 142, in __init__
    super().__init__(model, config, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 180, in __init__
    self.inject_adapter(self.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 433, in inject_adapter
    peft_config = self._prepare_adapter_config(peft_config, model_config)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/peft/tuners/lora/model.py", line 499, in _prepare_adapter_config
    raise ValueError("Please specify `target_modules` in `peft_config`")
ValueError: Please specify `target_modules` in `peft_config`

================================================================================
[2025-05-09 11:03:29] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:03:29] [INFO] DeBERTa-v3-Large LoRA 학습 시작
================================================================================
[2025-05-09 11:03:29] [INFO] 데이터 로드 중...
[2025-05-09 11:03:29] [INFO] 데이터 검증 결과:
[2025-05-09 11:03:29] [INFO] - 총 행 수: 3000
[2025-05-09 11:03:29] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:03:29] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:03:29] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:03:29] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:03:29] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:03:29] [INFO] 모델 및 토크나이저 로드 중...
================================================================================
[2025-05-09 11:03:34] [ERROR] 에러 발생: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

================================================================================
[2025-05-09 11:03:34] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1587, in extract_vocab_merges_from_model
    from tiktoken.load import load_tiktoken_bpe
ModuleNotFoundError: No module named 'tiktoken'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1730, in convert_slow_tokenizer
    ).converted()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1624, in converted
    tokenizer = self.tokenizer()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1617, in tokenizer
    vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1589, in extract_vocab_merges_from_model
    raise ValueError(
ValueError: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2302, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 103, in __init__
    super().__init__(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 139, in __init__
    fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1732, in convert_slow_tokenizer
    raise ValueError(
ValueError: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_deberta.py", line 203, in main
    train_deberta(cfg)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_deberta.py", line 38, in train_deberta
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1028, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2062, in from_pretrained
    return cls._from_pretrained(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2303, in _from_pretrained
    except import_protobuf_decode_error():
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 87, in import_protobuf_decode_error
    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))
ImportError: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.


================================================================================
[2025-05-09 11:03:40] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:03:40] [INFO] 스태킹 메타 모델 학습 시작
================================================================================
================================================================================
[2025-05-09 11:03:40] [INFO] 스태킹 학습 시작 (총 5 단계)
================================================================================
[2025-05-09 11:03:40] [INFO] 검증 데이터 로드 중...
[2025-05-09 11:03:40] [INFO] 데이터 검증 결과:
[2025-05-09 11:03:40] [INFO] - 총 행 수: 900
[2025-05-09 11:03:40] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:03:40] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:03:40] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:03:40] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:03:40] [INFO] 검증 데이터 로드 완료: 900개 샘플
[2025-05-09 11:03:40] [INFO] 스태킹 학습 진행 중: 1/5 (20.0%) - 데이터 로드 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 11:03:40] [INFO] Mistral-7B 추론 중...
[2025-05-09 11:03:40] [INFO] Mistral-7B 모델 로드 중...
================================================================================
[2025-05-09 11:03:49] [ERROR] 에러 발생: 체크포인트 파일이 없습니다: checkpoint/mistral.pt
================================================================================
[2025-05-09 11:03:49] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/stack.py", line 171, in main
    train_stacking(cfg)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/stack.py", line 36, in train_stacking
    mistral_prob = probs_model1(val_df['text'])
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 184, in probs_model1
    raise FileNotFoundError(f"체크포인트 파일이 없습니다: {ckpt_path}")
FileNotFoundError: 체크포인트 파일이 없습니다: checkpoint/mistral.pt

================================================================================
[2025-05-09 11:03:56] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:03:56] [INFO] 최종 추론 시작
================================================================================
================================================================================
[2025-05-09 11:03:56] [INFO] 추론 파이프라인 시작 (총 6 단계)
================================================================================
[2025-05-09 11:03:56] [INFO] 테스트 데이터 로드 중...
[2025-05-09 11:03:56] [INFO] 추론 파이프라인 진행 중: 1/6 (16.7%) - 데이터 로드 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 11:03:56] [INFO] 데이터 검증 및 정제 중...
[2025-05-09 11:03:56] [INFO] 데이터 검증 결과:
[2025-05-09 11:03:56] [INFO] - 총 행 수: 1200
[2025-05-09 11:03:56] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:03:56] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:03:56] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:03:56] [INFO] 추론 파이프라인 진행 중: 2/6 (33.3%) - 데이터 검증 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 11:03:56] [WARNING] 경고: 다음 모델 파일이 없습니다: meta.pkl
[2025-05-09 11:03:56] [WARNING] 일부 모델 없이도 계속 진행합니다. 결과가 부정확할 수 있습니다.
[2025-05-09 11:03:56] [INFO] TF-IDF 모델 로드 중...
[2025-05-09 11:03:58] [INFO] TF-IDF 및 LGBM 모델 로드 완료 (소요시간: 1초)
[2025-05-09 11:03:58] [INFO] TF-IDF 예측 중...
[2025-05-09 11:04:00] [INFO] TF-IDF 예측 완료 (소요시간: 2초)
[2025-05-09 11:04:00] [INFO] 추론 파이프라인 진행 중: 3/6 (50.0%) - TF-IDF 예측 완료 - 경과: 3초, 예상 남은 시간: 3초
[2025-05-09 11:04:00] [INFO] Perplexity 계산 중...
[2025-05-09 11:04:00] [ERROR] KenLM 처리 중 오류 발생: Cannot read model 'checkpoint/data3.binary' (lm/read_arpa.cc:65 in void lm::ReadARPACounts(util::FilePiece&, std::vector<long unsigned int>&) threw FormatLoadException. first non-empty line was "DUMMY_KENLM_FILE" not \data\. Byte: 16)
[2025-05-09 11:04:00] [INFO] 랜덤 Perplexity 값을 사용합니다.
[2025-05-09 11:04:00] [INFO] Perplexity 계산 완료 (소요시간: 0초)
[2025-05-09 11:04:00] [INFO] 추론 파이프라인 진행 중: 4/6 (66.7%) - Perplexity 계산 완료 - 경과: 3초, 예상 남은 시간: 1초
[2025-05-09 11:04:00] [INFO] Mistral 모델 예측 중...
[2025-05-09 11:04:00] [INFO] Mistral-7B 모델 로드 중...
[2025-05-09 11:04:05] [ERROR] Mistral 모델 예측 중 오류 발생: 체크포인트 파일이 없습니다: checkpoint/mistral.pt
[2025-05-09 11:04:05] [INFO] 랜덤 확률값을 사용합니다.
[2025-05-09 11:04:06] [INFO] 추론 파이프라인 진행 중: 5/6 (83.3%) - Mistral 예측 완료 - 경과: 10초, 예상 남은 시간: 2초
[2025-05-09 11:04:06] [INFO] DeBERTa 모델 예측 중...
[2025-05-09 11:04:06] [INFO] DeBERTa-v3-Large 모델 로드 중...
[2025-05-09 11:04:07] [ERROR] DeBERTa 모델 예측 중 오류 발생: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

[2025-05-09 11:04:07] [INFO] 랜덤 확률값을 사용합니다.
[2025-05-09 11:04:07] [INFO] 추론 파이프라인 진행 중: 6/6 (100.0%) - DeBERTa 예측 완료 - 경과: 10초, 예상 남은 시간: 0초
[2025-05-09 11:04:07] [INFO] 메타 모델 로드 중...
[2025-05-09 11:04:07] [ERROR] 메타 모델 로드 중 오류 발생: [Errno 2] No such file or directory: 'checkpoint/meta.pkl'
[2025-05-09 11:04:07] [WARNING] 간단한 투표 모델로 대체합니다.
[2025-05-09 11:04:07] [INFO] 메타 모델로 앙상블 중...
[2025-05-09 11:04:07] [ERROR] 메타 모델 예측 중 오류 발생: 'Series' object has no attribute 'reshape'
[2025-05-09 11:04:07] [WARNING] 평균값으로 대체합니다.
[2025-05-09 11:04:07] [INFO] 예측 클래스 분포: 0(진짜)=596개, 1(가짜)=604개
[2025-05-09 11:04:07] [INFO] 결과 통계: 총 1200개, 가짜 텍스트 비율: 50.33%
================================================================================
[2025-05-09 11:04:07] [INFO] 추론 파이프라인 완료 - submission.csv 저장 완료: submission.csv (소요시간: 10초)
================================================================================
================================================================================
[2025-05-09 11:11:02] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:11:02] [INFO] K-Fold 분할 시작
================================================================================
[2025-05-09 11:11:02] [INFO] 데이터 로드 중...
[2025-05-09 11:11:02] [INFO] 총 3000개 샘플 로드 완료
[2025-05-09 11:11:02] [INFO] 데이터 검증 결과:
[2025-05-09 11:11:02] [INFO] - 총 행 수: 3000
[2025-05-09 11:11:02] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:11:02] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:11:02] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:11:03] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:11:03] [INFO] KFold 비활성화 상태 - 전체 데이터 저장
================================================================================
[2025-05-09 11:11:03] [INFO] train_folds.csv 저장 완료: data/train_folds.csv
================================================================================
================================================================================
[2025-05-09 11:11:06] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:11:06] [INFO] TF-IDF + LightGBM 학습 시작
================================================================================
[2025-05-09 11:11:06] [INFO] 데이터 로드 중...
[2025-05-09 11:11:07] [INFO] 데이터 검증 결과:
[2025-05-09 11:11:07] [INFO] - 총 행 수: 3000
[2025-05-09 11:11:07] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:11:07] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:11:07] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:11:07] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:11:07] [INFO] 데이터 분할 중...
[2025-05-09 11:11:07] [INFO] 학습 샘플: 2400개, 검증 샘플: 600개
[2025-05-09 11:11:07] [INFO] TF-IDF 피처 추출 중 (ngrams=[1, 5], max_features=150000)...
[2025-05-09 11:11:14] [INFO] TF-IDF 피처 추출 완료: 150000 피처 (소요시간: 7초)
[2025-05-09 11:11:14] [INFO] LightGBM 모델 학습 중 (n_estimators=500, lr=0.05)...
[2025-05-09 11:11:21] [INFO] LightGBM 학습 완료 (소요시간: 7초)
[2025-05-09 11:11:22] [INFO] 검증 정확도: 97.50%
[2025-05-09 11:11:22] [INFO] 모델 저장 중...
================================================================================
[2025-05-09 11:11:25] [INFO] TF-IDF 및 LGBM 모델 저장 완료
================================================================================
================================================================================
[2025-05-09 11:11:25] [INFO] 전체 TF-IDF + LGBM 과정 완료 (총 소요시간: 18초)
================================================================================
================================================================================
[2025-05-09 11:11:28] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:11:28] [INFO] KenLM 3-gram 모델 구축 시작
================================================================================
================================================================================
[2025-05-09 11:11:28] [INFO] 이미 KenLM 모델이 존재합니다. 재사용합니다.
================================================================================
================================================================================
[2025-05-09 11:11:28] [INFO] 전체 KenLM 구축 과정 완료 (총 소요시간: 0초)
================================================================================
================================================================================
[2025-05-09 11:11:34] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:11:35] [INFO] Mistral-7B QLoRA 학습 시작
================================================================================
[2025-05-09 11:11:35] [INFO] 데이터 로드 중...
[2025-05-09 11:11:35] [INFO] 데이터 검증 결과:
[2025-05-09 11:11:35] [INFO] - 총 행 수: 3000
[2025-05-09 11:11:35] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:11:35] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:11:35] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:11:35] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:11:35] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:11:35] [INFO] 모델 및 토크나이저 로드 중...
================================================================================
[2025-05-09 11:11:35] [ERROR] 에러 발생: mistralai/Mistral-7B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
================================================================================
[2025-05-09 11:11:35] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 424, in cached_files
    hf_hub_download(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1008, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1115, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1643, in _raise_on_head_call_error
    raise head_call_error
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1531, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1448, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 310, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 459, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-681de2e7-1a717c560b444ee05d3b81d1;00005cf1-db52-4ac4-909f-30e7d10f18fe)

Repository Not Found for url: https://huggingface.co/mistralai/Mistral-7B-Instruct/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 237, in main
    train_lora(cfg,
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 49, in train_lora
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 946, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 778, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 456, in cached_files
    raise OSError(
OSError: mistralai/Mistral-7B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

================================================================================
[2025-05-09 11:11:42] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:11:42] [INFO] DeBERTa-v3-Large LoRA 학습 시작
================================================================================
[2025-05-09 11:11:42] [INFO] 데이터 로드 중...
[2025-05-09 11:11:42] [INFO] 데이터 검증 결과:
[2025-05-09 11:11:42] [INFO] - 총 행 수: 3000
[2025-05-09 11:11:42] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:11:42] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:11:42] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:11:42] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:11:42] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:11:42] [INFO] 모델 및 토크나이저 로드 중...
================================================================================
[2025-05-09 11:11:42] [ERROR] 에러 발생: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']
================================================================================
[2025-05-09 11:11:42] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1730, in convert_slow_tokenizer
    ).converted()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1624, in converted
    tokenizer = self.tokenizer()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1617, in tokenizer
    vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1593, in extract_vocab_merges_from_model
    bpe_ranks = load_tiktoken_bpe(tiktoken_url)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/tiktoken/load.py", line 148, in load_tiktoken_bpe
    contents = read_file_cached(tiktoken_bpe_file, expected_hash)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/tiktoken/load.py", line 48, in read_file_cached
    cache_key = hashlib.sha1(blobpath.encode()).hexdigest()
AttributeError: 'NoneType' object has no attribute 'encode'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_deberta.py", line 203, in main
    train_deberta(cfg)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_deberta.py", line 38, in train_deberta
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1028, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2062, in from_pretrained
    return cls._from_pretrained(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2302, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 103, in __init__
    super().__init__(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 139, in __init__
    fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1732, in convert_slow_tokenizer
    raise ValueError(
ValueError: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']

================================================================================
[2025-05-09 11:11:49] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:11:49] [INFO] 스태킹 메타 모델 학습 시작
================================================================================
================================================================================
[2025-05-09 11:11:49] [INFO] 스태킹 학습 시작 (총 5 단계)
================================================================================
[2025-05-09 11:11:49] [INFO] 검증 데이터 로드 중...
[2025-05-09 11:11:49] [INFO] 데이터 검증 결과:
[2025-05-09 11:11:49] [INFO] - 총 행 수: 900
[2025-05-09 11:11:49] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:11:49] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:11:49] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:11:49] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:11:49] [INFO] 검증 데이터 로드 완료: 900개 샘플
[2025-05-09 11:11:49] [INFO] 스태킹 학습 진행 중: 1/5 (20.0%) - 데이터 로드 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 11:11:49] [INFO] Mistral-7B 추론 중...
[2025-05-09 11:11:49] [INFO] Mistral-7B 모델 로드 중...
================================================================================
[2025-05-09 11:11:49] [ERROR] 에러 발생: mistralai/Mistral-7B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
================================================================================
[2025-05-09 11:11:49] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 424, in cached_files
    hf_hub_download(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1008, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1115, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1643, in _raise_on_head_call_error
    raise head_call_error
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1531, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1448, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 310, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 459, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-681de2f5-74dda7897526e13d4747cf1a;0fe6698c-d7a6-4ff2-8cb5-dfa49937e517)

Repository Not Found for url: https://huggingface.co/mistralai/Mistral-7B-Instruct/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/stack.py", line 171, in main
    train_stacking(cfg)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/stack.py", line 36, in train_stacking
    mistral_prob = probs_model1(val_df['text'])
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 183, in probs_model1
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 946, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 778, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 456, in cached_files
    raise OSError(
OSError: mistralai/Mistral-7B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

================================================================================
[2025-05-09 11:11:56] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:11:56] [INFO] 최종 추론 시작
================================================================================
================================================================================
[2025-05-09 11:11:56] [INFO] 추론 파이프라인 시작 (총 6 단계)
================================================================================
[2025-05-09 11:11:56] [INFO] 테스트 데이터 로드 중...
[2025-05-09 11:11:56] [INFO] 추론 파이프라인 진행 중: 1/6 (16.7%) - 데이터 로드 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 11:11:56] [INFO] 데이터 검증 및 정제 중...
[2025-05-09 11:11:56] [INFO] 데이터 검증 결과:
[2025-05-09 11:11:56] [INFO] - 총 행 수: 1200
[2025-05-09 11:11:56] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:11:56] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:11:56] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:11:56] [INFO] 추론 파이프라인 진행 중: 2/6 (33.3%) - 데이터 검증 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 11:11:56] [WARNING] 경고: 다음 모델 파일이 없습니다: meta.pkl
[2025-05-09 11:11:56] [WARNING] 일부 모델 없이도 계속 진행합니다. 결과가 부정확할 수 있습니다.
[2025-05-09 11:11:56] [INFO] TF-IDF 모델 로드 중...
[2025-05-09 11:11:57] [INFO] TF-IDF 및 LGBM 모델 로드 완료 (소요시간: 1초)
[2025-05-09 11:11:57] [INFO] TF-IDF 예측 중...
[2025-05-09 11:12:00] [INFO] TF-IDF 예측 완료 (소요시간: 2초)
[2025-05-09 11:12:00] [INFO] 추론 파이프라인 진행 중: 3/6 (50.0%) - TF-IDF 예측 완료 - 경과: 3초, 예상 남은 시간: 3초
[2025-05-09 11:12:00] [INFO] Perplexity 계산 중...
[2025-05-09 11:12:00] [ERROR] KenLM 처리 중 오류 발생: Cannot read model 'checkpoint/data3.binary' (lm/read_arpa.cc:65 in void lm::ReadARPACounts(util::FilePiece&, std::vector<long unsigned int>&) threw FormatLoadException. first non-empty line was "DUMMY_KENLM_FILE" not \data\. Byte: 16)
[2025-05-09 11:12:00] [INFO] 랜덤 Perplexity 값을 사용합니다.
[2025-05-09 11:12:00] [INFO] Perplexity 계산 완료 (소요시간: 0초)
[2025-05-09 11:12:00] [INFO] 추론 파이프라인 진행 중: 4/6 (66.7%) - Perplexity 계산 완료 - 경과: 3초, 예상 남은 시간: 1초
[2025-05-09 11:12:00] [INFO] Mistral 모델 예측 중...
[2025-05-09 11:12:00] [INFO] Mistral-7B 모델 로드 중...
[2025-05-09 11:12:00] [ERROR] Mistral 모델 예측 중 오류 발생: mistralai/Mistral-7B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
[2025-05-09 11:12:00] [INFO] 랜덤 확률값을 사용합니다.
[2025-05-09 11:12:00] [INFO] 추론 파이프라인 진행 중: 5/6 (83.3%) - Mistral 예측 완료 - 경과: 4초, 예상 남은 시간: 0초
[2025-05-09 11:12:00] [INFO] DeBERTa 모델 예측 중...
[2025-05-09 11:12:00] [INFO] DeBERTa-v3-Large 모델 로드 중...
[2025-05-09 11:12:00] [ERROR] DeBERTa 모델 예측 중 오류 발생: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']
[2025-05-09 11:12:00] [INFO] 랜덤 확률값을 사용합니다.
[2025-05-09 11:12:00] [INFO] 추론 파이프라인 진행 중: 6/6 (100.0%) - DeBERTa 예측 완료 - 경과: 4초, 예상 남은 시간: 0초
[2025-05-09 11:12:00] [INFO] 메타 모델 로드 중...
[2025-05-09 11:12:00] [ERROR] 메타 모델 로드 중 오류 발생: [Errno 2] No such file or directory: 'checkpoint/meta.pkl'
[2025-05-09 11:12:00] [WARNING] 간단한 투표 모델로 대체합니다.
[2025-05-09 11:12:00] [INFO] 메타 모델로 앙상블 중...
[2025-05-09 11:12:00] [ERROR] 메타 모델 예측 중 오류 발생: 'Series' object has no attribute 'reshape'
[2025-05-09 11:12:00] [WARNING] 평균값으로 대체합니다.
[2025-05-09 11:12:00] [INFO] 예측 클래스 분포: 0(진짜)=596개, 1(가짜)=604개
[2025-05-09 11:12:00] [INFO] 결과 통계: 총 1200개, 가짜 텍스트 비율: 50.33%
================================================================================
[2025-05-09 11:12:00] [INFO] 추론 파이프라인 완료 - submission.csv 저장 완료: submission.csv (소요시간: 4초)
================================================================================
================================================================================
[2025-05-09 11:14:10] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:14:10] [INFO] K-Fold 분할 시작
================================================================================
[2025-05-09 11:14:10] [INFO] 데이터 로드 중...
[2025-05-09 11:14:10] [INFO] 총 3000개 샘플 로드 완료
[2025-05-09 11:14:10] [INFO] 데이터 검증 결과:
[2025-05-09 11:14:10] [INFO] - 총 행 수: 3000
[2025-05-09 11:14:10] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:14:10] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:14:10] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:14:10] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:14:10] [INFO] KFold 비활성화 상태 - 전체 데이터 저장
================================================================================
[2025-05-09 11:14:10] [INFO] train_folds.csv 저장 완료: data/train_folds.csv
================================================================================
================================================================================
[2025-05-09 11:14:14] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:14:14] [INFO] TF-IDF + LightGBM 학습 시작
================================================================================
[2025-05-09 11:14:14] [INFO] 데이터 로드 중...
[2025-05-09 11:14:14] [INFO] 데이터 검증 결과:
[2025-05-09 11:14:14] [INFO] - 총 행 수: 3000
[2025-05-09 11:14:14] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:14:14] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:14:14] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:14:15] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:14:15] [INFO] 데이터 분할 중...
[2025-05-09 11:14:15] [INFO] 학습 샘플: 2400개, 검증 샘플: 600개
[2025-05-09 11:14:15] [INFO] TF-IDF 피처 추출 중 (ngrams=[1, 5], max_features=150000)...
[2025-05-09 11:14:22] [INFO] TF-IDF 피처 추출 완료: 150000 피처 (소요시간: 6초)
[2025-05-09 11:14:22] [INFO] LightGBM 모델 학습 중 (n_estimators=500, lr=0.05)...
[2025-05-09 11:14:29] [INFO] LightGBM 학습 완료 (소요시간: 7초)
[2025-05-09 11:14:30] [INFO] 검증 정확도: 97.50%
[2025-05-09 11:14:30] [INFO] 모델 저장 중...
================================================================================
[2025-05-09 11:14:33] [INFO] TF-IDF 및 LGBM 모델 저장 완료
================================================================================
================================================================================
[2025-05-09 11:14:33] [INFO] 전체 TF-IDF + LGBM 과정 완료 (총 소요시간: 18초)
================================================================================
================================================================================
[2025-05-09 11:14:36] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:14:36] [INFO] KenLM 3-gram 모델 구축 시작
================================================================================
================================================================================
[2025-05-09 11:14:36] [INFO] 이미 KenLM 모델이 존재합니다. 재사용합니다.
================================================================================
================================================================================
[2025-05-09 11:14:36] [INFO] 전체 KenLM 구축 과정 완료 (총 소요시간: 0초)
================================================================================
================================================================================
[2025-05-09 11:14:42] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:14:42] [INFO] Mistral-7B QLoRA 학습 시작
================================================================================
[2025-05-09 11:14:42] [INFO] 데이터 로드 중...
[2025-05-09 11:14:42] [INFO] 데이터 검증 결과:
[2025-05-09 11:14:42] [INFO] - 총 행 수: 3000
[2025-05-09 11:14:42] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:14:42] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:14:42] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:14:43] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:14:43] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:14:43] [INFO] 모델 및 토크나이저 로드 중...
================================================================================
[2025-05-09 11:14:43] [ERROR] 에러 발생: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3.
401 Client Error. (Request ID: Root=1-681de3a3-4bb3a8ab4d39810a55824c3d;20c93130-7adf-4594-ae26-0efb64be46cd)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.
================================================================================
[2025-05-09 11:14:43] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 424, in cached_files
    hf_hub_download(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1008, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1115, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1643, in _raise_on_head_call_error
    raise head_call_error
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1531, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1448, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 310, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 426, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-681de3a3-4bb3a8ab4d39810a55824c3d;20c93130-7adf-4594-ae26-0efb64be46cd)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 237, in main
    train_lora(cfg,
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 49, in train_lora
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 966, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1114, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 590, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 649, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3.
401 Client Error. (Request ID: Root=1-681de3a3-4bb3a8ab4d39810a55824c3d;20c93130-7adf-4594-ae26-0efb64be46cd)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.

================================================================================
[2025-05-09 11:14:49] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:14:49] [INFO] DeBERTa-v3-Large LoRA 학습 시작
================================================================================
[2025-05-09 11:14:49] [INFO] 데이터 로드 중...
[2025-05-09 11:14:50] [INFO] 데이터 검증 결과:
[2025-05-09 11:14:50] [INFO] - 총 행 수: 3000
[2025-05-09 11:14:50] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:14:50] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:14:50] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:14:50] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:14:50] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:14:50] [INFO] 모델 및 토크나이저 로드 중...
================================================================================
[2025-05-09 11:14:54] [ERROR] 에러 발생: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']
================================================================================
[2025-05-09 11:14:54] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1730, in convert_slow_tokenizer
    ).converted()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1624, in converted
    tokenizer = self.tokenizer()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1617, in tokenizer
    vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1593, in extract_vocab_merges_from_model
    bpe_ranks = load_tiktoken_bpe(tiktoken_url)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/tiktoken/load.py", line 148, in load_tiktoken_bpe
    contents = read_file_cached(tiktoken_bpe_file, expected_hash)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/tiktoken/load.py", line 48, in read_file_cached
    cache_key = hashlib.sha1(blobpath.encode()).hexdigest()
AttributeError: 'NoneType' object has no attribute 'encode'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_deberta.py", line 203, in main
    train_deberta(cfg)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_deberta.py", line 38, in train_deberta
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1028, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2062, in from_pretrained
    return cls._from_pretrained(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2302, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 103, in __init__
    super().__init__(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 139, in __init__
    fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1732, in convert_slow_tokenizer
    raise ValueError(
ValueError: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']

================================================================================
[2025-05-09 11:15:00] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:15:00] [INFO] 스태킹 메타 모델 학습 시작
================================================================================
================================================================================
[2025-05-09 11:15:00] [INFO] 스태킹 학습 시작 (총 5 단계)
================================================================================
[2025-05-09 11:15:00] [INFO] 검증 데이터 로드 중...
[2025-05-09 11:15:01] [INFO] 데이터 검증 결과:
[2025-05-09 11:15:01] [INFO] - 총 행 수: 900
[2025-05-09 11:15:01] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:15:01] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:15:01] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:15:01] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:15:01] [INFO] 검증 데이터 로드 완료: 900개 샘플
[2025-05-09 11:15:01] [INFO] 스태킹 학습 진행 중: 1/5 (20.0%) - 데이터 로드 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 11:15:01] [INFO] Mistral-7B 추론 중...
[2025-05-09 11:15:01] [INFO] Mistral-7B 모델 로드 중...
================================================================================
[2025-05-09 11:15:01] [ERROR] 에러 발생: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3.
401 Client Error. (Request ID: Root=1-681de3b5-70319ce5515b62e62e35c09a;3ad0d8ae-fdee-427c-9d76-6c12a5ffce41)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.
================================================================================
[2025-05-09 11:15:01] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 424, in cached_files
    hf_hub_download(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1008, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1115, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1643, in _raise_on_head_call_error
    raise head_call_error
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1531, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1448, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 310, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 426, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-681de3b5-70319ce5515b62e62e35c09a;3ad0d8ae-fdee-427c-9d76-6c12a5ffce41)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/stack.py", line 171, in main
    train_stacking(cfg)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/stack.py", line 36, in train_stacking
    mistral_prob = probs_model1(val_df['text'])
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 183, in probs_model1
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 966, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1114, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 590, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 649, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3.
401 Client Error. (Request ID: Root=1-681de3b5-70319ce5515b62e62e35c09a;3ad0d8ae-fdee-427c-9d76-6c12a5ffce41)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.

================================================================================
[2025-05-09 11:15:08] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:15:08] [INFO] 최종 추론 시작
================================================================================
================================================================================
[2025-05-09 11:15:08] [INFO] 추론 파이프라인 시작 (총 6 단계)
================================================================================
[2025-05-09 11:15:08] [INFO] 테스트 데이터 로드 중...
[2025-05-09 11:15:08] [INFO] 추론 파이프라인 진행 중: 1/6 (16.7%) - 데이터 로드 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 11:15:08] [INFO] 데이터 검증 및 정제 중...
[2025-05-09 11:15:08] [INFO] 데이터 검증 결과:
[2025-05-09 11:15:08] [INFO] - 총 행 수: 1200
[2025-05-09 11:15:08] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:15:08] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:15:08] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:15:08] [INFO] 추론 파이프라인 진행 중: 2/6 (33.3%) - 데이터 검증 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 11:15:08] [WARNING] 경고: 다음 모델 파일이 없습니다: meta.pkl
[2025-05-09 11:15:08] [WARNING] 일부 모델 없이도 계속 진행합니다. 결과가 부정확할 수 있습니다.
[2025-05-09 11:15:08] [INFO] TF-IDF 모델 로드 중...
[2025-05-09 11:15:09] [INFO] TF-IDF 및 LGBM 모델 로드 완료 (소요시간: 1초)
[2025-05-09 11:15:09] [INFO] TF-IDF 예측 중...
[2025-05-09 11:15:12] [INFO] TF-IDF 예측 완료 (소요시간: 2초)
[2025-05-09 11:15:12] [INFO] 추론 파이프라인 진행 중: 3/6 (50.0%) - TF-IDF 예측 완료 - 경과: 3초, 예상 남은 시간: 3초
[2025-05-09 11:15:12] [INFO] Perplexity 계산 중...
[2025-05-09 11:15:12] [ERROR] KenLM 처리 중 오류 발생: Cannot read model 'checkpoint/data3.binary' (lm/read_arpa.cc:65 in void lm::ReadARPACounts(util::FilePiece&, std::vector<long unsigned int>&) threw FormatLoadException. first non-empty line was "DUMMY_KENLM_FILE" not \data\. Byte: 16)
[2025-05-09 11:15:12] [INFO] 랜덤 Perplexity 값을 사용합니다.
[2025-05-09 11:15:12] [INFO] Perplexity 계산 완료 (소요시간: 0초)
[2025-05-09 11:15:12] [INFO] 추론 파이프라인 진행 중: 4/6 (66.7%) - Perplexity 계산 완료 - 경과: 3초, 예상 남은 시간: 1초
[2025-05-09 11:15:12] [INFO] Mistral 모델 예측 중...
[2025-05-09 11:15:12] [INFO] Mistral-7B 모델 로드 중...
[2025-05-09 11:15:12] [ERROR] Mistral 모델 예측 중 오류 발생: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3.
401 Client Error. (Request ID: Root=1-681de3c0-379c837d3a4185e97be5c9b1;50fd25c2-f4ee-4305-9aa2-5ba6aeff0a68)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.
[2025-05-09 11:15:12] [INFO] 랜덤 확률값을 사용합니다.
[2025-05-09 11:15:12] [INFO] 추론 파이프라인 진행 중: 5/6 (83.3%) - Mistral 예측 완료 - 경과: 4초, 예상 남은 시간: 0초
[2025-05-09 11:15:12] [INFO] DeBERTa 모델 예측 중...
[2025-05-09 11:15:12] [INFO] DeBERTa-v3-Large 모델 로드 중...
[2025-05-09 11:15:12] [ERROR] DeBERTa 모델 예측 중 오류 발생: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']
[2025-05-09 11:15:12] [INFO] 랜덤 확률값을 사용합니다.
[2025-05-09 11:15:12] [INFO] 추론 파이프라인 진행 중: 6/6 (100.0%) - DeBERTa 예측 완료 - 경과: 4초, 예상 남은 시간: 0초
[2025-05-09 11:15:12] [INFO] 메타 모델 로드 중...
[2025-05-09 11:15:12] [ERROR] 메타 모델 로드 중 오류 발생: [Errno 2] No such file or directory: 'checkpoint/meta.pkl'
[2025-05-09 11:15:12] [WARNING] 간단한 투표 모델로 대체합니다.
[2025-05-09 11:15:12] [INFO] 메타 모델로 앙상블 중...
[2025-05-09 11:15:12] [ERROR] 메타 모델 예측 중 오류 발생: 'Series' object has no attribute 'reshape'
[2025-05-09 11:15:12] [WARNING] 평균값으로 대체합니다.
[2025-05-09 11:15:12] [INFO] 예측 클래스 분포: 0(진짜)=596개, 1(가짜)=604개
[2025-05-09 11:15:12] [INFO] 결과 통계: 총 1200개, 가짜 텍스트 비율: 50.33%
================================================================================
[2025-05-09 11:15:12] [INFO] 추론 파이프라인 완료 - submission.csv 저장 완료: submission.csv (소요시간: 4초)
================================================================================
================================================================================
[2025-05-09 11:18:44] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:18:45] [INFO] K-Fold 분할 시작
================================================================================
[2025-05-09 11:18:45] [INFO] 데이터 로드 중...
[2025-05-09 11:18:45] [INFO] 총 3000개 샘플 로드 완료
[2025-05-09 11:18:45] [INFO] 데이터 검증 결과:
[2025-05-09 11:18:45] [INFO] - 총 행 수: 3000
[2025-05-09 11:18:45] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:18:45] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:18:45] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:18:45] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:18:45] [INFO] KFold 비활성화 상태 - 전체 데이터 저장
================================================================================
[2025-05-09 11:18:45] [INFO] train_folds.csv 저장 완료: data/train_folds.csv
================================================================================
================================================================================
[2025-05-09 11:18:49] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:18:49] [INFO] TF-IDF + LightGBM 학습 시작
================================================================================
[2025-05-09 11:18:49] [INFO] 데이터 로드 중...
[2025-05-09 11:18:49] [INFO] 데이터 검증 결과:
[2025-05-09 11:18:49] [INFO] - 총 행 수: 3000
[2025-05-09 11:18:49] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:18:49] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:18:49] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:18:49] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:18:49] [INFO] 데이터 분할 중...
[2025-05-09 11:18:49] [INFO] 학습 샘플: 2400개, 검증 샘플: 600개
[2025-05-09 11:18:49] [INFO] TF-IDF 피처 추출 중 (ngrams=[1, 5], max_features=150000)...
[2025-05-09 11:18:56] [INFO] TF-IDF 피처 추출 완료: 150000 피처 (소요시간: 6초)
[2025-05-09 11:18:56] [INFO] LightGBM 모델 학습 중 (n_estimators=500, lr=0.05)...
[2025-05-09 11:19:03] [INFO] LightGBM 학습 완료 (소요시간: 7초)
[2025-05-09 11:19:04] [INFO] 검증 정확도: 97.50%
[2025-05-09 11:19:04] [INFO] 모델 저장 중...
================================================================================
[2025-05-09 11:19:08] [INFO] TF-IDF 및 LGBM 모델 저장 완료
================================================================================
================================================================================
[2025-05-09 11:19:08] [INFO] 전체 TF-IDF + LGBM 과정 완료 (총 소요시간: 18초)
================================================================================
================================================================================
[2025-05-09 11:19:11] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:19:11] [INFO] KenLM 3-gram 모델 구축 시작
================================================================================
================================================================================
[2025-05-09 11:19:11] [INFO] 이미 KenLM 모델이 존재합니다. 재사용합니다.
================================================================================
================================================================================
[2025-05-09 11:19:11] [INFO] 전체 KenLM 구축 과정 완료 (총 소요시간: 0초)
================================================================================
================================================================================
[2025-05-09 11:19:17] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:19:17] [INFO] Mistral-7B QLoRA 학습 시작
================================================================================
[2025-05-09 11:19:17] [INFO] 데이터 로드 중...
[2025-05-09 11:19:17] [INFO] 데이터 검증 결과:
[2025-05-09 11:19:17] [INFO] - 총 행 수: 3000
[2025-05-09 11:19:17] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:19:17] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:19:17] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:19:17] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:19:17] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:19:17] [INFO] 모델 및 토크나이저 로드 중...
================================================================================
[2025-05-09 11:19:20] [ERROR] 에러 발생: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.
================================================================================
[2025-05-09 11:19:20] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 237, in main
    train_lora(cfg,
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 49, in train_lora
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1009, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2062, in from_pretrained
    return cls._from_pretrained(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2302, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py", line 157, in __init__
    super().__init__(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 108, in __init__
    raise ValueError(
ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.

================================================================================
[2025-05-09 11:19:27] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:19:27] [INFO] DeBERTa-v3-Large LoRA 학습 시작
================================================================================
[2025-05-09 11:19:27] [INFO] 데이터 로드 중...
[2025-05-09 11:19:27] [INFO] 데이터 검증 결과:
[2025-05-09 11:19:27] [INFO] - 총 행 수: 3000
[2025-05-09 11:19:27] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:19:27] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:19:27] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:19:27] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:19:27] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:19:27] [INFO] 모델 및 토크나이저 로드 중...
================================================================================
[2025-05-09 11:19:27] [ERROR] 에러 발생: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']
================================================================================
[2025-05-09 11:19:27] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1730, in convert_slow_tokenizer
    ).converted()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1624, in converted
    tokenizer = self.tokenizer()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1617, in tokenizer
    vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1593, in extract_vocab_merges_from_model
    bpe_ranks = load_tiktoken_bpe(tiktoken_url)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/tiktoken/load.py", line 148, in load_tiktoken_bpe
    contents = read_file_cached(tiktoken_bpe_file, expected_hash)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/tiktoken/load.py", line 48, in read_file_cached
    cache_key = hashlib.sha1(blobpath.encode()).hexdigest()
AttributeError: 'NoneType' object has no attribute 'encode'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_deberta.py", line 203, in main
    train_deberta(cfg)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_deberta.py", line 38, in train_deberta
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1028, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2062, in from_pretrained
    return cls._from_pretrained(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2302, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 103, in __init__
    super().__init__(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 139, in __init__
    fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py", line 1732, in convert_slow_tokenizer
    raise ValueError(
ValueError: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']

================================================================================
[2025-05-09 11:21:18] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:21:18] [INFO] K-Fold 분할 시작
================================================================================
[2025-05-09 11:21:18] [INFO] 데이터 로드 중...
[2025-05-09 11:21:18] [INFO] 총 3000개 샘플 로드 완료
[2025-05-09 11:21:19] [INFO] 데이터 검증 결과:
[2025-05-09 11:21:19] [INFO] - 총 행 수: 3000
[2025-05-09 11:21:19] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:21:19] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:21:19] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:21:19] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:21:19] [INFO] KFold 비활성화 상태 - 전체 데이터 저장
================================================================================
[2025-05-09 11:21:19] [INFO] train_folds.csv 저장 완료: data/train_folds.csv
================================================================================
================================================================================
[2025-05-09 11:21:23] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:21:23] [INFO] TF-IDF + LightGBM 학습 시작
================================================================================
[2025-05-09 11:21:23] [INFO] 데이터 로드 중...
[2025-05-09 11:21:23] [INFO] 데이터 검증 결과:
[2025-05-09 11:21:23] [INFO] - 총 행 수: 3000
[2025-05-09 11:21:23] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:21:23] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:21:23] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:21:23] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:21:23] [INFO] 데이터 분할 중...
[2025-05-09 11:21:23] [INFO] 학습 샘플: 2400개, 검증 샘플: 600개
[2025-05-09 11:21:23] [INFO] TF-IDF 피처 추출 중 (ngrams=[1, 5], max_features=150000)...
[2025-05-09 11:21:30] [INFO] TF-IDF 피처 추출 완료: 150000 피처 (소요시간: 7초)
[2025-05-09 11:21:30] [INFO] LightGBM 모델 학습 중 (n_estimators=500, lr=0.05)...
[2025-05-09 11:21:37] [INFO] LightGBM 학습 완료 (소요시간: 7초)
[2025-05-09 11:21:38] [INFO] 검증 정확도: 97.50%
[2025-05-09 11:21:38] [INFO] 모델 저장 중...
================================================================================
[2025-05-09 11:21:42] [INFO] TF-IDF 및 LGBM 모델 저장 완료
================================================================================
================================================================================
[2025-05-09 11:21:42] [INFO] 전체 TF-IDF + LGBM 과정 완료 (총 소요시간: 19초)
================================================================================
================================================================================
[2025-05-09 11:21:45] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:21:45] [INFO] KenLM 3-gram 모델 구축 시작
================================================================================
================================================================================
[2025-05-09 11:21:45] [INFO] 이미 KenLM 모델이 존재합니다. 재사용합니다.
================================================================================
================================================================================
[2025-05-09 11:21:45] [INFO] 전체 KenLM 구축 과정 완료 (총 소요시간: 0초)
================================================================================
================================================================================
[2025-05-09 11:21:51] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:21:51] [INFO] Mistral-7B QLoRA 학습 시작
================================================================================
[2025-05-09 11:21:51] [INFO] 데이터 로드 중...
[2025-05-09 11:21:51] [INFO] 데이터 검증 결과:
[2025-05-09 11:21:51] [INFO] - 총 행 수: 3000
[2025-05-09 11:21:51] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:21:51] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:21:51] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:21:51] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:21:51] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:21:51] [INFO] 모델 및 토크나이저 로드 중...
================================================================================
[2025-05-09 11:21:52] [ERROR] 에러 발생: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3.
401 Client Error. (Request ID: Root=1-681de550-0206ccf513b18d384434356e;0339f54a-b062-41ba-a37a-7f03832602aa)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.
================================================================================
[2025-05-09 11:21:52] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 424, in cached_files
    hf_hub_download(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1008, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1115, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1643, in _raise_on_head_call_error
    raise head_call_error
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1531, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1448, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 310, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 426, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-681de550-0206ccf513b18d384434356e;0339f54a-b062-41ba-a37a-7f03832602aa)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 237, in main
    train_lora(cfg,
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 49, in train_lora
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 966, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1114, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 590, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 649, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3.
401 Client Error. (Request ID: Root=1-681de550-0206ccf513b18d384434356e;0339f54a-b062-41ba-a37a-7f03832602aa)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.

================================================================================
[2025-05-09 11:21:58] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:21:58] [INFO] DeBERTa-v3-Large LoRA 학습 시작
================================================================================
[2025-05-09 11:21:58] [INFO] 데이터 로드 중...
[2025-05-09 11:21:58] [INFO] 데이터 검증 결과:
[2025-05-09 11:21:58] [INFO] - 총 행 수: 3000
[2025-05-09 11:21:58] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:21:58] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:21:58] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:21:59] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:21:59] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:21:59] [INFO] 모델 및 토크나이저 로드 중...
[2025-05-09 11:22:03] [INFO] DeBERTa-v3-Large 모델 로드 중...
[2025-05-09 11:22:20] [INFO] LoRA 어댑터 초기화 중...
[2025-05-09 11:22:20] [INFO] 모델 준비 완료 (소요시간: 21초)
[2025-05-09 11:22:20] [INFO] 학습 디바이스: cuda
[2025-05-09 11:22:20] [INFO] 하이퍼파라미터 설정: epochs=3, batch_size=32, lr=2.0e-05
[2025-05-09 11:22:20] [INFO] 데이터로더 초기화 중...
================================================================================
[2025-05-09 11:22:20] [INFO] 학습 시작
================================================================================
================================================================================
[2025-05-09 11:22:20] [INFO] DeBERTa-v3-Large LoRA 학습 시작 (총 3 단계)
================================================================================
[2025-05-09 11:22:44] [INFO] [Epoch 1/3] 학습 loss: nan, 검증 loss: nan, 정확도: 50.00% (소요시간: 24초)
[2025-05-09 11:22:44] [INFO] DeBERTa-v3-Large LoRA 학습 진행 중: 1/3 (33.3%) - loss=nan, acc=50.00% - 경과: 24초, 예상 남은 시간: 48초
[2025-05-09 11:22:44] [INFO] EarlyStopping counter: 1/3
[2025-05-09 11:23:05] [INFO] [Epoch 2/3] 학습 loss: nan, 검증 loss: nan, 정확도: 50.00% (소요시간: 20초)
[2025-05-09 11:23:05] [INFO] DeBERTa-v3-Large LoRA 학습 진행 중: 2/3 (66.7%) - loss=nan, acc=50.00% - 경과: 44초, 예상 남은 시간: 22초
[2025-05-09 11:23:05] [INFO] EarlyStopping counter: 2/3
[2025-05-09 11:23:25] [INFO] [Epoch 3/3] 학습 loss: nan, 검증 loss: nan, 정확도: 50.00% (소요시간: 20초)
[2025-05-09 11:23:25] [INFO] DeBERTa-v3-Large LoRA 학습 진행 중: 3/3 (100.0%) - loss=nan, acc=50.00% - 경과: 1분 4초, 예상 남은 시간: 0초
[2025-05-09 11:23:25] [INFO] EarlyStopping counter: 3/3
================================================================================
[2025-05-09 11:23:25] [INFO] Early stopping 발동! 학습을 조기 종료합니다.
================================================================================
================================================================================
[2025-05-09 11:23:26] [INFO] DeBERTa-v3-Large LoRA 학습 완료 - 모델 저장 완료: checkpoint/deberta.pt (소요시간: 1분 6초)
================================================================================
================================================================================
[2025-05-09 11:23:26] [INFO] DeBERTa-v3-Large LoRA 학습 완료 (총 소요시간: 1분 27초)
================================================================================
================================================================================
[2025-05-09 11:23:33] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:23:33] [INFO] 스태킹 메타 모델 학습 시작
================================================================================
================================================================================
[2025-05-09 11:23:33] [INFO] 스태킹 학습 시작 (총 5 단계)
================================================================================
[2025-05-09 11:23:33] [INFO] 검증 데이터 로드 중...
[2025-05-09 11:23:33] [INFO] 데이터 검증 결과:
[2025-05-09 11:23:33] [INFO] - 총 행 수: 900
[2025-05-09 11:23:33] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:23:33] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:23:33] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:23:33] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:23:33] [INFO] 검증 데이터 로드 완료: 900개 샘플
[2025-05-09 11:23:33] [INFO] 스태킹 학습 진행 중: 1/5 (20.0%) - 데이터 로드 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 11:23:33] [INFO] Mistral-7B 추론 중...
[2025-05-09 11:23:33] [INFO] Mistral-7B 모델 로드 중...
================================================================================
[2025-05-09 11:23:33] [ERROR] 에러 발생: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3.
401 Client Error. (Request ID: Root=1-681de5b5-3dba6634149405bd1d3c563a;ef5f1b35-c64c-4afe-a8ab-056ff357dc99)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.
================================================================================
[2025-05-09 11:23:33] [ERROR] 상세 오류: Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 424, in cached_files
    hf_hub_download(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1008, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1115, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1643, in _raise_on_head_call_error
    raise head_call_error
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1531, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1448, in get_hf_file_metadata
    r = _request_wrapper(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 286, in _request_wrapper
    response = _request_wrapper(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 310, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 426, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-681de5b5-3dba6634149405bd1d3c563a;ef5f1b35-c64c-4afe-a8ab-056ff357dc99)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/stack.py", line 171, in main
    train_stacking(cfg)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/stack.py", line 36, in train_stacking
    mistral_prob = probs_model1(val_df['text'])
  File "/home/elicer/workspace/fake1/AI-Text-Detect/src/train_lora.py", line 183, in probs_model1
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 966, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1114, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 590, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 649, in _get_config_dict
    resolved_config_file = cached_file(
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 266, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "/home/elicer/workspace/fake1/AI-Text-Detect/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 481, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3.
401 Client Error. (Request ID: Root=1-681de5b5-3dba6634149405bd1d3c563a;ef5f1b35-c64c-4afe-a8ab-056ff357dc99)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.

================================================================================
[2025-05-09 11:23:40] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:23:40] [INFO] 최종 추론 시작
================================================================================
================================================================================
[2025-05-09 11:23:40] [INFO] 추론 파이프라인 시작 (총 6 단계)
================================================================================
[2025-05-09 11:23:40] [INFO] 테스트 데이터 로드 중...
[2025-05-09 11:23:40] [INFO] 추론 파이프라인 진행 중: 1/6 (16.7%) - 데이터 로드 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 11:23:40] [INFO] 데이터 검증 및 정제 중...
[2025-05-09 11:23:40] [INFO] 데이터 검증 결과:
[2025-05-09 11:23:40] [INFO] - 총 행 수: 1200
[2025-05-09 11:23:40] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:23:40] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:23:40] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:23:40] [INFO] 추론 파이프라인 진행 중: 2/6 (33.3%) - 데이터 검증 완료 - 경과: 0초, 예상 남은 시간: 0초
[2025-05-09 11:23:40] [WARNING] 경고: 다음 모델 파일이 없습니다: meta.pkl
[2025-05-09 11:23:40] [WARNING] 일부 모델 없이도 계속 진행합니다. 결과가 부정확할 수 있습니다.
[2025-05-09 11:23:40] [INFO] TF-IDF 모델 로드 중...
[2025-05-09 11:23:41] [INFO] TF-IDF 및 LGBM 모델 로드 완료 (소요시간: 1초)
[2025-05-09 11:23:41] [INFO] TF-IDF 예측 중...
[2025-05-09 11:23:44] [INFO] TF-IDF 예측 완료 (소요시간: 2초)
[2025-05-09 11:23:44] [INFO] 추론 파이프라인 진행 중: 3/6 (50.0%) - TF-IDF 예측 완료 - 경과: 3초, 예상 남은 시간: 3초
[2025-05-09 11:23:44] [INFO] Perplexity 계산 중...
[2025-05-09 11:23:44] [ERROR] KenLM 처리 중 오류 발생: Cannot read model 'checkpoint/data3.binary' (lm/read_arpa.cc:65 in void lm::ReadARPACounts(util::FilePiece&, std::vector<long unsigned int>&) threw FormatLoadException. first non-empty line was "DUMMY_KENLM_FILE" not \data\. Byte: 16)
[2025-05-09 11:23:44] [INFO] 랜덤 Perplexity 값을 사용합니다.
[2025-05-09 11:23:44] [INFO] Perplexity 계산 완료 (소요시간: 0초)
[2025-05-09 11:23:44] [INFO] 추론 파이프라인 진행 중: 4/6 (66.7%) - Perplexity 계산 완료 - 경과: 3초, 예상 남은 시간: 1초
[2025-05-09 11:23:44] [INFO] Mistral 모델 예측 중...
[2025-05-09 11:23:44] [INFO] Mistral-7B 모델 로드 중...
[2025-05-09 11:23:44] [ERROR] Mistral 모델 예측 중 오류 발생: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3.
401 Client Error. (Request ID: Root=1-681de5c0-2ae48f731aa96e0151667737;d4f810aa-befe-4c91-80d8-b01850b6f2e9)

Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.
[2025-05-09 11:23:44] [INFO] 랜덤 확률값을 사용합니다.
[2025-05-09 11:23:44] [INFO] 추론 파이프라인 진행 중: 5/6 (83.3%) - Mistral 예측 완료 - 경과: 4초, 예상 남은 시간: 0초
[2025-05-09 11:23:44] [INFO] DeBERTa 모델 예측 중...
[2025-05-09 11:23:44] [INFO] DeBERTa-v3-Large 모델 로드 중...
================================================================================
[2025-05-09 11:24:14] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:24:14] [INFO] K-Fold 분할 시작
================================================================================
[2025-05-09 11:24:14] [INFO] 데이터 로드 중...
[2025-05-09 11:24:14] [INFO] 총 3000개 샘플 로드 완료
[2025-05-09 11:24:14] [INFO] 데이터 검증 결과:
[2025-05-09 11:24:14] [INFO] - 총 행 수: 3000
[2025-05-09 11:24:14] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:24:14] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:24:14] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:24:15] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:24:15] [INFO] KFold 비활성화 상태 - 전체 데이터 저장
================================================================================
[2025-05-09 11:24:15] [INFO] train_folds.csv 저장 완료: data/train_folds.csv
================================================================================
================================================================================
[2025-05-09 11:24:19] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:24:19] [INFO] TF-IDF + LightGBM 학습 시작
================================================================================
[2025-05-09 11:24:19] [INFO] 데이터 로드 중...
[2025-05-09 11:24:19] [INFO] 데이터 검증 결과:
[2025-05-09 11:24:19] [INFO] - 총 행 수: 3000
[2025-05-09 11:24:19] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:24:19] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:24:19] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:24:19] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:24:19] [INFO] 데이터 분할 중...
[2025-05-09 11:24:19] [INFO] 학습 샘플: 2400개, 검증 샘플: 600개
[2025-05-09 11:24:19] [INFO] TF-IDF 피처 추출 중 (ngrams=[1, 5], max_features=150000)...
[2025-05-09 11:24:26] [INFO] TF-IDF 피처 추출 완료: 150000 피처 (소요시간: 6초)
[2025-05-09 11:24:26] [INFO] LightGBM 모델 학습 중 (n_estimators=500, lr=0.05)...
[2025-05-09 11:24:33] [INFO] LightGBM 학습 완료 (소요시간: 6초)
[2025-05-09 11:24:34] [INFO] 검증 정확도: 97.50%
[2025-05-09 11:24:34] [INFO] 모델 저장 중...
================================================================================
[2025-05-09 11:24:37] [INFO] TF-IDF 및 LGBM 모델 저장 완료
================================================================================
================================================================================
[2025-05-09 11:24:37] [INFO] 전체 TF-IDF + LGBM 과정 완료 (총 소요시간: 18초)
================================================================================
================================================================================
[2025-05-09 11:24:40] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:24:40] [INFO] KenLM 3-gram 모델 구축 시작
================================================================================
================================================================================
[2025-05-09 11:24:40] [INFO] 이미 KenLM 모델이 존재합니다. 재사용합니다.
================================================================================
================================================================================
[2025-05-09 11:24:40] [INFO] 전체 KenLM 구축 과정 완료 (총 소요시간: 0초)
================================================================================
================================================================================
[2025-05-09 11:24:47] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:24:47] [INFO] Mistral-7B QLoRA 학습 시작
================================================================================
[2025-05-09 11:24:47] [INFO] 데이터 로드 중...
[2025-05-09 11:24:47] [INFO] 데이터 검증 결과:
[2025-05-09 11:24:47] [INFO] - 총 행 수: 3000
[2025-05-09 11:24:47] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:24:47] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:24:47] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:24:47] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:24:47] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:24:47] [INFO] 모델 및 토크나이저 로드 중...
[2025-05-09 11:24:54] [INFO] Mistral-7B 모델 로드 중 (4비트 양자화)...
[2025-05-09 11:27:26] [INFO] LoRA 어댑터 초기화 중...
[2025-05-09 11:27:26] [INFO] 모델 준비 완료 (소요시간: 2분 38초)
[2025-05-09 11:27:26] [INFO] 학습 디바이스: cuda
[2025-05-09 11:27:26] [INFO] 하이퍼파라미터 설정: epochs=3, batch_size=16, lr=1.0e-04
[2025-05-09 11:27:26] [INFO] 데이터로더 초기화 중...
================================================================================
[2025-05-09 11:27:26] [INFO] 학습 시작
================================================================================
================================================================================
[2025-05-09 11:27:26] [INFO] Mistral-7B QLoRA 학습 시작 (총 3 단계)
================================================================================
================================================================================
[2025-05-09 11:30:35] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:30:35] [INFO] K-Fold 분할 시작
================================================================================
[2025-05-09 11:30:35] [INFO] 데이터 로드 중...
[2025-05-09 11:30:35] [INFO] 총 3000개 샘플 로드 완료
[2025-05-09 11:30:35] [INFO] 데이터 검증 결과:
[2025-05-09 11:30:35] [INFO] - 총 행 수: 3000
[2025-05-09 11:30:35] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:30:35] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:30:35] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:30:35] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:30:35] [INFO] KFold 비활성화 상태 - 전체 데이터 저장
================================================================================
[2025-05-09 11:30:35] [INFO] train_folds.csv 저장 완료: data/train_folds.csv
================================================================================
================================================================================
[2025-05-09 11:30:39] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:30:39] [INFO] TF-IDF + LightGBM 학습 시작
================================================================================
[2025-05-09 11:30:39] [INFO] 데이터 로드 중...
[2025-05-09 11:30:39] [INFO] 데이터 검증 결과:
[2025-05-09 11:30:39] [INFO] - 총 행 수: 3000
[2025-05-09 11:30:39] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:30:39] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:30:39] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:30:40] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:30:40] [INFO] 데이터 분할 중...
[2025-05-09 11:30:40] [INFO] 학습 샘플: 2400개, 검증 샘플: 600개
[2025-05-09 11:30:40] [INFO] TF-IDF 피처 추출 중 (ngrams=[1, 5], max_features=150000)...
[2025-05-09 11:30:47] [INFO] TF-IDF 피처 추출 완료: 150000 피처 (소요시간: 6초)
[2025-05-09 11:30:47] [INFO] LightGBM 모델 학습 중 (n_estimators=500, lr=0.05)...
[2025-05-09 11:30:54] [INFO] LightGBM 학습 완료 (소요시간: 7초)
[2025-05-09 11:30:55] [INFO] 검증 정확도: 97.50%
[2025-05-09 11:30:55] [INFO] 모델 저장 중...
================================================================================
[2025-05-09 11:30:58] [INFO] TF-IDF 및 LGBM 모델 저장 완료
================================================================================
================================================================================
[2025-05-09 11:30:58] [INFO] 전체 TF-IDF + LGBM 과정 완료 (총 소요시간: 18초)
================================================================================
================================================================================
[2025-05-09 11:31:01] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:31:01] [INFO] KenLM 3-gram 모델 구축 시작
================================================================================
================================================================================
[2025-05-09 11:31:01] [INFO] 이미 KenLM 모델이 존재합니다. 재사용합니다.
================================================================================
================================================================================
[2025-05-09 11:31:01] [INFO] 전체 KenLM 구축 과정 완료 (총 소요시간: 0초)
================================================================================
================================================================================
[2025-05-09 11:31:08] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:31:08] [INFO] Mistral-7B QLoRA 학습 시작
================================================================================
[2025-05-09 11:31:08] [INFO] 데이터 로드 중...
[2025-05-09 11:31:08] [INFO] 데이터 검증 결과:
[2025-05-09 11:31:08] [INFO] - 총 행 수: 3000
[2025-05-09 11:31:08] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:31:08] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:31:08] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:31:08] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:31:08] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:31:08] [INFO] 모델 및 토크나이저 로드 중...
[2025-05-09 11:31:09] [INFO] Mistral-7B 모델 로드 중 (4비트 양자화)...
[2025-05-09 11:31:24] [INFO] LoRA 어댑터 초기화 중...
[2025-05-09 11:31:24] [INFO] 모델 준비 완료 (소요시간: 16초)
[2025-05-09 11:31:24] [INFO] 학습 디바이스: cuda
[2025-05-09 11:31:24] [INFO] 하이퍼파라미터 설정: epochs=3, batch_size=16, lr=1.0e-04
[2025-05-09 11:31:24] [INFO] 데이터로더 초기화 중...
================================================================================
[2025-05-09 11:31:24] [INFO] 학습 시작
================================================================================
================================================================================
[2025-05-09 11:31:24] [INFO] Mistral-7B QLoRA 학습 시작 (총 3 단계)
================================================================================
================================================================================
[2025-05-09 11:36:41] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:36:41] [INFO] Mistral-7B QLoRA 학습 시작
================================================================================
[2025-05-09 11:36:41] [INFO] 데이터 로드 중...
[2025-05-09 11:36:41] [INFO] 데이터 검증 결과:
[2025-05-09 11:36:41] [INFO] - 총 행 수: 3000
[2025-05-09 11:36:41] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:36:41] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:36:41] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:36:41] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:36:41] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:36:41] [INFO] 모델 및 토크나이저 로드 중...
[2025-05-09 11:36:42] [INFO] Mistral-7B 모델 로드 중 (4비트 양자화)...
[2025-05-09 11:36:57] [INFO] LoRA 어댑터 초기화 중...
[2025-05-09 11:36:57] [INFO] 모델 준비 완료 (소요시간: 15초)
[2025-05-09 11:36:57] [INFO] 학습 디바이스: cuda
[2025-05-09 11:36:57] [INFO] 하이퍼파라미터 설정: epochs=3, batch_size=8, lr=1.0e-05
[2025-05-09 11:36:57] [INFO] 데이터로더 초기화 중...
================================================================================
[2025-05-09 11:36:57] [INFO] 학습 시작
================================================================================
================================================================================
[2025-05-09 11:36:57] [INFO] Mistral-7B QLoRA 학습 시작 (총 3 단계)
================================================================================
[2025-05-09 11:41:52] [INFO] [Epoch 1/3] 학습 loss: nan, 검증 loss: nan, 정확도: 50.00% (소요시간: 4분 55초)
[2025-05-09 11:41:52] [INFO] Mistral-7B QLoRA 학습 진행 중: 1/3 (33.3%) - loss=nan, acc=50.00% - 경과: 4분 55초, 예상 남은 시간: 9분 50초
[2025-05-09 11:41:52] [INFO] EarlyStopping counter: 1/3
================================================================================
[2025-05-09 11:45:11] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:45:12] [INFO] Mistral-7B QLoRA 학습 시작
================================================================================
[2025-05-09 11:45:12] [INFO] 데이터 로드 중...
[2025-05-09 11:45:12] [INFO] 데이터 검증 결과:
[2025-05-09 11:45:12] [INFO] - 총 행 수: 3000
[2025-05-09 11:45:12] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:45:12] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:45:12] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:45:12] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:45:12] [INFO] [라벨 분포] train: {0: 1050, 1: 1050} val: {0: 450, 1: 450}
[2025-05-09 11:45:12] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:45:12] [INFO] 모델 및 토크나이저 로드 중...
[2025-05-09 11:45:13] [INFO] Mistral-7B 모델 로드 중 (4비트 양자화)...
[2025-05-09 11:45:27] [INFO] LoRA 어댑터 초기화 중...
[2025-05-09 11:45:27] [INFO] 모델 준비 완료 (소요시간: 15초)
[2025-05-09 11:45:27] [INFO] 학습 디바이스: cuda
[2025-05-09 11:45:27] [INFO] 하이퍼파라미터 설정: epochs=3, batch_size=8, lr=1.0e-05
[2025-05-09 11:45:27] [INFO] 데이터로더 초기화 중...
================================================================================
[2025-05-09 11:45:27] [INFO] 학습 시작
================================================================================
================================================================================
[2025-05-09 11:45:27] [INFO] Mistral-7B QLoRA 학습 시작 (총 3 단계)
================================================================================
[2025-05-09 11:45:30] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:30] [INFO] [nan 감지] labels: tensor([1, 1, 1, 0, 0, 0, 0, 0], device='cuda:0')
[2025-05-09 11:45:30] [INFO] [nan 감지] input_ids: tensor([[    1,  1619,  2433, 29501,  6502,  4661, 29501,  3775,  1050,  4649,
         28065,  1066,  8852,  1566, 14387,  1072,  5556,  1070,  1309,  1241,
         17247,  1265,  1065, 22840, 29491, 16925,  4632,  1032, 10238,  1070,
          4877, 17230,  1072,  5432, 17230,  6330, 29493,  1040,  4000,  1070,
          4100, 15425, 15757,  3751, 29473, 29508, 29518, 14127, 10518,  1066,
          8852, 12676, 11858,  1065,  4682, 14733,  1158,  1930,  1158,  1706,
          6695, 10572,  1066, 18569],
        [    1, 29473, 31078, 32109, 29473, 30421,  1007,   947,   947, 31362,
         29473, 31008,  1007,   949,   940, 30083, 29473, 30874,  1007,   922,
           903, 29473, 30478, 31539, 29473, 30083,  1007,   956,   944, 29473,
         30368, 30062, 30455, 32398, 29783, 29473,  1006,   921,   931, 32365,
         30304, 29811, 29962, 32486, 29473, 30004, 32231, 30707, 29955, 29473,
         31433, 31093, 32170, 29957, 29473, 31481,  1008,   914,   944, 30004,
         31417, 30279, 29473, 30372],
        [    1, 29473, 31078, 32109, 29473, 32331, 31878, 30050, 29473, 31099,
         30056, 29473, 29904,  1006,   928,   960, 30489, 29916, 29473,  1008,
           929,   923, 30197, 30201, 29473, 30652, 30083, 32324, 29473, 31250,
         30862, 29473, 30004, 30515, 29783, 29473,  1006,   953,   900, 30919,
         29783, 32486, 29473, 32041, 29932, 29968, 29473, 30616, 31929, 30279,
         29473, 30372, 29811, 29491, 29473, 29932, 31491, 31099, 30056, 29473,
         30092, 29904, 31043, 29473],
        [    1, 29473,  1006,   939,   959, 30766, 29473, 31547, 31544, 31008,
         30512, 29916, 29473,  1008,   922,   911, 30894, 29904, 30279, 29473,
         30372, 29943, 29473, 30194,  1006,   950,   916, 30402, 30083, 32282,
         30992, 29473, 29783, 29904, 29473, 29929,  1008,   927,   923, 30992,
         29493, 29473, 30382, 30056, 30279, 29473, 30515,  1006,   950,   916,
         29473, 30595, 31012, 30985,  1005,   958,   923, 29473, 32170, 30083,
         30238, 32519, 29962, 29811],
        [    1, 12540,  1290,  5865,  1427,  1518,  2037,  1065,  1040,  3377,
          2432,  2035,  1065,  1040,  4867,  1070,  8222, 26102,  1062,  1559,
         28087,  1137,  1309,  8411,  6229,  2813,  1065,  1195,  1550,  1046,
         29491,  3761, 29493, 22653,  4024,  8787,  1066,  2426,  1040,  3671,
          1070,  1401,  7961,  1546,  6013,  1117,  1227,  2511,  4356, 29491,
          1098,  4481,  3383,  1171,  6652,  9586,  1054,  1066,  3852, 16756,
          1122,  9605,  8787,  1124],
        [    1,  1133,  5382, 15002, 11701, 29515,  4589,  1089,  3898, 29494,
          1273, 13336,  8288,  1032,  4294,  8803, 29491,  1328,  8061, 29493,
          1040,  5050,  3296, 21526, 12330, 11350,  1137, 12744,  1040,  8426,
         29493, 13180,  5429, 29501, 28881,  1706,  1070, 22447,  9488,  1831,
         29491,  1183,  4649,  1749,  1624,  1111,  1925, 16497,  1814,  1070,
          4589,  1361, 29478,  8607,  6706,  1122,  2760, 29488,  1129,  6145,
          1088,  2604,  1328, 24266],
        [    1, 29473, 31008, 30083, 31362, 30515,  1006,   910,   923, 29493,
         29473, 29929, 32644, 29473, 29542, 31607, 29916, 29473, 32364, 30056,
         30498, 29903, 29473, 30233, 30778,  1006,   915,   915,  1007,   934,
           931, 29491, 29473, 30489, 30874, 29473, 30183,  1005,   950,   948,
         29473, 30524, 31771, 29473, 31250, 30083, 29916, 29473, 30233, 30778,
         31444, 29943, 30737, 29473,  1007,   927,   935, 32353, 31011, 29473,
         30402, 31468, 32282,  1005],
        [    1, 21642,  9023,  1152,  1093, 11061, 29499,  1569, 10143,  1117,
          6131,  1163,  2777,  3088,  1153,  1173,  1663,  1050, 26005, 29491,
          1619,  4649,  4942,  2071,  3929,  1769, 29501, 19490,  1812,  1120,
          1070,  1073, 29501, 29485,  1595, 29474,  1241,  1093,  9930, 29499,
          1072,  1318, 29525,  1117,  2641,  1589,  2094, 28611,  1163,  1161,
         29511,  1210,  1318, 29525,  1206,  1157,  3071, 29478,  1845,  2777,
          3088,  1153,  1173,  1663]], device='cuda:0')
[2025-05-09 11:45:31] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:31] [INFO] [nan 감지] labels: tensor([1, 1, 0, 0, 0, 0, 1, 1], device='cuda:0')
[2025-05-09 11:45:31] [INFO] [nan 감지] input_ids: tensor([[    1,  1619,  4649,  7462,  1770,  1040,  1469,  2159,  8460,  1293,
          1070,  2611, 29501,  2508,  9139,  1062,  6638,  1394, 16727, 29501,
          1833,  3236,  3204,  1163,  1789, 29493,  4099, 29493,  1072,  5876,
          8798,  1093, 29537, 29533,  5683,  1377, 10384,  1135,  6496,  6282,
          1066, 12776,  1413,  6046,  6638,  1394,  1129,  7680,  1065,  1469,
          2159, 12482,  1245,  7324,  2100,  1150, 29533,  5683,  7651,  1461,
         13911,  7855,  1122,  3207],
        [    1,  8750,  1546,  1040,  5396,  3296, 15447,  6131,  1163,  1950,
          1521,  5533, 12177,  1098, 29516, 29537, 29555, 29527, 29542,  1093,
         29509, 29478, 29509, 29516, 29537, 29555, 29527, 29542, 29499,  1117,
          1032, 13356,  3466,  1070,  4649,  1065, 20923,  1370,  3681, 23772,
         29493,  5845, 29491, 10959,  1639, 27409,  2122, 13481,  5945, 12754,
          1072,  9189,  6817,  4120,  1052,  4387,  2321,  1740,  8480,  3441,
          1040,  5192, 29493,  1504],
        [    1, 29473, 30549, 32247, 29473, 29549, 31607, 30543, 30433, 29473,
         30455, 30056, 30463, 30468, 29903, 29473, 30489, 30874, 29916, 30073,
         29473, 30304,  1008,   924,   908, 29473, 30524, 31771, 29911, 29473,
         32698, 30062, 30919, 30351, 29911, 29473, 29919, 30927, 31310, 30245,
         30588,  1006,   912,   955, 29473, 30367, 30197, 30050, 29473, 31547,
         29903, 29903, 29473, 30665, 29473, 31207, 30306, 29473, 31663,  1006,
           932,   931, 29473, 30062],
        [    1, 29473, 30382,  1006,   930,   947, 30737, 29473, 30294, 29903,
         29473, 30612, 29932, 30543, 29916, 29811, 29903, 29473, 30233, 30612,
         29473, 32291, 29903, 29473, 30294, 30367, 29968, 29473, 30201,  1006,
           921,   958, 30601, 29473, 30908,  1005,   961,   955, 30004, 30279,
         29473, 30372,  1006,   909,   915,  1006,   902,   915, 29473, 29904,
         29943, 29493, 29473, 30062, 30989, 29968, 29473, 31373, 32645, 29943,
         30737, 29473, 30766, 29932],
        [    1, 29473, 30294, 29903, 29473, 32021,  1007,   919,   899, 29957,
         29473, 30238, 30056, 32578, 30338, 29962, 29811, 29491, 29473, 32513,
         29473, 30524, 31771, 29493, 29473, 30092, 30394, 30612, 29473, 30992,
         32376, 30194, 30073, 29473, 30294, 32012, 29473, 30499, 30782, 32227,
         30083, 29473, 31055, 30004, 29916, 29473, 31217, 30668, 29473, 31468,
         30894,  1006,   938,   928, 29955, 29473, 31433, 32082, 29783, 29473,
         30824, 30601, 29473, 30478],
        [    1, 24651, 12314,  8906, 29515,  2559, 10916,  1099,  1040,  9380,
          6131,  1163,  8251,  6229,  2813,  4120,  7503, 13476,  1163,  1150,
         29508, 29527, 29508, 29491,  1119, 21906,  3664, 29503, 29515,  6052,
          1422, 12344,  1054,  1245,  3400,  7324,  8574,  1070,  3204,  2876,
          1589, 29473, 29508, 29518,  1105,  1070,  4363, 29493,  1461,  1422,
         13816,  1066,  1086,  1257,  5741,  4360, 13522,  1065,  1135,  2570,
         29493,  6326, 29493,  1163],
        [    1,  1619,  4598,  2717,  4394,  1040,  5396,  1070,  1032,  1401,
         29493, 14867, 29501,  2062,  6026,  3098, 22018,  1066, 12185,  1040,
          6817,  1070,  1181,  3097, 29476, 15790, 29491,  3231, 23367,  4822,
          7362,  6282,  1072, 15668,  1065,  5788,  1505, 17966, 14088, 29493,
          2191,  1076,  9028, 10658,  1072, 12059, 15961, 20761, 29493, 16102,
          3441,  2055, 12528,  2071,  1274,  6970,  1164, 17595,  5933,  1137,
          1597,  3852,  4085,  1065],
        [    1, 29473, 30402, 30668, 29783, 29473, 32230, 30463, 30279, 29473,
          1006,   944,   915, 29903, 29473, 31850, 30468, 32727, 31346, 29572,
         29473, 32423, 30346, 29473, 30402, 30668, 29783, 29473, 32230, 30463,
         30333, 29473, 30001, 30927, 30919, 29473, 30985, 30512, 29932, 29903,
         29473, 31113, 30001, 32473, 29473,  1006,   962,   915, 29493, 29473,
         30455, 30056, 29473, 30183, 30346, 30461, 32041, 29916, 29473, 31660,
         31790, 30919, 30092, 29473]], device='cuda:0')
[2025-05-09 11:45:32] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:32] [INFO] [nan 감지] labels: tensor([1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')
[2025-05-09 11:45:32] [INFO] [nan 감지] input_ids: tensor([[    1,  1619,  4649, 16658,  1032,  7860,  1990,  1070,  5937,  1463,
          1658,  3422,  1033, 29476, 28213,  1163, 11973,  1845,  3760,  5396,
          5398,  1042,  1065,  1164,  7323,  8517, 29491,  1183,  9046, 22735,
          1171, 19178, 12135,  1827, 25351,  9028, 29493,  1072, 15003,  6595,
         11752,  6821,  1065,  1605,  1069,  2550,  1845,  1040, 13148,  6131,
          1163,  1224,  5412, 29491,  1183, 18915,  3852,  2639,  1124,  1040,
          6179,  1070,  1224,  1289],
        [    1,  1183,  3921,  1090,  2184,  1062,  1072, 24170,  1090,  2184,
          1062, 22447,  1942,  3667,  1066,  7348, 23165,  1171,  8566,  1065,
          2480,  1052,  3325,  4896,  1179,  1072,  1420, 13227,  1741, 29491,
          1183,  4891,  9670,  1465,  1070,  7659, 29501,  6258, 23165, 22447,
          1118,  1265,  1065,  4896,  1179,  4243, 23394, 29497,  1072,  4896,
          1179, 29577,  1072, 13227,  1741, 29706,  1969,  1151,  1522,  1040,
          1675, 29473, 29508, 29502],
        [    1, 29473, 31078, 32109, 29473, 29550, 30525, 30584, 29783, 30468,
         29943, 29473, 32098, 30062, 29903, 29473, 30050, 31547, 29473, 30083,
         30652, 29955, 29473, 30499, 31377, 29903, 29473, 30616, 30333, 30073,
         29493, 29473, 30382, 29473, 31093, 29916, 29473, 32337,  1005,   955,
           951, 29473, 29783, 31031, 29932, 30707, 29783, 29473,  1006,   921,
           931, 30415, 31859, 29932, 29473, 30004, 30549, 31444, 29811, 29491,
         29473, 29550, 30525, 30584],
        [    1, 29473, 32227, 30351, 29911, 29473, 32415, 32415, 30538, 29962,
         29811, 29491, 29473, 30612, 30875, 29473, 31624, 30294, 29473, 30707,
         30201, 30415, 29943, 29929, 29473, 30455, 30056, 29473, 31547, 32231,
         30707, 29783, 29473, 29929,  1007,   959,   927,  1006,   950,   959,
         29473,  1008,   904,   915, 30737, 29473, 30455, 30056, 29473, 30001,
         30543, 29955, 29473, 31217, 31132, 29957, 29473, 30279, 30072,  1006,
           929,   899, 32677, 29473],
        [    1, 29473, 31078, 32109, 29473, 30402, 30668, 29473, 31321, 30482,
         29957, 29473, 30245, 30333, 30073, 29473,  1006,   921,   931, 30415,
         31859, 29943, 29473, 30461, 31518, 29783, 29473, 30372, 29811, 29491,
         29473, 30735, 30657, 29473, 29932,  1007,   907,   931, 29955, 29473,
         31008, 32228, 29957, 29473, 32223, 30455, 30491, 29473,  1008,   929,
           923, 29957, 29473, 29929,  1006,   910,   911, 16875, 29903, 29473,
         31357, 31357, 29473, 31287],
        [    1,  1619,  6215, 16658,  1032,  7860,  5199,  1122,  1040, 11997,
          1072,  3703, 15877,  1070, 22447,  1118,  1265,  5970,  1066,  3698,
         13698,  1193,  1068, 23165, 29501, 20472,  1369,  2103,  1072,  7983,
          5292, 22994,  4369,  2021, 21011,  7711,  1093, 22874, 29558, 29501,
         29537, 29527,  4083, 29511,  1377,  7457,  8395,  1323, 29493,  1164,
          1249, 18072, 29474,  1137,  1645,  1814,  2829,  1507, 14206,  1066,
          3320, 12928, 22417, 29493],
        [    1,  1183,  4673,  1070,  4195, 29501,  6295,  1559, 28087,  1065,
          1157,  3071, 29478,  1845,  1040,  6045,  1072, 19603, 12580,  1070,
          1052,  1223, 27706,  9643,  7651,  1427, 19708,  7256, 11237,  2913,
          1522,  6159, 10841, 29491,  1619,  4826,  2717,  4394, 20821,  4100,
          1124,  1040, 24566,  1072, 21110,  7077,  5191, 24168,  6131,  1163,
          4195, 29501,  6295, 14713,  1066,  5684,  4813,  1070,  2179,  4120,
          2163, 11948,  1254, 10791],
        [    1,  1133,  5382, 15002, 11701, 29515,  1328,  1224,  4649, 29493,
          1246,  6099,  5465,  1072,  1876, 29492,  2350,  1037, 11130, 22742,
          1042,  1070,  1040,  1318, 29521, 29518, 24971,  1245, 13921,  1309,
          1241,  1708, 29496,  1394, 16727,  1093,  4868, 29558, 29499,  1885,
          2374,  8133,  1245, 20574, 13634, 10308,  1072,  1032, 13119,  1052,
          6672,  1093, 29509,  1077,  7685, 11537,  9738,  1044,  2022,  2253,
         29476, 29499,  1065,  5845]], device='cuda:0')
[2025-05-09 11:45:32] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:33] [INFO] [nan 감지] labels: tensor([0, 1, 1, 1, 0, 1, 0, 1], device='cuda:0')
[2025-05-09 11:45:33] [INFO] [nan 감지] input_ids: tensor([[    1,  1133,  5382, 15002, 11701, 29515,  6532,  9139,  1375,  1969,
          1241,  2864,  1323, 23126,  9826,  1228,  6131,  1163,  7659, 29501,
         22023,  1082,  3844,  1463,  1072,  1198, 29501,  8699, 29496,  3866,
          2260,  5494, 18054,  1369,  4981, 13859, 29501, 27521, 11912,  1178,
          5007,  1030,  1080, 19701, 29501, 10414, 15079, 29473, 29508,  1093,
         29526, 29311, 29508, 29499,  2260,  5494, 29491, 15368, 29508, 29552,
          1117,  1032,  3253,  1928],
        [    1,  1619,  4598,  2717,  4394,  1040,  3299,  1047,  2301, 22317,
          2725,  6476,  1070,  1401,  1075,  3134,  1130, 29477, 27379,  1306,
          1072, 20787,  1288, 27379,  1306, 29501,  6295, 13082,  5855,  1122,
          1040,  6595,  1070,  1744, 29532, 25324, 29577, 29481,  8798,  1065,
          2513,  1066,  1289,  4417,  5396,  8222, 26102,  1062,  7058, 29491,
         10959, 28273,  2037,  5851, 20492,  1122,  1744, 29532, 25324, 29510,
         29481, 29493,  1504,  1117],
        [    1, 29473, 31078, 32109, 29473, 31900, 31123, 29473, 29903, 31878,
         29783, 29473, 32331, 31158, 29904, 29943, 29473, 30588, 30338, 29957,
         29473, 30245, 32486, 29473, 30050,  1007,   907,   939, 29957, 29473,
         30433,  1006,   927,   939, 30056, 29943, 29473, 30083, 32158, 30707,
         29783, 29473,  1006,   938,   913, 30304,  1007,   932,   911, 31727,
         30285, 29491, 29473, 29903,  1006,   900,   919, 30306, 29473,  1006,
           938,   926, 30372, 29943],
        [    1,  1619,  4598, 21334,  1066, 18569,  1040, 10963,  1396,  1072,
          1055,  3331,  2356,  1240, 23338,  1070,  1032, 15241, 25658,  1054,
          2839,  1070,  2938, 11589, 29493,  1036,  1260, 13724, 29485, 10840,
         29491, 10959,  1420, 26139,  1404,  1065, 11786,  2479, 28142, 29481,
         29493,  7284,  2639,  7376,  8985,  1040,  4673,  1070,  1935, 16947,
          3509,  1047,  1375,  1514,  6098,  1065, 18645,  1055,  3331,  9602,
         10518, 29491,  1619,  4649],
        [    1, 29473, 29904, 30595, 32053, 29473, 30382,  1006,   930,   947,
         30737, 29473, 29932, 31303, 30919, 30351, 29911, 29473, 30382,  1006,
           931,   906, 31083, 29473, 32021, 29783, 29929, 30285, 29491, 29473,
         30294, 29903, 29473, 30543, 32464, 30056,  1005,   958,   923, 29473,
         32060, 29473, 30050, 31547, 30306, 30862, 29473, 32464, 30782,  1005,
           958,   923, 29473, 30455, 30056, 29473, 31547, 30001, 32170, 30083,
         29473, 30679, 30679, 29473],
        [    1,  1619,  4598, 16658,  1040,  5261, 13349, 29501,  2166,  1121,
          6145,  6276,  1070,  1029,  1121,  1266,  1204, 29492, 21011,  1065,
          2727, 29485,  1241,  2094, 27488,  1192, 29501,  1594,  2743, 19877,
          1489,  2059, 27325,  1163, 13169, 29485,  1241,  4125,  1079,  3246,
          1263,  1072,  2760, 29488,  1129,  6145,  8049,  1060,  8189, 15790,
          1093,  5170, 12074, 29496,  1377,  1183,  5856,  1070,  1224, 22447,
         29478,  8607,  1124,  3493],
        [    1, 29473, 30766, 29943, 29473, 30415, 32677, 31478, 29473, 29783,
         29473, 30062, 30056, 29916, 29473, 30874, 30894, 29473, 30455, 30056,
         29473, 29783, 30306, 32347, 29473, 30512, 30584,  1006,   910,   923,
         29783, 30463, 29473, 32060, 29943, 29473, 29811, 31647, 29473,  1007,
           953,   940, 31362, 32347, 29473, 30512, 30584,  1006,   910,   923,
         30050, 31334, 30367, 29473, 30294, 31093, 30238, 30056, 30279, 29473,
          1007,   910,   953, 30338],
        [    1,  1299, 29473,  1006,   926,   951, 29811, 29943, 29473, 29911,
          1006,   951,   906, 29473, 29811, 29473, 30588, 32726, 29811, 29515,
         29473, 30795, 31547, 30073, 29473, 32513, 29473,  1008,   927,   951,
          1006,   939,   955, 31293, 29783, 30238, 29473, 30871, 30468,  1008,
           905,   935, 29473, 31011, 29473, 30661, 29955, 29473,  1008,   911,
           919, 29811, 30056, 29903, 29473,  1007,   962,   952,  1007,   961,
           904, 30781, 30056, 32486]], device='cuda:0')
[2025-05-09 11:45:33] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:33] [INFO] [nan 감지] labels: tensor([1, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')
[2025-05-09 11:45:33] [INFO] [nan 감지] input_ids: tensor([[    1,  1619,  4598, 16658,  1164, 12365,  1070,  1040,  8783,  1070,
          1040,  1784, 29509, 17966,  1124, 13915,  1153,  1293, 16574, 29484,
         13888,  1065,  1181,  3220,  1031,  1307,  1283,  1758, 29478, 29491,
          1429,  1427,  1518,  3419,  1122,  1509,  1495,  1823,  1137,  1181,
         29491,  2164, 29478,  6866,  5934, 12876,  1066,  9725,  1245,  7495,
         26005, 29493,  3258,  1032,  4356,  1330,  6413, 14943,  2755, 13915,
          1153,  1068,  1050, 16574],
        [    1,  1619,  6215, 16658,  1032,  7860,  1252,  4924,  1062,  7659,
         29501,  8592, 14982,  4632, 22447,  1942,  1102, 29564, 29509, 29538,
         29493,  6970,  1122,  2472,  1488,  1833,  1250,  1198,  1489,  3719,
          9173,  1072,  6595,  1070, 20787,  1288, 29512, 29491, 13008,  1288,
         29512,  1117,  1032,  1066, 29512,  1030, 29501,  4045,  1369, 18652,
          1273, 18965,  8102,  1254,  1133,  1091,  1193,  1149, 20787, 21950,
          1046,  1137,  1309,  1115],
        [    1, 12193,  1030, 29501,  6295,  8585,  3422,  1516,  1117,  1032,
          7860, 29493, 14867,  1451, 19158,  2864,  8180,  6282,  1163,  2366,
          5396,  1122,  6411,  1070,  5398, 22317, 12482, 29491,  1183,  6282,
          1390,  3852, 14428,  5460,  2100,  1576, 29501,  9791, 26516,  1360,
          1066,  2993, 18437,  2681,  1054,  2725,  6476,  1070,  7885, 15606,
          1070, 22317, 12482,  1065,  1032,  7184,  3716, 29493,  6107,  1263,
         29493,  1072, 14277,  9485],
        [    1,  1862,  1065,  1131,  6230, 19751, 16883,  2997,  1171,  6970,
          1066,  9002,  7961,  1070,  2630,  1045,  5243,  1261,  1241,  5653,
          1072,  4369,  1866,  4694,  2706,  4429,  1082,  3844,  1463,  1072,
         21698, 22087,  1065,  1040, 19726, 29491,  1088, 12449,  1252, 25940,
          2037,  1070,  1036,  1273,  4088, 21220, 12756,  1056,  1422, 24898,
          2712,  6243,  8034,  1065,  1040,  1851,  8654, 19602, 19751,  1072,
         12482,  1070, 19751, 16883],
        [    1, 29473, 30382, 29473, 30304,  1008,   919,   919, 29957, 29473,
          1005,   949,   941, 30306, 29473, 30985, 29783, 30004, 30279, 29473,
         31078, 32441, 30498, 29473, 30083, 30515, 30367, 29473, 30382, 29473,
         30304,  1008,   919,   919, 29957, 29473,  1005,   949,   941, 31359,
         30201, 30285, 29491, 29473, 30382,  1006,   930,   947, 30737, 29473,
         31078, 32441, 30498, 29473, 30083, 30515, 29473, 30616, 30279, 29473,
         29783,  1006,   931,   906],
        [    1, 29473, 31377, 31334, 31859, 31334, 29473, 30402, 31468, 32282,
         30367, 29473, 32493, 31288, 30050, 29473, 29783, 30657, 29473,  1006,
           949,   919,  1007,   934,   903, 31043, 29955, 29473, 30368,  1007,
           930,   900, 29916, 30073, 29473, 30595, 31012, 29473, 29903, 29929,
         29911, 29473,  1007,   933,   899, 29473, 31287, 29473, 30183, 30960,
         29473, 30668, 31113, 31547, 29473, 29932, 31491, 30351, 29911, 29473,
          1006,   951,   935, 29957],
        [    1,  1133,  5382, 15002, 11701, 29515,  8343,  4845,  6080,  1040,
          5396,  1066, 14968,  1032,  6103,  8706,  1452, 11237,  4100, 29493,
          3258,  1040,  1566, 29491,  3761, 29493,  2396,  8679, 26305,  4100,
          7376,  1066,  9095,  3296, 15835,  8985,  1535,  4559,  1072,  1678,
          1066,  7765,  1421,  5856, 29491,  1584, 21900,  1040,  3577,  4845,
          6542,  1190,  4969,  1061,  4178, 29482,  2539,  7508,  1065, 29473,
         29518, 29502, 29508, 29549],
        [    1,  1183, 13287,  1137, 29493,  1122,  1040,  4003,  8517, 29493,
          1040,  8717,  1070, 12278,  1072,  6085,  1706,  1070,  3493,  1089,
          3898, 29494, 12742,  1343,  1537,  1724, 29481,  1475,  5396,  7009,
          1117,  1032,  6632, 20412,  1066, 12725,  1109, 24667,  5528,  1070,
          1935, 13356, 12535, 29491,  4589,  1089,  3898, 29494,  1273,  1109,
         24667,  5528,  1761,  1115, 22655,  1158,  5247, 18231,  1176,  1120,
          1056, 29493, 12014,  1066]], device='cuda:0')
[2025-05-09 11:45:34] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:34] [INFO] [nan 감지] labels: tensor([0, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')
[2025-05-09 11:45:34] [INFO] [nan 감지] input_ids: tensor([[    1, 29473, 30382,  1006,   930,   947, 30737, 29473, 30679, 30543,
         30919, 30351, 29911, 29473, 30245, 30333, 29473,  1006,   901,   940,
         30489, 30062, 29473, 32347, 30056, 30547, 30294, 29473, 31735, 30499,
         29968, 29473, 30512, 30194, 30073, 29473,  1006,   901,   940, 30489,
         30062, 29916, 30601, 29473, 30595, 31012, 29473, 29932, 30652, 29968,
         29473, 30543, 30595, 29904, 30279, 29473, 30372, 29943, 30737, 29473,
         30382,  1006,   931,   906],
        [    1, 29473, 30766, 29943, 29473, 30382, 29473, 31246, 30499, 29903,
         29473, 30461, 31518, 29783, 29473,  1006,   901,   940, 29962, 29811,
         29491, 29473, 30766, 29943, 29473, 29783,  1006,   930,   947, 29473,
          1007,   955,   932, 32109, 29783, 29473, 30001, 32021, 29473, 32227,
         29473, 29955, 30056, 29903, 29473, 30372, 29943, 29473, 30985, 29783,
         29811, 29473, 29783,  1006,   931,   906, 30601, 29473, 30461, 31518,
         30538, 29962, 29811, 29491],
        [    1, 29473, 29932, 31338, 29903, 29473, 31287, 29473, 29783, 30346,
         30194, 31113, 29811, 29491, 29473,  1008,   914,   944,  1007,   922,
           959, 29493, 29473,  1008,   924,   912, 29919, 29493, 29473, 29903,
          1006,   944,   903, 29783, 29473,  1007,   929,   937, 30304, 29929,
         30279, 29493, 29473,  1005,   954,   956, 32755, 30050, 29473, 29932,
         30346, 29473, 30973, 30346, 29783, 29473,  1006,   956,   907, 30524,
         30194, 29929, 30333, 30073],
        [    1,  1619,  4649,  5442,  1546,  1040, 26139,  1404,  1070,  3698,
          1055,  1170, 23165,  1093, 29537,  7449, 29558, 29499,  1065, 24266,
          4120,  7324,  2100,  3204,  1072, 12879, 29491,  1183, 14408,  1117,
          1066,  8423,  2254,  5391, 20207,  1122,  1150,  7449, 29558,  8460,
          1050, 16977, 29493,  1065,  2513,  1066,  9124,  4397, 16932,  1124,
          6413, 12876,  1122, 24916,  1420,  6817, 29491,  1098, 16081,  4826,
          1070,  9366, 15961, 20761],
        [    1, 13646, 29515,  1086,  3439,  1046,  1117,  1032,  4066,  3379,
          1122,  1484,  4865,  2424,  5796,  1093,  2372, 29547, 29499, 24256,
          1072,  6229,  2813,  1065, 19355, 29547,  7651, 29491, 10959,  7256,
          2913,  1065,  6595, 12876, 28699, 21826, 16136,  1065, 19355, 29547,
          7651, 29493,  1476, 20761,  1274,  7499, 16806,  8477, 29491,  9202,
         21826,  1065,  7651,  1163,  1195,  1550,  1046, 29491,  1119, 21906,
          3664, 29503, 29515,  1584],
        [    1, 29473, 30543,  1008,   902,   900, 30238, 30056, 32578, 30338,
         29962, 29811, 29491, 29473, 30382, 30056, 30279, 29473,  1007,   953,
           940,  1006,   953,   900, 31780,  1007,   927,   932, 32170,  1006,
           910,   923, 29904, 30279, 29473,  1007,   953,   940, 31951, 31780,
          1007,   927,   932, 32170,  1006,   910,   923, 29916, 30601, 29473,
         29783, 29473, 30478, 30294, 29916, 29473, 30402, 30050, 29473, 30050,
         32021,  1007,   919,   899],
        [    1,  1183,  1131, 28908,  1716, 15971,  1912,  1151,  1309,  1115,
          5293,  1158,  1164,  2938, 29516, 29475, 12449,  1458, 25861,  1042,
          1032,  2094,  1096,  3154,  1070,  1716,  1090,  3573,  1273,  9662,
          1458,  7799,  1158,  1032, 13342,  1255,  1895,  1290,  4369,  1866,
         20412,  4707,  1845,  1040,  8218,  3532,  1070,  5010, 23599,  1245,
          1639, 12796, 19751,  1072,  1066,  3370,  1040,  9645,  1070, 14625,
          3066,  1072,  9662,  2212],
        [    1, 29473, 30402,  1007,   953,   940, 29473, 30489, 29473, 29903,
         29929, 29473, 30478, 30294, 31357, 29473, 31604, 30304, 30285, 29491,
         29473, 32170, 30083, 30584, 29473, 29932, 31132, 29473, 30795, 29916,
         29473, 30652, 30874, 31205, 30083, 29473, 29932, 31132, 30612, 29473,
         30862, 30001, 29932, 30992, 29473, 32170,  1007,   947,   947, 29473,
         29932, 31132, 29957, 29473, 30201,  1006,   921,   958, 30601, 29473,
         30985, 30056, 30491, 29473]], device='cuda:0')
[2025-05-09 11:45:35] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:35] [INFO] [nan 감지] labels: tensor([0, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')
[2025-05-09 11:45:35] [INFO] [nan 감지] input_ids: tensor([[    1,  1457,  8392,  1168,  4380, 18196,  1102, 29493,  1458,  1117,
          8102,  1254, 18965,  1163,  1040,  1168,  4380, 18196,  1102, 15790,
          1093, 17341, 29558,  1325,  1117,  1032,  4294,  3296,  3468, 29491,
         10384,  1032, 13501,  2997,  1070,  1168,  4380, 18196,  1102, 29493,
          1246, 21900,  1040,  8222, 26102,  1062,  6860,  1070,  1032,  1080,
          1443,  6489,  1208, 11748,  1030,  1283, 15790,  1093, 29480, 29558,
         29558, 29499,  1137,  3292],
        [    1, 29473, 30402, 30547, 29473,  1006,   953,   900, 30547, 29957,
         29955, 29473,  1008,   924,   912, 29955,  1006,   928,   960, 29473,
         30512, 30584, 30197, 29962, 29811, 29491, 29473, 30294, 29903, 29473,
         29783,  1006,   931,   906, 30601, 29473, 30245, 30333, 29493, 29473,
         29518, 31607, 29473, 29508, 29555, 30183, 29473, 31801, 29473, 30294,
         29903, 29473, 30073, 32526, 29473, 32287, 30468,  1007,   924,   919,
         29811, 29903, 29473, 30402],
        [    1,  1619,  4649,  7462,  1770,  1040, 11384, 20486,  1065, 28475,
          1050,  5010,  1073,  1850,  1489, 27488,  1192,  1851, 13188,  1070,
          7651, 27397,  1163, 29473, 29518, 29502, 29502, 29542, 16517,  1150,
         29508, 29527, 29508,  2487,  1241, 29501, 15419,  5533, 12177,  1098,
         15790, 18965, 29491,  1183, 29473, 29518, 29502, 29502, 29542, 16517,
         26798,  1171,  8102,  1254,  1032,  7860, 21610,  1070,  1040, 15790,
         29493,  1072,  1146,  4931],
        [    1, 29473, 30382, 31207, 29916, 29473, 32127, 30468, 30073, 29473,
         30304, 30871, 29473, 30092, 30547, 30001,  1007,   948,   904, 30661,
         30668, 29932,  1008,   925,   912, 31083, 29783, 30468, 29943, 29473,
         30304, 29783, 31167, 30201, 29903, 29473, 30463, 32154, 29473, 31207,
         29473, 31604, 30338, 29962, 29811, 29491, 29473, 30382, 30946, 29473,
         30766,  1008,   928,   943, 29903, 29473,  1006,   938,   913, 29783,
         29473, 29783, 31031, 29932],
        [    1,  1183,  6229,  2813, 19864,  1290,  1066, 19355, 29547, 29501,
          1091,  1132,  2128,  5010,  4656, 18965,  1093, 29528,  3823, 29499,
          3568,  1172,  2212,  7961,  3708,  1066, 22024,  6330,  2075,  1122,
          1769, 29484,  1187, 12391, 29491, 12850,  6222, 29501, 12599, 12391,
          1427,  2252,  1518,  2075,  1066,  5229,  2719,  1608, 16885,  1507,
         15096,  1133,  3823, 19864,  1290,  6229,  2813,  1065,  1040, 19355,
         29547, 29491,  1584, 13584],
        [    1, 29473, 32252, 29473, 31744, 32578, 30338, 29962, 29811, 29491,
         29473, 32170, 30083, 30538, 29962, 29811, 29491, 29473, 31377, 29473,
         30524, 31771, 29943, 30285, 29473, 31547, 31008, 30661,  1008,   923,
           900, 29955, 29473,  1008,   920,   952, 32755, 29783, 29473, 30543,
         30402, 30661, 32398, 29473, 30083, 31047, 30092, 30737, 29473, 29929,
         31099, 29473, 30245, 30333, 29473, 31195, 29919, 29473, 30368, 30394,
         29955, 29473, 29929, 30875],
        [    1, 29473, 31078, 32109, 29473, 31663, 29903, 29473, 30346, 32441,
         29783, 29473, 29929, 31341, 30616, 30333, 30073, 29473, 31547, 32231,
         30707, 29955, 29473,  1007,   901,   953, 29916, 29473, 32462, 29473,
         30543, 32337, 29957, 29473, 30325, 30279, 29473, 30372, 29811, 29491,
         29473, 32452, 32677, 29473, 31123,  1008,   902,   900, 29473, 31663,
         29903, 29943, 29473, 29903, 30874, 29473, 30334, 30627, 29955, 29473,
         32462, 29473, 30543, 30985],
        [    1, 29473, 30382,  1005,   948,   955, 29473, 29783, 30895, 29911,
         29473, 30194, 30073, 29473, 31250, 30368, 29473, 29552, 30083, 30367,
         29473, 31604, 29783, 29473, 30782, 31217, 30512,  1008,   922,   923,
         29932, 30992, 30351, 29911, 29473, 31155, 30706, 30004,  1007,   959,
           927, 30073, 29473, 30547, 30609, 30609, 30001, 29957, 29473, 29904,
         30468, 30279, 29473, 31444, 29943, 30737, 29493, 29473, 30382, 31012,
         29962, 31346, 29473, 30547]], device='cuda:0')
[2025-05-09 11:45:36] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:36] [INFO] [nan 감지] labels: tensor([1, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')
[2025-05-09 11:45:36] [INFO] [nan 감지] input_ids: tensor([[    1,  1619,  4598,  4948,  2145,  1032,  7860, 22859, 16008,  7352,
          7292,  1285,  1120,  4449,  1137, 21933,  1124,  1040,  4867,  1072,
         10554,  1070,  7352,  2508,  2929,  1065, 11710, 29491,  1183, 14408,
          1171,  1066,  2999,  1164, 23737, 29493,  9864, 16008,  6282,  5949,
         11405,  1070, 21293,  1620,  1066,  1639,  6211,  1122,  4510, 22735,
          1072, 24195,  2424,  1522,  1032,  9528,  4449, 29491,  1619, 12752,
          4514,  4649,  8335,  2636],
        [    1,  1619,  4598, 16658,  1164, 16415,  6411,  1070,  3662,  3420,
          1293,  1072,  1971,  3257,  1293,  6099, 11463,  1065,  1032,  3286,
          8517,  1163,  5903,  1202,  1062,  9643, 29491,  1183,  4649, 18707,
          1066, 16395,  1167,  4152, 29501,  9698,  4958,  1245,  2328, 17701,
          7026, 15052,  1039,  1072,  1073,  1850,  1489,  3947,  2192,  1297,
          2786, 29493,  1158,  1930,  1158,  3460, 18104,  1090,  1315,  3858,
          2317,  1093, 15792, 29558],
        [    1,  1619,  4649, 21334,  1066, 17982,  1040,  5203,  1070,  1354,
          1170,  1256,  1273, 17966,  6550,  1072, 15079, 16228,  1065,  7651,
          1163, 16640,  1369,  4916, 29478, 13724, 24229, 29492,  1093,  7281,
         29523, 29499,  1461,  1274,  1518,  4585,  1054,  1066,  5261,  1138,
          2859,  7032,  1536, 22849,  1163,  5261,  1138,  1244, 29482,  1366,
         25494, 12149,  7297, 29491, 13374, 29523,  1117,  1032, 21226, 29493,
          3376, 20942,  4916, 19530],
        [    1,  9332, 15248, 16950,  3505,  1046,  1117,  1164,  3046,  3207,
          5191,  7219,  1070,  4886,  4916, 19530, 28908, 19025, 29491,  1183,
         13194,  1070, 21592, 29552,  8296,  1065, 24571,  2263, 29493, 21698,
         23238, 29493, 15052,  1039,  4115, 15467, 29493,  1072,  1080,  3108,
         16950,  3505,  1046,  1274,  1518, 27392, 29491,  3761, 29493,  1040,
          1676,  1072, 14943,  1070, 21592, 29552,  8296,  1065,  5065, 28410,
          1522, 11143,  1120, 29501],
        [    1, 29473,  1007,   953,   940, 31951, 29473,  1006,   904,   959,
         31104, 30874,  1006,   932,   948, 31099, 31104, 29473, 30626, 30894,
         29955, 29473, 31637, 31258, 32231, 30197, 29962, 29811, 29491, 29473,
         30969, 30515,  1006,   910,   923, 29493, 29473, 30455, 30056, 29473,
         30402, 30050, 32231, 31547, 29916, 29473, 29518, 29508, 30489, 29932,
         29473, 30707, 30201, 30073, 29493, 29473, 29518, 29502, 29502, 29502,
         32247, 29473, 29783, 31338],
        [    1, 29473, 30325, 29911, 29473, 30421, 30294, 30233, 29957, 29473,
         32127,  1007,   932,   911, 29811, 29943, 29473, 31207, 29473, 30304,
          1006,   910,   924, 29962, 31346, 29491, 29473, 30382,  1006,   930,
           947, 30737, 29473, 30421, 30294, 30233, 30367, 29473, 30050, 29919,
         30584, 29916, 30073, 29473, 29518, 29502, 29508, 29551, 32247, 29473,
         29538, 31607, 29473, 31850, 29916, 29943, 29473, 30421, 30294, 30233,
         29783, 29473, 29538, 29555],
        [    1,  1619,  4598,  4948,  2145,  1072,  7316,  1770,  1032, 13995,
          4689,  1066,  9819,  1040, 10122,  8059,  1070, 29214, 28320,  1062,
         14706, 29491,  1328,  6159,  2035, 29493,  3191,  4813,  1427,  3495,
         13875,  3046,  3708,  1066, 12630, 11632,  1452,  3698,  3296,  1072,
         13275,  8915, 29491,  1904,  2027, 29493,  1032,  1695,  1365,  4668,
          1122,  6413, 15849,  6330,  1122,  1643,  6987, 25879,  1245,  4886,
          8059,  1065,  2513,  1066],
        [    1,  1183, 15965,  1070,  1040, 21698, 15683,  7906,  1254,  1188,
          3878, 29501, 29502, 29508, 29502, 29502, 29493,  1164, 12201,  6156,
          8034,  5533, 12177, 22018, 10330,  2355, 29493,  2181,  2349,  5261,
          1138,  2605, 12876,  1117,  1032, 13808,  4475,  4880,  7167,  1040,
         25930,  5396,  1072, 11742,  1056,  5261, 23280,  2603,  2055,  8460,
          1050, 13982,  1042, 29491,  1619,  4598, 21334,  1066, 21533,  1678,
          1040,  1058,  2253,  1153]], device='cuda:0')
[2025-05-09 11:45:37] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:37] [INFO] [nan 감지] labels: tensor([0, 0, 1, 1, 0, 0, 1, 1], device='cuda:0')
[2025-05-09 11:45:37] [INFO] [nan 감지] input_ids: tensor([[    1, 13646, 29515,  1183,  6800,  1070,  1040,  4649,  1171,  1066,
          8423,  1281,  1032,  6046, 14663,  7376,  2212,  1365,  1128,  1273,
         21849,  4268, 28800,  1072,  3720,  2100,  1057,  3221, 29477,  4384,
          1640,  1046,  1093, 29530, 29509,  1377,  4257, 24360,  1117,  1137,
          3720,  2100,  1219, 29509,  1117,  2466,  4020,  1067,  1070,  1040,
          2192,  1143,  8205, 28249, 29491,  1119,  1730,  1493, 11872, 29503,
          6539,  1119, 21906,  3664],
        [    1, 29473, 30382,  1006,   930,   947, 30737, 29473, 29783, 29473,
         32750, 30001, 29473, 30325, 30285, 29473, 31433, 30538, 31878, 29473,
         29932, 31491, 29957, 29473, 30245, 30333, 29473, 30245, 31468, 29473,
         30985, 30707, 29955, 29473, 30194, 32353, 30306, 29473,  1006,   939,
           955, 30056, 29473,  1008,   902,   907, 30588, 29955, 29473, 29538,
         29502, 29591, 29473, 29932, 31491, 30351, 29911, 29473, 30245, 29943,
         29473, 31207, 29783, 29929],
        [    1,  1292,  8176, 29473, 30735, 30927, 30367, 30600, 29473, 30083,
         32263, 30194, 30285, 29493, 29473,  1008,   914,   944, 30455, 29916,
         30367, 29473, 30990, 29783, 29473, 30433, 31710, 30468, 29473, 31900,
          1007,   950,   935, 29811, 29491, 29473,  1005,   960,   902, 32170,
         32698, 29783, 29473,  1005,   959,   960, 30306, 29473, 29783, 29473,
         30478, 30547, 29943, 29473, 29518, 29502, 29518, 29538, 32247, 29473,
          1006,   903,   954, 32054],
        [    1,  1299, 29473,  1007,   947,   940, 30050, 29473, 29903, 31878,
         29493, 29473,  1007,   924,   927, 29473,  1006,   910,   962, 29929,
         29473, 30872, 29943, 29903, 29515, 29473, 30778, 30778, 31171, 31850,
          1007,   920,   948, 29955, 29473, 30279, 32231, 29473, 31078, 32109,
         29473, 30778, 30778, 31171, 31850,  1007,   920,   948, 29783, 29473,
          1006,   927,   939,  1005,   949,   900, 30601, 29473,  1006,   904,
           959, 29955, 30616, 30279],
        [    1,  1058, 29522,  4152, 23555,  1261,  2246, 29481,  1274,  1032,
         13324,  1121,  6145,  1676,  1065,  2775,  2359,  9566, 29508, 13915,
          1153,  1068,  1050, 16574, 29484, 13888,  1093, 29691, 29508, 11285,
         29533,  1377, 10848,  1246,  9129,  8335,  1032,  2997,  1122,  1678,
          1058, 29522,  4152, 23555,  1261,  2246, 29481,  2427, 28622,  1040,
         14943,  1122,  9566, 29508, 11285, 29533, 29493,  1146,  1631,  1227,
          3730,  1040,  3764,  1070],
        [    1,  2559,  8423,  1040, 24566,  1070,  5261,  1138,  2605, 12876,
          2075,  1065,  8222, 26102,  1062, 22447,  1942,  1210, 22018,  4867,
         29493,  1146,  1117,  8044,  1066,  8852,  1040,  4813,  1070,  5261,
          1138,  2605, 29501,  1275, 23101,  1928, 10359, 14689, 22447,  1942,
         15683, 29491,  4771, 29493,  1246,  6970,  1032,  1539,  5124,  1137,
          6866, 14277,  6330,  1066,  5432,  1047,  7076,  1072,  4877,  1047,
          7076,  8852, 21698, 15683],
        [    1,  1619,  4649,  7462,  1770,  1040,  9402, 18637,  1070,  1150,
          1383,  1038, 18196,  1181,  7363,  1149,  1093,  2589, 29558, 29499,
         18965,  1065,  1152,  1204,  1054,  5606,  1245, 29473, 29508, 29542,
         29551, 29538,  1066, 29473, 29518, 29502, 29508, 29550,  3441,  3076,
          3738, 22391, 24636, 29515,  4732,  1072,  6459,  8478, 29493,  3737,
          9879,  3697,  1129,  1770, 29493,  1072, 16469, 29491,  1183, 12507,
          1706, 24392, 18725,  5191],
        [    1,  1183, 11135,  1070, 16806, 13006,  1072, 17817,  1574, 13006,
          1122, 12163, 29577, 29481,  1052,  1741, 29492,  1065,  3204,  1427,
          7906, 28727,  1065,  6159,  2035,  3708,  1066,  1639, 14725,  1811,
         29501,  7358,  6860, 29491,  1619,  4598, 21334,  1066, 16395,  1040,
          2165,  1062,  3194,  1072,  7429,  1070,  1935,  6595,  1751,  1089,
          1364,  1122,  1224,  5412, 29491,  1328,  2513,  1066,  3730,  1224,
          9835, 29493,  1032,  2467]], device='cuda:0')
[2025-05-09 11:45:38] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:38] [INFO] [nan 감지] labels: tensor([1, 0, 0, 1, 1, 0, 0, 0], device='cuda:0')
[2025-05-09 11:45:38] [INFO] [nan 감지] input_ids: tensor([[    1,  1619,  4598,  4942,  2071,  1040,  5503,  1070, 22447,  1118,
          1265, 29510,  3667,  7531,  1254, 11599,  1253, 15790,  1093, 17587,
         29558, 29499,  1065, 12706, 29493,  1158,  1930,  1158,  1567,  8990,
         29491,  1150,  1502,  1042,  1072,  1567,  1289, 29489, 27653,  1505,
          1045,  4379,  1072, 20351,  1274,  1518,  8743,  1066, 15241,  3495,
         27325,  1163,  1040, 15790,  2622,  1146,  1171,  1675,  9092,  2169,
         29473, 29508, 29542, 29555],
        [    1, 19127,  1120,  1070,  1974,  1199,  4908,  1088,  9662,  1117,
          1032,  5660,  1808,  1330,  1227, 10174,  1808,  4475,  1065,  1040,
          4867,  1070,  1088,  4369, 18054,  1369,  4795,  9413, 23280, 29491,
          4513,  1199,  4908,  1088,  9662,  1309,  1115, 27792,  1254,  8460,
          1050,  1072, 18652,  1273,  1525,  1215,  2694,  1137,  3415,  1040,
         22199,  9064,  1122,  1119, 17341, 12928, 28737, 16430,  1072,  1088,
          4369,  1080, 19701, 14596],
        [    1,  1619,  4598, 13076,  1137,  1040,  4000,  1124, 23675,  6416,
          2605,  1065,  1040,  1792,  1696,  1070,  1040,  1822,  6780, 26798,
          1117, 19333,  1056,  1122,  5688,  1072,  3077, 22784,  1245,  1535,
          3976,  1066,  1115,  2971,  1066,  3730,  1040, 26798, 12234, 29491,
         26729,  7239,  3296,  3976,  1093, 29478, 29491, 29474, 29491,  6065,
          1263, 11350,  1325,  3321,  1163,  4100,  1072,  4867,  1147,  6782,
          1263,  1066,  1032, 22018],
        [    1,  1619,  4598,  6080, 16932,  1546,  1040,  6860,  1070,  1074,
         19840,  1092,  1241, 13082,  5855,  1124,  1169,  4154, 29516,  7715,
         29478, 16775,  1087,  1114,  6600,  3719,  1323,  7123,  2181,  1032,
         10605,  5199,  1070, 22199, 29493, 11384,  1072, 15341,  6411, 29491,
          2559, 12186,  1224,  3466, 29493,  1164,  1065, 29501, 23353,  2992,
          2997,  1171,  6970, 10987,  1845,  2509, 12928, 28737, 26459,  1158,
          1930,  1158, 23513,  1148],
        [    1,  1299, 29473, 31637, 30478, 29919, 29501, 31093,  1007,   949,
           931, 29919, 29493, 16875, 29473,  1008,   902,   903,  1008,   920,
           952, 29473,  1006,   904,   959,  1006,   929,   899, 29916, 29473,
          1008,   927,   923, 32021,  1006,   937,   947, 29473, 31547, 32231,
         29955,  1008,   929,   923, 29473, 29508, 31502, 29473, 30421, 30668,
         29473, 31544,  1006,   932,   931, 30652, 29473, 31078, 32109, 16875,
         29473,  1008,   902,   903],
        [    1, 29473, 30062, 32231, 32376, 29473, 30402, 30368, 29473, 30073,
         31858, 29473, 29929, 30547, 31055, 29473, 30626, 30894, 29473, 29783,
         30584,  1006,   949,   919, 29473, 30512, 30584, 30197, 29962, 29811,
         29491, 29473, 31793, 30421, 29904, 29943, 29473, 30512, 30584, 30515,
         29493, 29473, 30382, 30056, 30279, 29473, 30668, 31338, 31171, 30482,
         30989, 29473, 30512, 30584, 29473, 30595, 31012, 30985, 29576, 29473,
         31547, 31217, 32464, 30056],
        [    1, 29473, 31744, 32578, 30338, 29962, 29811, 29491, 29473, 32464,
         30782,  1006,   910,   923, 29493, 29473,  1007,   929,   931,  1005,
           956,   915, 30525, 30285, 29491, 29473, 30294, 29903, 29473, 30871,
         30766, 29473, 30609, 31099, 29473, 32021,  1007,   919,   899, 30238,
          1006,   937,   951, 30601, 30285, 29491, 29473, 30382, 30657, 30073,
         29473, 29929, 31099, 29473, 29783,  1006,   930,   947, 29473, 31207,
         29957, 29473,  1006,   938],
        [    1,  1133,  5382, 15002, 11701, 29515,  1183, 18367,  1070, 10005,
          1091, 16795,  1199,  1050, 26751,  2332, 21826,  1120,  1093,  9387,
          4863, 29499, 10572, 11120,  1066,  1451,  6071, 29491, 28356,  1309,
         29479,  3445,  6330,  1072, 25759,  1274,  1518,  9046,  1158,  1032,
          3667,  1066,  1032, 15129,  4916, 19530, 28908,  1072, 10789,  3408,
          1396,  4024, 29478,  2929,  1070,  1040,  7651, 29491,  1183,  1706,
          1070, 13403, 22173,  4863]], device='cuda:0')
[2025-05-09 11:45:39] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:39] [INFO] [nan 감지] labels: tensor([0, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')
[2025-05-09 11:45:39] [INFO] [nan 감지] input_ids: tensor([[    1,  6178,  1312,  1567,  6046, 29501,  1810,  1159,  1167,  4152,
          8460,  7680, 29493,  6638,  1394,  1129,  7680,  9038,  1401,  2938,
          9436,  1093,  1035,  3227,  2938,  9436,  1501,  2362, 29481,  3571,
          1163,  1032,  5610, 15079,  1072, 11912,  1081, 16228,  1124,  1458,
          1358, 18437,  1114,  1420,  8460,  1050,  3420,  1293, 29491,  3442,
          6147,  7808,  1122,  3711, 29501,  5628,  4577,  1056,  1070,  6638,
          1394, 16727, 14575, 29481],
        [    1,  1183,  6305,  1070,  1150, 29550, 29527, 29508,  8460,  7680,
          1066,  5012, 14413,  6229,  2813,  1072, 19751, 16556,  1065, 27325,
         20434,  1427,  8102,  1566,  3296, 11632, 16013, 29491,  1619,  4598,
         16658,  1032,  7860,  5199,  1122,  7167,  1040,  4673,  1070,  6525,
         24971,  1072,  1420, 12585,  1066,  3207,  9139, 25898, 29491, 10859,
          1164, 15361,  1576,  1350,  1072,  6411, 29493,  1146,  1117, 18561,
          1137,  6525, 24971,  1245],
        [    1,  1133,  5382, 15002, 11701, 29515,  2559, 17982,  1040,  4193,
          1070,  7659, 29501, 29475, 18507,  3088, 10563,  6595,  1065,  3958,
          2712,  4552,  7651, 29493,  1072,  1066, 12186,  1040,  1960,  1070,
         11270, 16365, 29501,  3327,  8188,  1065,  1879,  4037,  7659, 29501,
         29475, 18507,  3088, 10563,  6595, 29491,  1119, 21906,  3664, 29503,
         29515,  1619,  6411,  5426,  3958,  2712,  4552,  7651,  1163, 12756,
          1031,  3088, 10563, 29491],
        [    1,  1183,  6215,  4930,  1032,  4649,  1070,  1040,  8783,  1070,
         12611, 14183, 17656,  2605,  9077,  1093,  2862,  6077, 29481, 29499,
          1124,  7651,  1163, 14413, 25583,  1461,  1274,  9072,  4909, 11569,
         26302, 25400,  1250,  7668,  2181,  7312,  3370, 18164,  1178, 29476,
          1054,  1062, 12876, 29491,  4954,  3553, 12580,  1245,  1224,  1980,
          1070, 12022, 10334,  3427,  4024, 29478,  5191, 19979,  2550,  1120,
          1137, 21545,  1420, 11608],
        [    1,  1183,  6817,  1070, 25894,  1044,  6969, 29477, 22710, 16304,
         29484,  1125,  1062,  1169,  2316,  7363,  1149,  1093, 29511,  3073,
         29533, 29558, 29499,  1427,  3495,  1032,  3782,  4294,  3296,  6251,
         29493,  1716,  1370,  2899,  2328, 11357,  1072, 10446,  1142,  1397,
         22660, 29491, 10959,  6159,  5865,  1065,  1040,  7167,  1070,  3624,
          2713, 11704,  1070,  1224, 15790, 29510, 29481,  5461,  1072,  2179,
         10829, 29493,  2636,  3493],
        [    1,  1299, 29473, 31155, 31736, 30004, 29493, 29473, 31104, 31433,
         29473, 32074, 30194, 29473, 29929, 31371, 29916, 29473, 29518, 32012,
         30525, 30584,  1007,   919,   940, 29473, 30368, 31850, 29904, 32486,
         29473, 30512, 29911, 29955, 29473,  1007,   905,   915, 32038, 29473,
         29929, 32644, 29473, 29518, 30325, 30880, 29473,  1006,   938,   956,
         32611, 30050, 29473, 31104, 31433, 29911, 29473, 32462, 29473, 32074,
         30194, 29968, 29473, 30197],
        [    1,  1299, 29473, 30421,  1006,   953,   900, 29473, 31104, 31433,
         29473, 32074, 30194, 29929, 29493, 29473, 31104, 30083, 31301, 29473,
         30512,  1008,   922,   923, 29473, 29518, 29502, 29502, 31171, 29473,
         31481, 29903,  1869, 29473,  1007,   956,   939,  1006,   943,   952,
         29473, 31341, 29473,  1007,   907,   939,  1005,   949,   939, 31113,
         29473, 30512,  1008,   922,   923, 29473, 29929, 32644, 29473, 29555,
         31607, 29493, 29473, 30421],
        [    1,  1299, 29473, 31214, 32370, 29493, 29473,  1008,   914,   944,
         30455, 29473,  1007,   929,   919, 30782, 29473, 31341, 29473, 29538,
         29502, 30183, 29473, 31547, 29903, 30627, 30346, 30083, 31301, 29473,
         30668, 31155, 29473, 31214, 32370, 29473, 30001, 30543, 29943, 29473,
          1005,   954,   956, 32755, 30050, 29473,  1008,   914,   944, 30455,
         29911, 29473, 30092, 30050, 29473, 32074, 30194, 29473, 32750, 30588,
         29903, 29473, 32755, 31518]], device='cuda:0')
[2025-05-09 11:45:40] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:40] [INFO] [nan 감지] labels: tensor([1, 1, 0, 0, 1, 0, 1, 0], device='cuda:0')
[2025-05-09 11:45:40] [INFO] [nan 감지] input_ids: tensor([[    1, 29473, 31078, 32109, 29473, 30279, 30778, 30862, 31113, 29473,
         31461, 30238,  1006,   932,   931, 29473, 31099, 30056, 29903, 29473,
         29508, 29550, 29591, 29968, 29473,  1006,   903,   923, 30201, 29473,
         29508, 29550, 29491, 29552, 29591, 29911, 29473, 30927,  1007,   905,
           930, 32645, 30338, 29962, 29811, 29491, 29473, 29783, 29943, 29473,
         29518, 29502, 29502, 29551, 32247, 29473, 31099,  1007,   927,   952,
         30512, 29932, 29473, 29783],
        [    1,  1299, 29473, 29904, 30463, 31481, 32347, 29473, 30194, 31900,
         30325, 31123, 29493, 29473, 29783, 30294, 29473, 31287,  1007,   925,
           948, 29473,  1007,   908,   960, 30279, 29473,  1006,   923,   916,
          1006,   923,   916, 29904, 30601, 29576, 29473, 31078, 32109, 29473,
         30194, 31900, 29473,  1008,   907,   943, 30062, 29916, 29473, 30402,
         30050, 29473, 30992, 32755, 29783, 29473,  1006,   905,   917, 30304,
         29929, 30333, 30073, 29493],
        [    1, 29473, 29929, 32644, 29473, 29550, 32247, 29473, 30482, 31093,
         29916, 29473, 31288, 31047, 31258, 30083, 30062, 29955, 29473, 30782,
         30194,  1007,   927,   939, 29957, 29473, 30245, 30333, 29473, 29518,
         29502, 29502, 29555, 32247, 30367, 29916, 29473, 29508, 29538, 29538,
         29542, 30723, 29783, 31359,  1006,   912,   923, 29473, 31207, 29783,
         29473, 29518, 29502, 29502, 29542, 32247, 29957, 29473, 29932, 31357,
         30351, 29911, 29473, 31502],
        [    1, 29473, 30382,  1006,   930,   947, 30737, 29473, 30766, 29943,
         29473, 29929, 31099, 29473, 30661, 30668, 29904, 30004, 32578, 29811,
         30279, 29943, 29473, 31444, 29929, 30525, 29473,  1007,   921,   959,
         30871, 30463, 29473, 30919,  1005,   954,   956, 30919, 30351, 29911,
         29473, 29904, 30759, 29929, 29473, 30455, 31478, 29903, 29473, 30616,
         29943, 29473, 30601, 30285, 29491, 29473, 30294, 29903, 29473, 30549,
         32247, 29473, 31547, 30001],
        [    1, 29473, 30609, 30463, 31083, 30306, 29473, 32291,  1006,   949,
           919, 30050, 29473, 31660, 30515, 30092, 29783, 31359, 30201, 29491,
         29473, 31324, 30183, 29473,  1006,   923,   916, 31604, 30306, 29473,
         30626,  1008,   906,   951, 32109, 29493, 29473,  1006,   923,   916,
         31604, 30306, 29473, 31357, 32755, 29473, 30367, 30004,  1006,   928,
           960, 29493, 29473,  1006,   923,   916, 31604, 30306, 29473,  1007,
           901,   953, 29955, 29473],
        [    1, 29473, 30294, 29903, 29473, 30073, 30333, 32727, 29955, 29911,
         30367, 29473, 30595,  1007,   944,   935,  1006,   951,   935, 29943,
         30737, 29473, 30782,  1006,   949,   911, 32464, 29919, 29955, 29473,
         32102, 30843, 30478, 30707, 29916, 29473, 30245, 30333, 29473, 29783,
          1006,   930,   947, 29473, 32021, 29783, 29473,  1006,   938,   913,
         29783, 29473, 30463, 32397, 29962, 29811, 29491, 29473, 31547, 29903,
         29473, 30421, 30294, 29473],
        [    1,  1619,  4598, 28065,  1066, 17982,  1040,  4526,  2212, 11608,
          1129,  6145,  3312, 10447,  1240,  1065,  5087,  2317,  6149,  1478,
          7358,  1072,  1757,  5008,  1068,  1487,  1172, 29493, 26812, 29501,
          1497,  2019, 29521,  1072, 22791, 29501,  1497,  2862, 29508, 29491,
         28495,  7961,  1274, 28434,  1040,  1198,  4974,  1272,  1062,  1960,
          1070,  2328,  1935,  1525,  1215,  2694,  1158,  5396,  7550,  1502,
          1122,  1040,  4867,  1070],
        [    1,  1183, 20728, 29447,  1263,  5945,  6956, 24729,  1228, 16306,
         29493,  1040, 20728,  6065,  1263, 11350,  1309,  1115,  3982,  1066,
          6175,  1040,  1373,  1077,  2408,  1070,  2723, 10305, 12836, 29491,
          9237, 29493, 10677,  1695,  1066, 10989,  1122,  5945,  6956, 24729,
          1254, 29411,  5482,  1263,  3370, 24168, 29493,  1458,  2084, 18005,
          7434, 29501,  6502,  6942,  1093,  1159,  8429, 29499,  1163,  1040,
          6942,  1070,  2630, 25777]], device='cuda:0')
[2025-05-09 11:45:41] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:41] [INFO] [nan 감지] labels: tensor([1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0')
[2025-05-09 11:45:41] [INFO] [nan 감지] input_ids: tensor([[    1,  1299,  1292,  2154, 29505, 29473, 29518, 29502, 29518, 29550,
          1186,  5579, 29511, 29515, 29473, 30072, 31334, 29903, 31293, 30382,
         30657, 32074, 29473, 31195, 30056, 30072, 29473, 29932, 31747, 29955,
         29473, 31761, 29911, 31321, 29473, 29903, 31132, 30233, 29473, 29518,
         29502, 29518, 29550, 32247,  1292,  2154, 29505, 29500, 30050, 31547,
         30001, 30245, 31468, 30894, 29932,  1007,   907,   931, 30584, 29499,
         29916, 30073, 29473, 30661],
        [    1,  6178,  1312,  8460,  7680, 29493,  3698,  5261,  1138,  1341,
         29490, 10143,  8460,  7680,  1093, 29537,  6810, 29481, 29499,  1072,
          1420,  3181,  1148,  1073,  1076,  1217, 16727, 22436,  2348,  6638,
          9662,  1065,  2513,  1066,  1080, 13880,  1072, 29493,  3095,  7531,
         29493,  1401,  8460,  1362,  1695,  1066,  8558,  1066,  6817,  1066,
          1401, 14496, 29491,  3725, 10305,  3467,  1040, 15790,  1066,  4661,
          1040, 28909, 26751,  2332],
        [    1,  1183,  4867,  1070,  5829, 15262,  7808,  1122,  3698,  5261,
          1138,  1178,  1308,  1090,  2184,  1056,  1066,  1643,  2343,  7651,
          1546, 25556,  8798,  1716,  1090,  5221,  1117,  2825,  7929,  4095,
          2100, 13006, 29491,  3761, 29493,  1476, 26992,  5199,  1427,  1518,
          6970,  1122,  1040,  4649,  1070,  4429,  1082,  3844,  1463, 15734,
         19025,  1163,  3376,  5398,  1072,  7184, 24170, 24007,  8798,  1679,
         29478,  9544, 29491,  1584],
        [    1, 24651, 12314,  6810,  3023, 29515, 15478,  1047,  2613, 29501,
         28137, 11748,  2071, 11607,  7184,  7729,  1054,  1072,  5261,  1138,
          1118, 16025,  1208,  4537,  1047, 15104,  1274, 16956,  1158, 23217,
         14713,  1066, 17900,  3698,  5261,  1138,  1341, 29490, 10143, 15790,
         29501, 29508,  1093, 29537,  6810, 29501, 29508, 29499, 18965, 29491,
          1119, 21906,  3664, 29503, 29515,  6773,  3299,  1180, 20362,  2613,
         16775,  5890, 29481, 16855],
        [    1,  1086, 11455,  2730,  1117,  1164,  3046, 26605,  1122,  3698,
          3296, 29493,  4313,  1504,  1117,  2077,  7284,  5556,  1452,  1040,
         14943,  1070,  1639,  3760,  1124,  1040,  2955, 29491,  1183,  6800,
          1070,  1224,  4649,  1171,  1066, 17982,  1040,  4673,  8298,  1037,
          2730,  1072,  1639, 13082,  5855,  1093,  3093,  1037,  1178,  5007,
          1894,  1325,  1924,  1065,  1751, 14390,  4429,  7229,  1120, 29491,
          2031,  1224, 29493,  2349],
        [    1,  1619,  4649,  1289,  1883,  1508,  1032,  3946,  1826,  5316,
          1120,  1065,  1040, 15062,  3108, 29508, 17966,  1072,  1639,  4673,
          1065, 14774,  1050, 29493, 21962, 22836,  1513, 29493,  1072,  1045,
         28735, 25329,  1050,  1265, 29491,  2559,  1224,  1716, 29493,  1164,
          1167,  4152,  6099, 11463, 11876,  1171,  2075,  1124,  1032,  8092,
          1070,  2480,  7651,  1163,  1935, 25329,  1050,  1265, 29491, 18180,
         11692,  1137,  1358,  5180],
        [    1,  2752, 12685, 11983,  1473,  1052, 28819, 10346, 29474,  1117,
          1392,  1070,  1040,  1848,  4066,  3419, 18652,  1273,  3207,  1244,
          1364,  1070,  1040, 11608,  1129,  6145, 25948, 29493,  4780,  1065,
          7651,  2212, 29473, 29550,  1072, 29473, 29538, 29502,  2035,  1070,
          4363, 29491,  1429,  1761,  1115, 19916,  1206,  1032, 10870,  2254,
          5106,  1065,  1040,  2611, 29501, 11155, 29501,  1130,  1604,  2874,
         14773,  1070, 17530, 10356],
        [    1, 29473,  1007,   901,   959, 30233, 31321, 30192, 29783, 29473,
         29904,  1006,   928,   960, 30515, 29916, 29473, 30919,  1005,   954,
           956, 30919, 30351, 29911, 29473, 30402,  1007,   928,   916, 29904,
         32486, 29473,  1008,   907,   943, 30062, 29473, 29932, 30652, 29968,
         29473, 30294, 30778, 29904, 29932, 29473, 30512, 30194,  1181, 13120,
         29473, 29919, 29919, 30989, 29968, 29473, 30092, 29904, 29904, 29943,
         29473, 30843, 30001, 29957]], device='cuda:0')
[2025-05-09 11:45:42] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:42] [INFO] [nan 감지] labels: tensor([0, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')
[2025-05-09 11:45:42] [INFO] [nan 감지] input_ids: tensor([[    1,  2031,  1040, 19181,  1070,  1032, 24128,  4689,  1122,  1354,
         29485,  1366,  3209,  1273,  7786, 29493,  1032,  1512,  1070,  1040,
          1049,  2288, 29509, 17966,  1171, 17169,  2567,  1072,  6099,  5465,
          1245,  9605,  2712,  9366, 29473, 29518, 29555,  1354, 29485,  1366,
          3209,  1273,  7786,  1158,  1930,  1158, 29473, 29549, 29542, 15961,
         10545,  1770, 29491, 23484,  1404,  3858,  3205,  1065,  1040, 17169,
          2567, 11192,  1070,  1040],
        [    1,  1183,  6215, 22276,  1232,  1967,  1030,  1513, 28571,  1070,
          9955, 29501, 29511,  1334,  1037,  2571,  1822,  6780,  7363,  1149,
          1328, 12032,  1065, 19167,  2317,  1072, 13476, 29510, 11005,  1040,
          2636,  6334,  1452,  1040, 15961, 18782,  1070,  1822,  6780, 15790,
         18965,  5769,  1054,  2472, 10337,  1346, 29493, 10916, 15276,  1946,
          1245,  7961, 13584,  1065, 14497,  5120, 29491,  1183,  5192,  1171,
          4835,  2624,  1254,  1040],
        [    1,  1133,  5382, 15002, 11701, 29515,  1328,  1040,  3377,  2432,
          2035, 29493,  6632,  5865,  1427,  1518,  2037,  1065, 23126,  5096,
          1203,  1394, 28908,  2605,  1206,  1040, 15052,  1039,  4223, 29493,
         12932,  1573,  1040, 10014, 15922,  8770,  1070,  1040, 15052,  1039,
         29493,  1072, 23126,  5096, 15052,  1039,  6825,  1072,  2192,  1297,
         23083, 29491,  3761, 29493,  2055,  2864,  1894, 29516,  1154,  1215,
          2694,  1274,  1040, 27975],
        [    1, 24031, 29501, 21178,  4609,  1673,  1228,  4455,  1163, 26169,
          1065,  1851, 29501, 29503,  1680, 23522,  8478, 29513, 24392,  1134,
          1831,  1228, 13343, 28758,  1054, 29493,  3708,  1066,  2254,  6925,
          1070, 27986, 29491,  3761, 29493,  4294,  9431,  3003, 12876,  1279,
          1227,  2685, 27986, 29491,  1584,  1706,  6297,  5926,  1382, 11472,
          1245, 29473, 29542,  9505,  7238, 29516,  1540, 29481,  1066,  2997,
         27986,  1065, 17676,  1361],
        [    1, 17530,  3848, 19025,  1761,  6817, 14867,  1827, 20087,  1072,
          3243, 13038,  4522, 12935, 29493,  4780,  1358,  1228,  1971, 11054,
          1254,  1040,  3191, 23363,  7871, 29493,  1927,  1401, 11748,  2071,
          1309,  1115,  2037,  3400, 29491,  5853, 23363, 19025,  1761,  6817,
         14867,  1158,  1673,  4019,  2349, 25758, 20799,  1072,  1228,  1065,
         16921,  4522,  1163,  3431, 29491,  1584, 14147,  1032,  4356, 25758,
          4522,  2997,  1122,  1164],
        [    1,  1619,  4598,  1343,  8831,  1040,  5396, 12585,  1070,  1040,
          1249, 18072, 29474,  1098,  3322, 29518, 29501, 10969, 25662, 29508,
         29501, 29555, 10340, 29523,  1061, 11790,  1066,  1117,  1399, 13224,
         15452, 28307,  1072,  6595, 29491,  2459,  1399, 13224, 15452, 29493,
          1458, 21545, 11844, 16013, 29493,  9449,  1066,  6433,  1077, 24019,
          5203,  1065,  7155,  5461,  1072,  1676, 29513,  6652,  3486,  1056,
          8426,  3207,  2272,  1122],
        [    1, 29473, 30382, 30657, 30073, 29473, 29783, 29473, 30543, 30985,
         29916, 29473, 30402, 30194, 30073, 29473, 30921, 29903, 30919, 30092,
         29473, 29919, 30083, 29968, 29473, 29904, 30279, 29473, 30372, 29943,
         30737, 29493, 29473, 29783, 31207, 30306, 29473, 29929, 31099, 29473,
         30595, 29932, 29916, 29473, 30402, 30194, 30073, 29811, 30649, 29473,
         31214, 29783, 29929, 29473, 30245, 32578, 30338, 29962, 29811, 30001,
         30927,  1008,   902,   903],
        [    1, 24396, 16811,  1066,  4610, 27990, 29493, 11973,  2317,  1228,
          3376,  1206,  1164,  8251,  5391,  1122, 26169,  1072,  8460,  1050,
          1168,  4380, 18196, 29493,  2622,  1358,  1761,  1227,  1115,  8996,
          1163,  1040,  3296,  3140,  2355,  1210,  1115, 11405,  1070, 20407,
          1065,  6065,  1263, 24729, 29491,  1328, 16069,  1070,  1224,  5059,
         29493,  4770,  1081, 29565,  4948,  2145,  1032, 10957,  9813,  1124,
         22093, 26169, 29493,  1150]], device='cuda:0')
[2025-05-09 11:45:43] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:43] [INFO] [nan 감지] labels: tensor([0, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')
[2025-05-09 11:45:43] [INFO] [nan 감지] input_ids: tensor([[    1, 29473, 30083, 32158, 30245, 29811, 29473, 31287, 29473, 29783,
          1006,   931,   906, 30601, 29473, 30919,  1007,   931,   907, 30050,
         29473, 30382,  1006,   930,   947, 29473, 30782, 32102, 29957, 29473,
         29904, 29932, 29943, 29473, 30201,  1006,   931,   952, 29811, 29493,
         29473, 30766, 30367, 29473, 30382,  1006,   931,   906, 30601, 29473,
         30461, 31518, 30538, 29962, 29811, 29491, 29473, 30382,  1006,   930,
           947, 30737, 29473, 29783],
        [    1,  1102,  3362,  1082,  1050,  1218,  6710,  1218,  1093,  5962,
         29499,  1117,  1032,  4997, 19068, 13343,  2187,  1065,  5868,  1057,
          5312,  1072, 14850, 29491,  1328,  1224,  4649, 29493,  1246, 19876,
          2100,  1137, 10229,  1597,  1115,  2075,  1158,  1164,  6413,  8222,
         26102,  1062,  8841,  1122, 21301,  2603,  1484,  1142,  2243,  1117,
         23932,  1283, 29516,  1035, 25431,  4391,  1093,  5367, 10340,  1275,
         23101,  2938,  7312, 29491],
        [    1, 29473, 29783, 30781, 29473, 29811, 31647, 29473, 31539, 29473,
         31684, 29919, 29903, 29473, 31894,  1006,   901,   924, 29473, 32464,
         29473, 31951,  1006,   953,   900,  1008,   923,   916, 30394, 29932,
         31099, 29473, 29518, 29502, 29502, 29502,  1007,   921,   952, 29473,
         29903, 29929, 30279, 29473, 31055, 30875, 32677, 29473,  1007,   912,
           939, 31031, 29473, 30665, 29811, 30279, 29473, 30245, 30595, 29929,
         30279, 30285, 29491, 29473],
        [    1,  1619,  6215, 16658,  1032,  1757, 29501, 14358,  9835,  1736,
          4340,  1990,  4649,  1070, 18501,  2385, 24079,  1245,  1040,  3668,
          1070,  1186, 20528,  7077, 29493, 29223, 29491,  1183,  6847, 29501,
          1071,  3337,  9835,  1736,  4340,  5199, 13341,  1814,  1164,  1289,
          9322, 24081,  4807,  1076,  5936, 11490,  1066,  9124, 21483,  1546,
          1040, 11237, 18637,  1072, 14585, 13584,  1254, 16102,  1245,  1040,
         11307, 29491, 10859, 29466],
        [    1,  1183, 11608,  1129,  6145,  2355,  1117,  7079, 18276,  1066,
         18652,  1273,  1065, 24266, 29493,  3260,  1040,  7167,  1070,  1678,
         18652,  1273,  8807,  2605,  2427,  5856, 15790, 29501,  1275, 23101,
          3045,  6388,  1056,  1065,  3204,  1032,  8044,  7767,  1122,  7550,
          1056,  1040,  3193,  1240,  1070, 13982, 29491,  1328,  1224,  4649,
         29493,  1246, 13801,  1066, 16395,  1040,  4526,  2212,  3191,  1896,
          4429,  7229,  1120,  1072],
        [    1,  1619,  4649, 18707,  1066,  8852,  1040,  6305,  1070,  1157,
         29490,  2889, 29476, 10691,  1093, 16542,  8475,  3263,  5860,  1161,
          3742,  4840,  1056,  1040, 29473, 29551, 29550, 29528,  1093, 11767,
         29508, 29552, 29502, 29542, 29485, 29499,  3493,  4115,  1070,  2752,
         29485,  1366,  3209,  2730,  1950,  2730,  1851,  1654, 29491,  1708,
          1038, 18507,  3088, 10563,  1093,  3214, 29499,  1066,  1407, 19142,
          1811, 29501,  4849,  1056],
        [    1,  1299, 29473, 30402, 30668, 29473, 30778, 31928, 29493, 16875,
         29473, 30092, 30782, 29473, 31038, 30245, 29473, 30368,  1006,   929,
           952, 30306,  1232, 30368, 31217, 29510, 29572, 29473,  1005,   956,
           909, 29783, 29473, 30372, 29943, 29473,  1006,   904,   959, 29955,
         29473, 31195, 30285, 29473, 31078, 32109, 29473, 30402, 30668, 29473,
         30778, 31928, 29473, 30795, 29473, 29904, 30463, 29911, 16875, 29473,
         30092, 30782, 29473, 31038],
        [    1,  1619,  4649, 26249,  1040,  1706,  1070,  5406, 10468,  3223,
          1167,  4152,  6282,  1122,  1040,  5653,  1072, 15965,  1070, 13915,
          1034,  2253,  2687, 29501,  1179, 11911,  1093,  9042, 29499,  8460,
          1050, 26169, 29501, 29508,  1058, 29522,  4152, 10658, 29491, 10468,
          3223,  1167,  4152, 12928, 22417,  1228, 14767, 25292,  1066,  7830,
          7735,  1070,  5555,  2605,  1864,  1358,  1228,  7184,  1373, 11911,
          1066,  1249, 18072,  2479]], device='cuda:0')
[2025-05-09 11:45:44] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:44] [INFO] [nan 감지] labels: tensor([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
[2025-05-09 11:45:44] [INFO] [nan 감지] input_ids: tensor([[    1,  2386,  1126,  1489,  1050, 18196,  9118,  3593,  4429,  7229,
          1120,  1070,  1040,  7155, 29491,  1328,  3720, 29493,  1224,  4429,
          7229,  1120,  1309,  1972,  1245,  1032,  8460,  1050,  1210, 18652,
          1273, 18965,  1065,  1040,  7155,  4605,  1210,  6488,  7076,  1245,
          1032, 14229,  4795,  9413,  2570, 11053,  2603,  1164, 18965,  1210,
          1032, 15052,  1039,  1065,  1040,  2614,  1070,  1040,  2955, 29491,
         15630,  1224, 29493,  1249],
        [    1, 29473, 29929, 32644, 29473, 29551, 31607, 29473, 30001, 30543,
         29473, 30245, 30279, 29916, 29473, 29955, 29904, 30333, 29473, 30489,
         31747, 29473, 30661, 30001, 29916, 29473, 32127, 31647, 29473, 30489,
         30990, 31812, 29473, 30489, 30543, 32337, 29473,  1005,   954,   899,
          1007,   947,   940, 32249, 30612, 29968, 29473, 30985, 32353, 31444,
         30338, 29962, 29811, 29491, 29473, 30334,  1006,   918,   928, 30489,
         29473, 30489, 29919, 31481],
        [    1,  1102,  1285,  2006,  1070,  1164, 10545,  1148,  1070, 10524,
          1062,  1193,  2730,  2197,  1319,  6600,  1151,  1731, 29491, 14979,
          1178,  2730, 29493,  8133,  1245, 11909,  4896,  1421,  1065,  1164,
          3466,  1070,  1133,  1861,  1044,  1093,  1184, 11539, 29499,  1203,
          1489,  2274,  1266, 29492, 29589, 29558,  6076,  2934, 29493, 23158,
          1283, 29589,  4305, 19435, 22095,  1080,  3108, 12756,  1866,  2282,
          1362,  1507,  5042, 29501],
        [    1, 12955,  1044, 29501, 20347, 12542,  1030,  1093,  6721, 29526,
         29499,  1117,  1032,  1102, 29501,  1891,  1969,  1151, 12542,  1030,
         29493,  1458,  1117,  8529,  1066,  1924,  1164,  3046,  4673,  1065,
          1040,  5551,  1148,  5261, 23280,  2603,  1032,  7445,  1070,  3207,
          1244,  1364, 29491,  1119,  8920,  1309, 13133,  1066, 10752, 17478,
          2317,  1070,  1032,  6103,  7445,  1070,  8585,  9742, 11589, 29493,
         14982,  1421,  1474,  1072],
        [    1, 29473, 32725, 32474, 31970, 29962, 29811, 29491, 29473,  1008,
           923,   956, 30004, 29473, 32109, 30657, 29916, 29473, 31433, 30735,
         30072, 31012, 31321, 29473, 30183, 29911, 29473, 30778, 31083, 29957,
         29473,  1008,   906,   951, 31660, 30050, 29473, 30880, 30543, 29473,
         30626, 30894, 29783, 29473, 31502, 30723, 29473, 31928, 31418, 31047,
         30591, 29968, 29473, 30826, 32519, 30194, 30073, 29473, 30543, 31055,
         29783,  1006,   918,   928],
        [    1, 29473, 30382, 30056, 30279, 29473,  1007,   957,   931, 31061,
         29903, 29473, 29929, 31099, 29473, 29903, 31710,  1005,   947,   919,
         29811, 29903, 29473, 29811, 30004, 29473, 31159, 31324, 30609, 30927,
         30050, 29473, 31207, 29783, 29473, 29549, 30609, 29473, 29555, 29502,
         29502, 29502,  1007,   921,   952, 29473, 30001, 30367, 29473, 32135,
         29962, 29811, 29491, 29473, 30382,  1006,   930,   947, 30737, 29473,
         30478, 30294, 29943, 29473],
        [    1,  1133,  5382, 15002, 11701, 29515, 11795, 29501,  1091,  1132,
          2128,  1052, 28819, 10346,  1093, 14474, 29499,  7716, 26744,  1072,
          3716,  3493,  1089,  3898, 29494,  1273, 13006, 29491,  3761, 29493,
          1040,  4367,  1050,  3207,  9139,  1117, 10334,  9806,  1206,  1040,
          2073,  1507,  7659, 29501,  1030,  3561,  1263,  8222, 26102,  1831,
          2348,  1115, 28854, 29491, 12819,  8787, 14374,  1033,  2196,  2639,
          1245, 13604,  1946, 21740],
        [    1, 29473, 31362, 30515,  1006,   910,   923, 29493, 29473, 30279,
         32282, 29473, 31321, 30368, 30062, 29955, 29473, 30083, 30279,  1007,
           927,   939, 29916, 29473, 30402, 30194, 30073, 29473, 32727, 29955,
         29968, 29473, 29904, 32578, 30338, 29962, 29811, 29491, 29473, 30455,
         30056, 30463, 30468, 29955, 29473, 30279, 32282, 29473, 31321, 30368,
         30333,  1008,   922,   907, 29473, 30334, 29929, 30062, 29955, 29473,
         30368, 31547, 29473, 32291]], device='cuda:0')
[2025-05-09 11:45:45] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:45] [INFO] [nan 감지] labels: tensor([0, 0, 0, 1, 1, 0, 1, 0], device='cuda:0')
[2025-05-09 11:45:45] [INFO] [nan 감지] input_ids: tensor([[    1, 29473, 30382,  1006,   930,   947, 30737, 29473, 31479, 31780,
         29473, 30627, 30001, 32750, 31660, 29473, 31293, 30482, 30062, 30707,
         30306, 29473, 29508, 29502, 32247, 29473, 30482, 31093, 29473, 29508,
         29552, 29502, 30525, 29473, 30584, 30402, 29473, 31288, 31099, 29783,
         29473, 30050, 29473,  1008,   916,   959, 30367, 29473, 30415, 31859,
         30279, 29473, 30372, 29929, 29473, 30872, 30338, 29962, 29811, 29491,
         29473, 29518, 29502, 29508],
        [    1, 24651, 12314,  8906, 29515,  2559, 16395,  4886, 10878,  1070,
          9003, 10116,  2259,  1885,  1081,  1039,  1072,  1420,  6179,  4120,
         13816,  7651,  1065,  2200, 29473, 29518,  1105, 29491,  1119, 21906,
          3664, 29503, 29515,  9105,  3505,  9292,  4649,  1070, 29473, 29552,
         29555,  1885,  1081,  1039,  5237,  1065,  9003, 10116,  2259,  4363,
          2839,  1093,  3979,  6718,  1066, 29473, 29508, 29551,  1105,  1325,
         13816,  1066,  1040,  7714],
        [    1,  1183,  8525,  1070,  3698, 20821, 17759,  1693, 19025,  1093,
         29517, 24797, 29499,  1228,  1454,  5741,  8607, 29493,  1163,  8460,
          7680,  5784,  1845,  1065,  6875, 28222,  1741,  1070,  3598,  5136,
          1093, 29474, 29491, 29489, 29491, 26169, 29493,  1181,  3097, 29476,
         29493,  1086,  1855, 29503,  4615, 29508, 29584, 29538,  1377,  7373,
          9277, 12301,  1070,  8460,  1050, 19412,  1065, 28811,  1072, 17478,
          2317,  1070,  6821,  4661],
        [    1,  1619,  4649, 27529,  1040, 11893,  1066,  1458,  5934, 24152,
          1070, 22317, 17350, 12744,  5936,  1065, 14229,  6130, 29491, 18180,
         11692,  1137,  4031,  1056,  9577, 14577, 29493,  2027,  1158, 14503,
         29481, 29493, 11274, 29493,  1072,  5459,  1465,  1228,  7079,  6413,
          1206,  7256,  6484, 16381,  1163,  3131,  3804, 29491,  3231, 28466,
          3806, 29501,  1034,  5936,  9789,  1163,  7921,  1210, 17236,  6282,
         29493,  4335,  1422,  3126],
        [    1, 29473, 31078, 32109, 29473, 31104, 31433, 29783, 29473,  1007,
           929,   937, 30304, 29929, 30333, 30073, 29473, 31104, 31433, 29473,
         31113, 30499, 29473, 30515, 30627, 29916, 29473, 30402, 30050, 29473,
         30992, 32755, 29783, 29473,  1006,   905,   917, 30304, 29929, 30279,
         29473, 30372, 30338, 29962, 29811, 29491, 29473, 29783, 31012, 30050,
         29473, 30346,  1008,   924,   940, 29957, 29473,  1007,   920,   904,
         30192, 30050, 29473, 30083],
        [    1,  1862, 23862,  1070,  1040, 14586,  1070,  1040, 13139,  1562,
          1323,  8418, 11053, 29493, 19753,  1323,  8418, 11053, 29493,  1776,
         29501, 29481,  1237,  2506,  8536,  1080,  3227,  1072,  1954, 29920,
          1080,  2024,  1323,  1117,  2846, 29491,  1183,  5761,  1070,  1935,
          6330,  1122,  1040, 22735,  1070, 13768,  4007, 17759,  1693,  1072,
          1327,  7896,  1396, 19025,  1158,  1930,  1158,  1122,  1567, 24128,
         11468,  1117, 10719,  1072],
        [    1,  1619,  5657, 29501,  6295,  4649,  7462,  1770,  1040,  3340,
         14663,  2212,  1789, 29501, 29495,  5013, 11778,  1072,  3026,  3932,
          1070,  1951,  2491, 11608,  1129,  6145, 25948, 18965,  1093,  1855,
          4141, 29499,  1072,  5533, 12177, 29501,  4859, 13982,  1093,  3002,
         29505, 29499,  1065, 12879, 29491,  1183,  7271,  1171,  1066, 10916,
          1099,  1040, 20104,  6860,  1070,  1164,  4003, 29510, 29481,  1776,
         29501, 10914,  1054,  1789],
        [    1,  1328,  1902,  9024,  2934, 29501,  4859, 13982,  1093,  3002,
         29505, 29499, 26914,  1117,  3046,  1066,  9819, 15860,  1845,  1072,
         20821, 29516,  1035,  1134,  1031,  3848,  1885,  2374,  1072, 15382,
         24392, 18725,  5191, 18637, 29491,  1183,  2937,  4649, 18707,  1066,
          2879,  1164, 11997,  6522,  1070,  1040, 29473, 29518, 29502, 29508,
         29518, 29584, 29518, 29502, 29508, 29538,  1083,  5630, 26798,  1065,
          1102,  1502,  2054,  1254]], device='cuda:0')
[2025-05-09 11:45:46] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:46] [INFO] [nan 감지] labels: tensor([1, 1, 0, 0, 1, 1, 1, 0], device='cuda:0')
[2025-05-09 11:45:46] [INFO] [nan 감지] input_ids: tensor([[    1,  1183,  4709,  1070,  1224,  6215,  1117,  1232, 29511,  7749,
          1513,  1901,  7816,  1072,  2740,  2553,  1091,  5191, 24542,  1163,
          1065,  6812,  2992, 15962,  1124,  2512,  1038,  1272,  7159, 13698,
          1193,  1153,  1151,  8610, 29483, 29510,  1072,  7462,  1770,  1040,
          8629,  2243,  6860,  1070,  1032,  7786,  1137,  4203,  1245,  1040,
          2773,  1268,  3344,  1062, 20557, 29493, 15241,  3419,  1158,  1181,
         29491, 13698,  1193,  1153],
        [    1,  1299, 29473, 30627, 30172, 31061, 30092, 29493, 29473, 29549,
         30525, 29473, 31850, 31012, 31346, 29929, 29473,  1008,   914,   944,
          1006,   928,   960, 30491, 31346, 29572, 29473, 30368, 30478, 29903,
         30707, 29955, 29473, 30421, 30279, 29473, 31341, 29473, 32204, 30498,
         30499,  1008,   914,   915, 29473, 30004, 30515, 29955, 29473, 31433,
         31038, 30759, 30233, 29473, 31078, 32109, 29473, 32204, 30498, 30499,
          1008,   914,   915, 29473],
        [    1, 29473, 31744, 32578, 30338, 29962, 29811, 29491, 29473, 30895,
         32589, 30083, 31217, 32464, 30515, 29473, 31159, 31246, 29473, 30596,
         30004, 29929, 29916, 30367, 29473, 29518, 29502, 29550, 29502, 32247,
         29473,  1008,   902,   903, 30334, 30795, 32519, 30306, 29473, 31159,
         31246, 29904, 29943, 30737, 29473, 30382, 29473, 30990, 30959, 29916,
         29473,  1006,   938,   929, 30601, 29473, 29518, 29502, 29538, 29502,
         32247, 29473, 31547, 29903],
        [    1,  2297,  1153, 29316,  1228, 20821,  1158,  3046, 18054,  3885,
          1070,  4369, 29501,  8822, 15751,  1254,  3593,  1070,  3716,  1384,
          2821,  1064,  2864,  1894, 29491, 27925,  1065,  1040, 19751, 29501,
         16358,  1207,  1153,  1068,  1050, 15079,  6550,  1761,  1684,  6483,
          3207,  5191,  5099,  1738,  2171, 10005,  1091,  1247,  1866,  7879,
          2295,  2251,  1072,  2379,  1272, 23083,  1117,  1472, 18256, 29491,
          1219, 13491,  7289,  3250],
        [    1,  1299, 29473, 30627, 30172, 31061, 30092, 29493, 29473, 30279,
         30657, 29473,  1007,   926,   899, 31660, 31288, 29916, 29473,  1008,
           927,   923,  1006,   916,   923,  1006,   937,   951, 31346, 29572,
         29473, 31078, 32109, 29473, 30627, 30172, 31061, 30092, 29473, 30004,
         30489, 29903, 29473, 32331, 31158, 30489, 29968, 29473, 30245, 29783,
         32486, 29473, 29518, 30525, 29473, 31850, 31012, 29968, 29473,  1006,
           903,   923, 30201,  1007],
        [    1,  1619,  4598, 16351,  4243,  7715,  1097,  2418,  2605,  1070,
          2268, 29549, 29501,  1072,  1167, 29550, 29501, 29475, 26412, 26169,
         29501, 29508, 20329, 29549, 29501, 29538,  3858,  2317,  1254, 12971,
          1967, 29501, 25891,  1969,  1151,  5476,  1894, 29577,  4930,  1032,
          4649,  4627,  1066,  2137,  1040,  2165,  1062,  3194,  1070,  6595,
          6970,  1122,  6379, 12908,  1134,  1831,  1505, 11417,  3004, 10255,
          1341, 29490, 10143,  7363],
        [    1,  1619,  4598,  4942,  2071,  1040,  2636,  2433, 29501,  1777,
         29501,  2005, 29501,  1212,  1065,  2611, 30326, 19297,  8460,  7680,
          1137,  1274,  1518,  6970,  1158,  8222, 26102,  1062, 12535,  1122,
          1706,  1065, 11357, 29491,  1429,  6080,  1164, 23862,  1070,  1040,
          4886,  9197,  1072, 23338,  1070,  2027, 15790, 29501,  6295, 20492,
         29493,  3258,  1678,  1420,  5610, 16327,  1228, 11797,  4632, 16008,
         29491,  1183,  4598, 23857],
        [    1, 17530,  3848,  8460,  7680,  3672,  1032, 13502,  6251,  1066,
          3698,  3296,  1072,  1066,  2850,  2359,  8990, 29493,  1158,  1451,
          1081,  5465,  1124,  5934, 19091,  1254, 26798, 29481,  1070,  5533,
         12177, 29493,  9523,  1276, 23165,  1072,  1086,  1855, 29503, 29491,
         12076,  4597,  1452,  1040, 19412,  1070,  8460, 29316,  2937,  1065,
          6815,  8004,  7786,  1309,  2504,  1066,  1032,  2641,  7167,  1070,
          1040,  5784,  1070, 20821]], device='cuda:0')
[2025-05-09 11:45:47] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:47] [INFO] [nan 감지] labels: tensor([1, 1, 0, 0, 0, 1, 1, 0], device='cuda:0')
[2025-05-09 11:45:47] [INFO] [nan 감지] input_ids: tensor([[    1,  1299, 29473, 30402, 30368, 29493, 29473,  1006,   912,   951,
         30871, 30824, 31043, 29473, 30455, 30325, 31104, 31047, 29473,  1008,
           923,   916, 30394, 30351, 29911, 29473, 31761, 29911, 31321, 29473,
         30367, 31928, 29473, 31310, 30279, 29473, 30402, 30368, 30004, 29943,
         29473,  1006,   912,   951, 30871, 30824, 31043, 29473, 30455, 30325,
         31104, 31047, 29473, 30985, 31031, 29955, 29473,  1008,   923,   916,
         30394, 29957, 29473, 30512],
        [    1,  1619,  4649, 16658,  1032,  3460, 29501,  6084,  3427,  1070,
         11135,  1070, 10042, 29545, 29501,  1130,  1850,  1244, 29482,  1366,
         25494,  1072, 10042, 29545, 29501, 29533,  1179,  1037,  3938,  1122,
         13001,  4632, 15478,  7327, 29501, 29528,  3422, 15790,  1093, 15376,
         29558, 29499,  1065, 24266,  1072,  1087,  3637, 30326,  1508,  1149,
         30326,  4172,  8798,  1093, 29545, 29558, 13359, 29499,  1065,  7651,
          1163,  1168,  3844,  5191],
        [    1, 29473, 30382, 30056, 30279, 29473, 29783, 29473, 29549, 30525,
         29473, 29550, 29502, 29502, 29502, 30661, 30367, 29473, 29932, 31303,
         30919, 30351, 29911, 29473, 29932, 30782, 30543, 29473, 30778, 31217,
         30584, 30707, 29904, 30279, 29473, 31604, 29783, 29473, 30461, 31518,
         30194, 29473,  1006,   951,   915, 31031, 29473, 30616, 29943, 29473,
         30601, 29473, 29783, 30601, 29473,  1007,   924,   927, 29473, 30382,
         30463, 30871, 29473, 30766],
        [    1,  1119,  2022, 13491, 17478,  2317,  1070, 20232,  6634,  1029,
          1067,  3980,  1031,  2605,  1228,  1930, 23868, 29493,  1330,  2163,
          6058,  1065, 24170, 17013,  1062,  6634, 15845,  1228,  2876,  7083,
          9756, 29491,  1183, 24170, 17013,  1062, 16228,  1070,  9039, 12439,
          1117,  3376, 15014, 13656, 29491, 28316,  1062, 18104,  1090,  1315,
         29501, 29489,  1369,  1093, 15894, 29545, 29499, 12439,  1245, 17207,
          4492,  1199,  2126,  1502],
        [    1, 29473, 29508, 29508, 31885, 29916, 29473, 32227, 30368, 30192,
         30083, 30707, 29904, 30279, 29473, 30992, 32376, 30616, 29943, 29473,
         30595, 31012, 29473, 30083, 31047, 30707, 29783, 29473, 30372,  1006,
           912,   923, 30737, 29473, 30766, 29943, 29473, 30455, 30056, 30463,
         30468, 29916, 30073, 29473, 29783, 29473, 30543, 30985, 29916, 29473,
         30402, 30194, 30073, 29943, 29473, 32252, 29904, 30279, 29473, 30372,
         29811, 30279, 29473, 30461],
        [    1,  1619,  4826,  7462,  1770,  1040,  5396,  4673,  1070,  3493,
          1632, 20510,  5876, 10148,  2821, 29493,  9475, 23060,  6804, 29584,
         28791,  2735, 29584, 28791,  2259, 18997,  1315,  1093,  4800, 29499,
          1391,  5415, 29493,  1065,  7324,  7240,  1066,  8411,  6229,  2813,
          8746, 29491, 28495,  7961,  1274,  9518,  1032,  4526,  2212,  3296,
          1070, 21978,  8585, 29494,  1273, 17772,  2856,  1801,  1072,  2355,
          1062,  8798, 29491,  4257],
        [    1, 29473,  1007,   901,   959, 30233, 30368, 30062, 31043,  8896,
          4595, 29955, 29473, 30960, 30367, 30591, 29473, 30778, 30482, 29473,
         30875, 30547, 30661, 31250, 29500, 29522, 29568, 29525, 29499, 29473,
         30874,  1008,   925,   912, 29916, 29473, 30669, 30499, 29903, 29473,
         30461,  1005,   949,   959, 29811, 29491, 29473, 31377, 29473, 29932,
         31047, 30306, 29473, 31078, 31163, 29911, 29473, 29518, 29502, 29518,
         29518, 32247, 29916, 29473],
        [    1, 29473, 31717, 29932,  1008,   922,   911, 29473, 30512, 30584,
         30197, 29962, 29811, 29491, 29473, 32464, 30515,  1006,   910,   923,
         29493, 29473, 29919, 30083, 32347, 29473, 30609, 30001, 29473,  1006,
           904,   959, 29955, 29903, 29473, 30874, 31341, 30194, 30073, 29473,
          1006,   941,   906, 29473, 30402, 29916, 29473,  1005,   948,   955,
          1007,   950,   915, 30073, 29473, 29783, 32370, 30201, 29929, 30279,
         29473, 30372, 29943, 30737]], device='cuda:0')
[2025-05-09 11:45:48] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:48] [INFO] [nan 감지] labels: tensor([0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')
[2025-05-09 11:45:48] [INFO] [nan 감지] input_ids: tensor([[    1,  9174,  1276,  2207, 27398, 15790,  6644,  1032,  1088, 29501,
         22269, 29493, 13915,  1153,  1293, 29501, 20347, 29493, 14614,  8818,
         10522,  1093, 29506,  2151, 29499,  1065,  1639, 29473, 29538, 29577,
          2533, 29522,  1137, 15217,  1158,  1032, 15314,  1122, 15751,  6241,
          1040,  5192, 29491,  1183,  8460,  1050,  1167,  4152, 29501, 24519,
          1167,  4152, 13139,  1562,  1323,  1093, 29522, 29483, 29522, 29488,
         29499, 10878,  1040,  1088],
        [    1,  1299, 29473, 31727, 29783, 30809, 29473, 29929, 30367, 29473,
         31461,  1008,   905,   932, 30351, 29911, 29473, 30880, 32398, 29904,
         30601, 29473, 30778, 30778, 29473, 31334, 29962, 30072, 30515, 29473,
          1008,   917,   910,  1007,   901,   951, 30515, 29473, 31310, 31928,
         29493, 29473, 29783, 30294, 29473,  1008,   926,   939,  1007,   919,
           943, 29473,  1007,   908,   943, 31894,  1007,   932,   911, 30201,
         30285, 29576, 29473, 30285],
        [    1, 29473, 30346, 30325, 32440, 30512, 29955, 30233, 31362, 31717,
         29473, 29929, 31371, 29473, 31637, 31258, 31301, 29473, 30512, 30584,
         30197, 29962, 29811, 29491,  1161, 29537, 30778, 30083, 29473, 30083,
         30515,  1006,   910,   923, 30050, 31334, 29473, 32727, 30478, 30238,
         30056, 32578, 30338, 29962, 29811, 29491, 29473, 30778, 30083, 29943,
         29473, 31547, 32231, 30325, 30781, 31159, 30421, 29473, 30661, 30668,
         30612, 29473, 31547, 31544],
        [    1,  1619,  4649, 21334,  1066,  8852,  1040,  4526,  2212,  1055,
          2668,  1038,  2259, 14069, 18782,  4120,  7629,  2938,  1971, 18839,
          1093, 29503,  2953, 29499,  7651,  1072, 19218,  2261,  3241,  1831,
          1065,  2513,  1066,  5684,  5929, 29501, 15495,  1122,  2472, 29501,
          2049, 18839, 16008, 29491,  2562,  2937, 29493,  1983,  2396,  4100,
          1427,  3150,  1546,  1224, 14663, 29491,  2559,  3730,  1040,  5556,
         11235, 29493,  1032,  2467],
        [    1,  1183,  4709,  1070,  1224,  4598,  1117,  4243,  3835, 21761,
          1030,  9174,  2251, 28541,  1065, 16312,  1033,  1072, 23717,  1488,
          1070,  1119,  1303,  1328,  3561,  1054,  1163, 19398,  3777,  1044,
          9159,  1241,  2386,  1126,  1489,  1050, 13724,  1069, 18196,  7363,
          1149, 18854,  1619,  4100, 21933,  1124,  1040,  5396, 27104,  1070,
         19398,  3777,  1044,  2502,  1241,  1249,  1126,  1489,  1050, 13724,
          1069, 18196, 15790,  1093],
        [    1,  1119, 15284,  1346, 29501,  1594,  2743, 22447,  1942,  1066,
          6638,  1090, 26412, 13501,  1168,  4380, 18196, 15790,  1093, 29523,
         29537, 29558, 29499, 21610,  1395,  1171, 17034,  1066,  1052,  9791,
          1254,  2328,  8445,  1996,  1031,  1241,  1093, 29505, 29489, 29545,
         29499,  1072,  1311,  1078,  9139,  1062,  1093, 29505, 29489, 29509,
          1072,  1083, 29489, 29545, 29499, 17193, 29491,  4589,  1361,  1942,
          2937,  1065,  1040,  5903],
        [    1,  2760, 29488,  1129,  6145, 21608,  3396,  1163,  8946,  1910,
          1134,  1283,  1117,  4066,  1065,  3703,  7664,  1070, 14413,  6466,
         29493,  1163,  1210,  2439, 11387,  1065,  6787,  1120, 12022, 29491,
          1328,  1224,  6215, 29493,  1246,  4110,  1040, 24168,  6131,  1163,
          1040,  6032,  9230,  1070, 10789,  3408,  1396, 12022,  1065,  6466,
          1484,  4865,  2424,  7651, 29493,  1658, 14895, 14713,  1072,  8222,
         26102,  1831,  3645,  1066],
        [    1,  1093,  8760,  1040, 27491,  5285,  1396,  1254,  3957,  1046,
          1072,  1102,  1910, 29493,  1124,  7486, 29473, 29508, 29502, 29508,
         29502, 29584, 29508, 29502, 29508, 29518,  3742, 25084, 29491,  1161,
          1236,  1070,  2179, 29501,  5121,  5261, 23280,  2603,  5533, 12177,
          8460,  7680, 11419,  1032,  3782,  4294,  3296,  2424,  3468,  1163,
         20095,  6482,  1072,  7406, 14184, 29491,  1098,  7285,  7167,  1070,
          1040,  6609, 29501,  7770]], device='cuda:0')
[2025-05-09 11:45:48] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:48] [INFO] [nan 감지] labels: tensor([0, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')
[2025-05-09 11:45:48] [INFO] [nan 감지] input_ids: tensor([[    1,  1328,  1224,  6215,  1246, 10916,  1099,  1040,  5945, 12754,
          1070,  9480,  7123, 25411,  1040,  9789,  1070,  3286,  1673,  1065,
          1757,  5177, 10518, 29515,  4426, 14232,  1072,  7475, 29491,  1584,
          4000,  1124,  1757,  5454,  8131, 29515,  4318,  2276,  3617, 29491,
          2590, 29490,  4134,  1250, 29491,  1443,  1072,  4318,  2276,  3617,
         29491, 25276,  1557,  7131,  1106,  6733, 29491,  2677, 29491,  3718,
         29493, 11684,  1124, 18977],
        [    1,  1299, 29473, 30050, 31933, 30402, 29955, 30989, 30584, 29493,
         29473,  1006,   949,   931, 30172, 31951, 29916, 29473, 29911,  1006,
           951,   906, 12509, 29473, 31158, 29473, 30668, 31113, 29473,  1008,
           922,   943, 30072,  1007,   959,   899, 30201, 29473, 30368, 30574,
         29473,  1007,   920,   929, 30515, 30073, 32486, 29473, 31539, 31735,
         30050, 29473, 30735, 30657, 29968, 29473, 32228, 30194, 29473, 30463,
         30304, 30880, 29811, 29473],
        [    1, 28495,  4100,  1427, 11016,  1040,  4673,  1070, 15079, 22786,
          1323,  1102, 29501, 29926,  1093, 16002, 29511, 29501, 29926, 29499,
          1065, 18054,  1845,  1040,  1398,  1804,  1056,  1070,  4488,  9488,
          1364,  1030,  1282,  1111,  1925,  1249, 18072, 29474, 29473, 29518,
          1245,  1198, 29512,  3816, 12756,  1866,  9662, 29491,  1183,  6800,
          1070,  1224,  4649,  1171,  1066,  8852,  3929, 29287, 29511, 29501,
         29926,  1597,  1751,  6980],
        [    1,  1183, 10146,  2212,  3698,  2207,  1044,  1273, 27618, 28800,
          1093, 29511, 10866, 29499,  1072,  4886,  5944,  1070,  1040,  6045,
         20787,  2274,  5191, 22317,  8509,  1117,  4585,  1070, 16028,  9155,
         29491,  3677,  3671,  2477,  1066, 12402,  1476, 18069,  2212,  1102,
         10866,  1072,  1040, 22317,  8509,  1070,  1040,  4003, 29493, 15117,
          1567,  3671,  4165,  1137,  1102, 10866, 16848,  2427,  1115,  6625,
          1122, 20219, 11468, 29491],
        [    1, 29473, 30325, 30092, 29957, 29473, 31929,  1006,   912,   923,
         29473, 29508, 29491, 29508, 30609, 30584, 29473, 32750, 30588, 29955,
         29473, 30735, 31324, 31518, 29473, 30778, 30778, 31288, 30402, 30325,
         31210, 29783, 29473, 32287, 30194, 29473,  1007,   932,   948,  1007,
           932,   948, 29473, 31324, 31518, 30616, 31359, 29811, 29491, 29473,
         29783, 29943, 29473, 30778, 30778, 31288, 30402, 30325, 31210, 29955,
         29473, 31324, 31518, 29473],
        [    1,  1183,  5761,  1072, 15961,  6411,  1070,  1032, 10076, 17453,
          1050,  3799,  5106, 17650,  5949,  1065, 13502, 17453,  1050,  3799,
          5106, 17650,  6971,  1117,  3958,  2712, 23769,  1066, 14180,  1639,
          5396,  1706,  1158,  1164,  9864, 29493,  2121, 29501,  2304, 24128,
          4689, 29491,  1619,  4649, 21334,  1066, 16395,  1040, 14020,  1072,
         26721,  3800,  1070,  2181,  1224,  1401, 10076,  1053,  6423, 29501,
         29487,  1563,  3108,  7352],
        [    1,  1619,  4649, 16351,  1232,  1782,  6155,  2811, 29501,  4834,
         29527, 29501, 29920, 29501,  7859, 12023,  1098, 13426,  2459,  4423,
         13163, 13859,  1065,  4954,  3553,  1163,  6623,  2650,  1056, 29501,
          5907,  4025, 10481,  4959,  1086,  1280,  1031, 10563, 29510,  2717,
          4394,  1040, 21698,  2355,  1070,  7651, 11948,  1254,  1784,  2650,
          1056,  1771,  4025,  5934,  1036,  1280,  1031, 10563,  1093,  9042,
          4245,  1377, 24286,  1346],
        [    1, 29473, 31078, 32109, 29473, 30083, 29783, 30809, 29473, 30778,
         31878, 29473, 30083,  1006,   932,   899, 29916, 30073, 29473, 30512,
          1006,   946,   956, 30072, 31043, 29473, 29508, 31502, 29473, 30908,
         29783, 32018, 29473, 30795, 30880, 29473, 30421, 30612, 29473, 30245,
         30279, 29493, 29473, 30382, 30056, 30279, 29473, 30194,  1008,   901,
           956, 29473,  1008,   902,   907, 32493, 29473, 31663, 32082, 29473,
         30908, 29783, 32018, 29473]], device='cuda:0')
[2025-05-09 11:45:49] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:49] [INFO] [nan 감지] labels: tensor([0, 0, 1, 0, 0, 1, 1, 1], device='cuda:0')
[2025-05-09 11:45:49] [INFO] [nan 감지] input_ids: tensor([[    1, 16115,  3702, 12591, 29515,  3512, 11215,  1346, 24279,  6739,
          6023,  1033,  2300,  1093, 29521,  2372, 29511, 29481, 29499,  1228,
         11988,  2075,  1066,  6431,  8445,  3914,  1375,  3493,  1089,  3898,
         29494,  1273, 13006, 29491,  3761, 29493,  1065, 26149,  1148,  1135,
          2372, 29511,  1706,  1761,  2504,  1066,  8517,  7009, 29491, 24651,
         12314,  8906, 29515,  2559, 16395,  3929, 17759,  1693,  8798, 22533,
         15768,  5449,  1066,  1135],
        [    1,  1167, 29490,  1082,  1117,  1032,  6210,  1070,  1167,  4152,
          8536,  7580, 29493, 11251,  1254,  5934,  8536,  9521,  2107,  1072,
         17377,  2668,  1385,  5762,  1093, 15923, 29481,  1377,  1183,  7026,
          7271,  1070,  1167, 29490,  1082,  1117,  1066,  1164,  2246,  1148,
          1401,  4106,  1070,  3419,  1167,  4152,  7580,  1124, 18104,  1090,
          1315, 23224, 29493,  7079,  4928, 26335,  1042, 29493,  2181, 14277,
          1133, 28467, 17891,  1065],
        [    1,  1299, 29473, 30973, 30402, 31502, 29493, 29473, 30325, 30880,
         29929, 29473, 30668, 30001, 29473, 31078, 30279, 29473, 29904, 29783,
         31841, 30056, 30238, 29473, 30368, 29932, 31502, 29493, 29473,  1008,
           923,   900, 30894, 29955, 29473,  1005,   956,   902, 31250, 29957,
         29473, 31383, 29811, 29473, 31078, 32109, 29473, 30325, 30880, 29929,
          1232, 32037, 32037, 29510, 29903, 29473, 30668, 30001, 30050,  1232,
         29518, 29502, 29518, 29538],
        [    1,  1133,  5382, 15002, 11701, 29515, 11590,  1208,  2059,  1070,
          5934,  1365,  1128,  1273,  9435,  1814,  1122, 18935,  1396,  1365,
         16295,  1254,  4887,  1087,  3637,  1056,  1093, 29511,  2960, 29545,
         29499,  1274,  1518,  6513,  9129, 29491,  1584, 18707,  1066, 16395,
          1040,  5493, 29501,  7358, 18782,  1070,  5934,  1365,  1128,  1273,
          1102,  2960, 29545,  1093,  2660, 25783, 29499,  4120,  7651,  1163,
         17351,  1066, 23419,  2517],
        [    1, 24651, 12314,  8906, 29515,  1183,  4347, 19164,  1070,  6893,
         18732, 18559,  1117,  1164, 20821,  1566,  3296,  5136,  1823, 25950,
          1458,  1032, 18069,  1163,  1586,  1042,  1240, 29491,  1619,  4649,
         18707,  1066, 18569,  1040,  4526,  2212, 26048,  2192,  1143,  8205,
          5106,  1093, 15483, 29522, 29499,  1072, 10519,  1396,  1484,  1389,
          1070,  1454,  2582, 29493, 12549,  9023,  1102, 29493,  1072, 13915,
          1777, 13221,  1030,  1065],
        [    1,  1619, 26992,  4826,  4942,  2071,  1040, 26139,  1404, 29493,
         15961, 16327,  1072, 18782,  1070, 14946, 24938,  1068,  1149, 13491,
         26005,  1093, 29527,  5683, 29481, 29499,  1065,  3958,  2712,  4552,
          7651, 29491, 15962,  1245, 29473, 29518, 29502, 29502, 29502,  1066,
         29473, 29518, 29502, 29518, 29502,  1274,  1518,  5426,  1245,  1032,
          7445,  1070,  3814, 26484,  2027,  1158,  9510, 16542, 29516, 16542,
          1849, 29493,  3860, 19621],
        [    1,  1619,  4649,  7462,  1770,  1040,  1478,  1280,  1030,  1513,
          2165,  1062,  3194,  1070,  1032,  7860,  1029,  1582,  1050,  1076,
          3698,  1169, 29485, 29869,  9439, 29501,  4499,  1054, 12201,  6156,
          1208, 29501,  4437,  1851,  6074,  1058,  2253,  1153,  1050, 22018,
          2603, 10789,  3408,  1396,  1052, 28819,  1170,  1170,  2159, 18965,
         29491, 10959, 17727,  5406, 28273,  1065, 13270, 29493,  1146,  1117,
         12677,  1137, 11638,  3563],
        [    1,  1299, 29473, 30973, 30402, 31502, 29493, 29473, 30368, 29932,
         31502, 29473, 30461, 31104, 29473,  1006,   940,   907, 30921, 30463,
         29572, 29473, 30992, 30489, 30245, 30609, 31099, 29473,  1008,   914,
           915, 29929, 29493, 29473, 31104, 31047, 29916, 29473,  1006,   912,
           923, 29929, 29943, 29473, 30382, 31933, 30062, 29473, 31078, 32109,
         29473, 30973, 30402, 31502, 29903, 29473, 30368, 29932, 31502, 29473,
         30461, 31104, 29957, 29473]], device='cuda:0')
[2025-05-09 11:45:50] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:50] [INFO] [nan 감지] labels: tensor([0, 1, 0, 0, 1, 1, 0, 1], device='cuda:0')
[2025-05-09 11:45:50] [INFO] [nan 감지] input_ids: tensor([[    1, 29473, 30382, 30056, 30279, 29473,  1007,   953,   940, 31951,
         29473, 31637, 29929,  1007,   949,   931, 29473, 31780,  1007,   927,
           932, 32170,  1006,   910,   923, 29493, 29473,  1007,   953,   940,
          1006,   953,   900, 29473, 31637,  1006,   950,   916, 30455, 29473,
         31780,  1007,   927,   932, 32170,  1006,   910,   923,  1005,   958,
           923, 29473, 32727, 29955, 30238, 32519, 29962, 29811, 29491, 29473,
         29518, 29502, 29508, 29551],
        [    1,  1619,  6215, 21933,  1124,  1040,  2527,  1070, 17650, 11748,
         10203,  1827,  3577,  4845, 29491, 13396,  1040,  5331, 29493,  7471,
          1072, 10313,  1070,  3577,  4845, 12935,  1065, 15816, 18024, 29493,
          7167,  1420,  8783,  7523,  1032, 11563, 11510,  5059,  1122,  3296,
          7459,  4100, 29491,  1619, 26992,  1520, 23738,  4826, 28065,  1066,
         18830,  1421,  2639,  1245,  4222,  7961,  1124,  6330,  2075,  1066,
          9777, 11748,  3003, 29501],
        [    1,  5423,  3644, 29481,  1070, 17759,  1693, 19025,  3467,  1032,
          9189,  3667,  1245,  5688,  1058,  9542, 29491,  1183,  5550,  1070,
          1164, 23961,  2952,  1070,  3667,  1784,  1265,  4482,  3400,  5556,
          1070,  1040, 19165,  1072, 27337,  6055, 11337,  3207,  9139,  6817,
         29493, 28585, 29493, 19408,  3431, 29493,  1040, 18699,  3193,  1240,
          1070,  1040, 24392, 11539, 29491,  9284, 29493,  1507,  1032,  1401,
          3207,  9139,  1117,  8928],
        [    1, 29473, 30679, 29903, 29473,  1007,   924,   927, 29473, 29783,
         29473,  1007,   921,   923, 29932, 29968, 29473, 31444,  1006,   909,
           915,  1006,   902,   915, 29473, 29904, 30333, 29473, 29783, 29473,
         29783, 31031, 29932, 29968, 29473, 29904, 31478, 30279, 29473, 30050,
         29473, 30781, 31310, 30285, 29491, 29473, 31727, 30072, 30172, 29473,
         30498,  1008,   904,   919, 29916, 29473, 31113, 30001, 30073, 29903,
         29473, 30707, 30201,  1007],
        [    1, 13128,  1222, 16617,  1163,  1869,  6785,  2288, 12854,  3505,
          2932, 29515,  5143,  1046,  5096,  1040, 28571,  1070,  3248,  1081,
          1385,  5598,  3093,  1056,  1124, 16733, 20627,  1619,  4649,  4942,
          2071,  1040,  1811, 29501,  7358,  6860,  1070, 16756, 15393,  1056,
          1124,  6484,  5165, 29491,  3957,  2288, 12854,  3505,  2932, 29493,
          1032, 28324,  1054,  4100,  1031,  1065,  1040,  6130,  2602, 29493,
          1390,  4826, 10841,  5175],
        [    1,  1619,  4649,  1576,  1343,  1066, 17982,  1040, 10313,  1070,
         23513,  2374,  1065,  4239,  1070, 12083,  1090,  2184,  1062, 10051,
         19408,  1102, 29550, 29555,  8920, 29516, 29552,  1058,  1303, 29491,
          1328,  4639,  4612,  1032,  2686,  1070, 29473, 29551, 16102, 29493,
          1146, 18707,  1066, 18569,  1040, 19191,  1065, 18855,  2929, 29493,
          6942,  1072,  1567, 12083, 22243, 29501, 10414,  6276,  3441, 29473,
         29508, 29502,  2349, 23513],
        [    1,  1133,  5382, 15002, 11701, 29515,  1183,  1181,  3097, 29476,
         15790,  1427,  1518,  8100,  1122, 10464, 26798, 29481,  2622,  1040,
         29473, 29508, 29542, 29555, 29502, 29481, 29493,  1163,  1040,  1848,
          6159, 26798,  4112,  2401,  2212, 29473, 29518, 29502, 29508, 29549,
          1072, 29473, 29518, 29502, 29508, 29552,  1072, 13866,  1164,  6379,
          1566,  3296, 12611, 29491,  1181,  3097, 29476, 15790,  8798,  1093,
          8089, 29525, 29499,  1427],
        [    1,  1619,  4649,  4942,  2071,  1040,  6860,  1070, 29232,  3253,
          4459, 10770,  1167, 29527,  2966,  1093,  1579, 29522, 29527,  2966,
         29499,  1158,  1032,  3593,  1070,  3370,  1066,  6065,  1150,  9860,
          1044, 15790, 18965,  1065,  2328, 12549,  1079,  1072,  1131,  6230,
          5762, 29491,  1183,  1706,  1070,  1167,  4152, 27762,  9056,  5396,
          1066, 23126,  1047,  8460,  1050,  1080,  3227, 29493,  1330,  8288,
          1684,  2626,  2100,  3708]], device='cuda:0')
[2025-05-09 11:45:51] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:51] [INFO] [nan 감지] labels: tensor([0, 1, 1, 0, 0, 0, 0, 0], device='cuda:0')
[2025-05-09 11:45:51] [INFO] [nan 감지] input_ids: tensor([[    1,  1183,  2611, 29501,  1300, 15211,  5191, 10878,  1122, 10664,
          1369,  1971,  9023,  2786,  3792, 18965, 29493,  1117, 23932,  1283,
         29493,  2192,  1143,  8205,  1659,  1370,  5253, 29493,  1058,  4505,
          6171, 29493,  4795,  9413,  2570,  8798, 29493,  1072,  7026,  1087,
          3637,  8487,  1792,  1971, 18839, 29491,  5636,  2491, 25592,  8487,
          1117,  1032,  4066,  3207,  1896,  1122,  2055,  5099,  1072, 18209,
         29481, 29493,  6142,  1066],
        [    1,  1619,  4598, 16658,  1164, 23862,  1070,  1040, 11137,  1072,
          9580,  1137,  3356,  1122, 16865,  1428,  1030,  3964, 15760,  1093,
         29521,  7684,  1325,  1032, 15129,  5412,  1163,  1164,  3026,  3932,
          5106,  1070, 29473, 29550, 29501, 29518, 29550, 13978, 10859, 18977,
          1163,  6142,  6291,  5184, 10727,  2351, 29493,  7026,  2424, 15082,
          1072, 16008,  5929,  1058,  9542, 29493,  1246,  9819,  1198, 29512,
          4082,  9380,  5533, 11463],
        [    1,  1299, 29473, 30172,  1006,   930,   959, 30980, 29473, 29518,
         29932, 29473, 30992, 30489, 29473,  1007,   920,   920, 32313, 29916,
         30367, 29473, 31433, 30547, 29904, 30279, 29473, 31217, 31371,  1008,
           928,   916, 30062, 29473, 29929, 31341, 29515, 29473,  1005,   956,
           909, 29783, 29473, 30372, 29943, 29473, 30985, 32353, 29473, 31195,
         30285, 29473, 29518, 29502, 29508, 29552, 32247, 30543, 30433, 29473,
         29518, 29502, 29518, 29502],
        [    1, 29473, 30382,  1006,   930,   947, 30737, 29473, 30661,  1007,
           953,   920, 29916, 29473, 30463, 30073, 29943, 29473, 31207, 30306,
         29473, 30304, 30325, 29473,  1005,   955,   912, 30001, 30919, 30197,
         29962, 29811, 29491, 29473, 30382, 31012, 30463, 29473, 31038, 30245,
         30050, 29473, 31310, 31104, 29957, 29473, 30201,  1006,   921,   958,
         30601, 29473, 32474, 29943, 29903, 29903, 29473, 30766, 29943, 29473,
         30795, 30285, 29904, 29811],
        [    1, 27492,  7017, 27104,  1040,  6817,  1070,  3698,  1072, 28811,
         19025, 29493,  1072,  1504,  1117,  6632,  2913,  1065,  7167,  1040,
          4673,  1070, 27820, 29493, 22317, 29374,  1362,  1072,  1567, 28811,
         15839,  1065, 19165, 18965, 16330, 29491,  3761, 29493,  1040,  8783,
          1070, 10305, 11369,  1124,  1065, 24266,  2706,  1971,  1490, 23873,
          1070,  4202,  7017,  1117, 24890,  9756, 29491,  1584, 20101,  1032,
          6703,  1608, 11550,  1137],
        [    1,  1183, 14596,  1070,  3207,  9139, 29501,  1594,  2743, 12062,
          1254,  1102, 29501,  1891, 12542,  1894,  1072,  1040,  8230,  1090,
         20353,  7123, 18054,  1369,  1254,  1040,  1102,  3868, 29518, 29516,
          4788, 29522, 29518, 11790,  1228,  8044,  6712,  1065, 24457,  1040,
          4202, 21698,  3667,  1066,  1514,  6098, 29491,  1183,  2937,  4649,
          1171,  6450,  1066, 17982,  3929,  1040,  7471,  1070,  3460, 18104,
          1090,  1315,  1928,  1850],
        [    1, 20210,  1070,  7324,  5823,  5859,  1122, 29473, 29518, 29502,
          2035, 13076,  1137,  1504,  1427,  1518,  1164,  6015,  1065, 13927,
         10365, 29484,  1473,  1065,  2664,  1046, 29494,  2332, 29491,  1183,
         18613,  4070,  1050,  6108,  1070, 10365, 29484,  1473,  1163, 14063,
          1065, 25586,  1072,  8242,  1117, 15653,  1245,  1040,  2444,  1647,
          1070,  4363,  1072, 11120,  1546,  7323,  2179, 29491,  1429,  1427,
          1227,  1518,  8723, 29493],
        [    1, 20078,  1065,  1581,  7167,  1070, 26812, 29508, 29501,  1035,
          1810, 11705,  1058,  2253,  6457, 29501, 20472,  1369, 19335,  1088,
          1093,  2660,  1921, 29499,  9662,  1427,  7101,  2913,  1065,  1063,
         22932,  1056,  1935,  9662,  1122,  5261,  1138, 28611, 29491,  1183,
          5551,  1148, 29501,  4859,  3667, 16327, 29493, 26236,  1065,  1040,
          5010, 29493,  1717,  1039, 29501,  1138,  1035,  1810, 11705,  5503,
         29493,  1072, 12858,  2211]], device='cuda:0')
[2025-05-09 11:45:52] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:52] [INFO] [nan 감지] labels: tensor([1, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')
[2025-05-09 11:45:52] [INFO] [nan 감지] input_ids: tensor([[    1,  1619,  4649,  2717,  4394,  2686,  9288, 11137,  1065, 22921,
          1122,  3204,  1163,  6482, 18367, 29491, 10859,  1032,  4877, 17230,
          6411,  1070, 18977,  1072,  2424,  8574, 29493,  1040,  4476, 27627,
          1070, 15961,  5852,  4106,  1124,  1559,  2789,  9703,  4007, 16377,
          1228, 21900,  1066,  9819,  9380,  5533, 11463,  6821, 10330,  1070,
          7386,  6267,  3296,  4113, 29491,  1328,  1040,  5406, 16008, 14662,
         23868,  1254, 12630,  4042],
        [    1, 13528,  1693, 11912,  3066,  1072, 11912,  1081,  2192, 22298,
          4155,  1228,  4353,  1066,  1072, 10007,  1040,  2864,  1894,  1065,
          1085,  3718,  1396,  8607,  9662, 29493,  1458,  1228,  3419,  1158,
          4243,  9484,  1030, 11912,  1081,  1120, 18854,  2155,  1228,  3076,
          3782,  5282,  1070,  1040, 15079, 11912,  1081,  1120, 29493,  1381,
         29491, 29474, 29491,  1354,  2621,  1672,  2233, 29493,  5553,  2930,
          1672,  2233, 29493,  1478],
        [    1, 25533,  1058,  1303,  1422, 10620,  1163,  1108, 29512,  1202,
          1044, 11345, 29490,  1148, 20537,  2730,  1093, 29525,  2151, 29499,
          1072, 27325,  1163, 17102, 19211,  3209, 17207,  1076,  2730,  1122,
         11191,  1032,  7860,  5602,  1241,  1758, 18196,  2997, 29491,  1102,
         29550, 29555,  8920, 29516, 29552, 29527,  1058,  1303,  1093, 29555,
         29501, 14817, 29501,  1506, 29499,  1422, 14338,  1546,  3076,  5706,
         29491,  7834,  2839, 15789],
        [    1,  4419,  1097,  1266,  3089,  1093, 29545,  5579, 29499,  1569,
         10143,  1761,  1924,  1032,  1052,  1217,  6602,  4673,  1065,  1032,
          7445,  1070, 11364,  1289, 10414, 15961,  5099,  1072, 19025, 29491,
          2435,  1346, 23696,  1054,  1186, 29501,  1091,  1067,  3719, 29485,
          1595, 29474,  1241,  1093, 29527,  2413,  1325,  1458,  5570,  1037,
          8905,  1040,  1045,  1595, 29474,  1241,  3798,  1122,  1188,  5579,
         14374, 22305, 29493,  1427],
        [    1,  1183, 26169, 29516, 29509, 20722, 29501, 10414,  5688, 11550,
          1065,  5845,  1427,  2335,  1032,  1811,  1837,  1245, 13503, 19057,
          1066,  6065, 26169,  1245, 16203,  1040,  3707,  1065,  1040,  3703,
         16128,  1070,  1040, 24392, 11539,  1066, 13799, 24019, 16081,  5050,
         26169,  3667,  1070,  3922, 29491,  7834,  4475,  1070,  1040,  1837,
         29493,  8266,  1850,  9542,  1065,  5845,  1274,  1109, 28411,  1066,
          6175,  1137, 26169, 29501],
        [    1, 29473, 30382, 31012, 30333, 29473, 30595, 31012, 29473, 29932,
         30992, 29955, 29473, 30001, 30245, 29473, 32102, 31083, 30367, 29473,
         30382, 31012, 29962, 31346, 29473, 30334, 30875, 32291, 30367, 29493,
         29473, 30875, 32291, 30367, 29473, 30325, 30669, 29916, 30073, 29473,
         30609, 31578, 29943, 29473, 30960, 30004, 30874, 29473, 31008, 32228,
         30351, 29911, 29473,  1008,   928,   915, 31647, 29811, 30468, 30279,
         29473, 30843,  1006,   932],
        [    1,  1299, 29473, 29518, 32012, 29473,  1005,   956,   913, 32645,
          1007,   934,   931, 29572,  1119, 29596, 30489, 30402, 29493, 29473,
         30894,  1008,   923,   959, 30543, 30543, 29955,  1232, 30679, 31824,
         30871, 32376, 29473, 30627, 31747, 29510, 30306, 29473, 29783, 30294,
         29473, 29811, 31859, 29811,  1232, 30894,  1008,   923,   959, 30543,
         30543, 29493, 29473, 30679, 29473, 31824, 29473, 30871, 32376, 30306,
         29473,  1005,   962,   907],
        [    1,  1299,  1232, 31246,  1007,   926,   904, 29955, 29473,  1008,
           927,   951, 31123, 29510, 29473, 30543, 31104, 29916, 29473, 31929,
         30304, 32154, 29473,  1007,   957,   915, 30463, 29811, 29473, 30368,
          1007,   930,   900, 29473, 31246,  1007,   926,   904, 29473, 29508,
         29542, 29550, 29502, 32247, 30402, 29493, 29473, 31878, 32611, 30050,
         29473, 30050, 31547, 30368,  1007,   930,   900, 29955, 29473,  1007,
           929,   919,  1008,   923]], device='cuda:0')
[2025-05-09 11:45:53] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:53] [INFO] [nan 감지] labels: tensor([0, 1, 0, 1, 0, 1, 0, 1], device='cuda:0')
[2025-05-09 11:45:53] [INFO] [nan 감지] input_ids: tensor([[    1, 29473, 30382, 30056, 30279, 29473, 30766, 30367, 29473, 29783,
         30601, 29473, 30871, 29929, 32423, 29473, 32727, 30478, 29783, 29473,
         32473, 29473, 31207, 29473, 31604, 30338, 29962, 29811, 29491, 29473,
         30871, 29929, 32423, 30351, 29911, 29473, 29811, 30004, 29473, 30050,
         30524, 29473, 32464, 30056, 31338, 30245, 30062, 29911, 30073, 29955,
         29473, 30201,  1006,   921,   935, 29473, 30334, 30894, 30612, 29473,
         30325, 30992, 29957, 29473],
        [    1,  1152,  1306, 19801,  1077,  1117,  1032, 21627, 14163,  2075,
          1122,  1040,  6595,  1070,  1744, 29532, 25324, 29510, 29481,  8798,
          1072,  1427,  9129,  1518,  6513,  1066, 15380,  5396,  7964,  1122,
          7651,  1163,  1131, 28908,  1108,  1234,  1283, 29491,  1328,  1224,
          4598, 29493,  1246, 26249,  4397,  1254,  2181,  1176,  2217,  1158,
         15361,  5762,  1066, 21533,  3929,  2971, 19801,  1077,  1309,  1766,
          5033,  1148, 26514,  3592],
        [    1, 29473, 30402, 30083,  1006,   910,   923, 29493, 29473, 29929,
         32644, 30524, 29916, 29473, 30050, 30735, 29473, 30001, 30346, 29473,
         31468, 30499, 29473, 30895, 30626, 29473, 30083, 31301, 29903, 29473,
         30372, 31359, 29929, 30285, 29572, 29473, 32109, 31303, 30919, 30351,
         29911, 29473, 31900, 31780, 30543, 29903, 29473, 30368, 30960, 30919,
         30351, 29911, 29473, 30194, 29783, 30194,  1007,   932,   911, 29811,
         30468, 30279, 29473, 29904],
        [    1,  1183,  4709,  1070,  1224,  6215,  1117,  1232,  8984,  9765,
          3020, 29515, 11108,  1072, 24991, 10991,  1070,  1164, 11162,  8392,
         21786,  1122, 20831, 10234, 10120,  1070, 12963,  7373,  7104, 29486,
          1770,  1065, 12781, 29501, 25009,  2113, 19691,  4903, 10959, 28273,
          1065,  6482,  7459, 29493,  2055, 16008,  5686,  1065,  4483, 29501,
         19144,  7240,  2077, 12139,  1066,  3852, 23961, 13894,  1124,  5747,
          4126,  6179, 29491, 12296],
        [    1, 29473, 30402, 30368, 29473, 30073, 31858, 29473, 31287, 31433,
         30201, 32231, 30325, 31055, 29955, 29473, 32313,  1006,   950,   916,
         32353, 29473, 30512, 30584, 30197, 29962, 29811, 29491, 29473, 30001,
         32347, 29955, 29473, 30669, 30499, 31043, 29473, 30992, 30874, 30731,
         29783, 29473, 30415,  1006,   929,   942, 30482, 31093, 29473, 30001,
          1007,   948,   904, 29955, 29473, 29929, 31341, 30233, 29957, 29473,
         30895, 29929, 30491, 29473],
        [    1,  1619,  4598, 16658,  8068, 29533,  4412, 29493,  1032,  7860,
         16712, 19082,  6450,  1066,  4100,  4369, 29501,  6719,  3445, 24170,
         18112,  1240,  1245,  2254, 29501, 15736,  1527,  6099, 11463, 27650,
         29491,  1619,  4689,  1384, 12088,  1164, 17595,  5199,  1137, 10623,
          1122,  2328,  8536,  4813,  1072,  1988,  9246, 29491,  1429,  1117,
          1164, 27368, 29501, 29481, 17188, 10232, 29493, 11405,  1070,  7473,
          1056,  8393, 10732,  3858],
        [    1, 29473, 30382, 30056, 30279, 29473,  1006,   905,   899, 30468,
         31321, 29473, 31357, 29783, 29473,  1006,   944,   915,  1006,   902,
           915, 29473, 29904, 30333, 29473, 30402, 30001, 30543, 32727, 30478,
         29473, 30612, 30001, 29916, 30073, 29473,  1005,   952,   908, 30515,
         32677, 29473,  1006,   938,   928, 31624, 29783, 29473,  1006,   938,
           913, 29783, 29473, 30463, 31043, 30073, 29473, 30368, 29473, 31547,
         32231, 29783, 29473, 30609],
        [    1,  1619,  4598, 16658,  1032,  1520, 23738,  4826,  1070,  6159,
          1566,  3296,  1072,  4294,  3296,  1065,  4530,  1831, 12122,  1065,
          2513,  1066, 12186,  1040,  8044, 18637, 29493,  2937,  5396,  1240,
         28380, 29493,  1072,  1407,  2253,  1081,  1148, 23338,  1122,  4100,
          1072,  5688, 29491, 22251,  1164,  8679, 26305, 21744,  1070,  1040,
         14389,  2602,  1070,  1065,  4530,  1831,  1065, 16008,  3526, 29481,
         29493,  1224,  4598,  9904]], device='cuda:0')
[2025-05-09 11:45:54] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:54] [INFO] [nan 감지] labels: tensor([1, 0, 1, 0, 1, 0, 1, 0], device='cuda:0')
[2025-05-09 11:45:54] [INFO] [nan 감지] input_ids: tensor([[    1,  1299, 29473, 31858, 30549, 30072, 31012, 31321, 29473,  1006,
           951,   903, 29955, 29473,  1006,   940,   907,  1007,   953,   935,
         29493, 29473, 31735, 30584, 30367, 29473, 31104, 30880, 29929, 31008,
         29916, 29473, 29549, 31607, 29473,  1008,   914,   944, 30826, 29473,
         31310, 30245, 29473, 32291, 30334, 29916, 29943, 29473,  1006,   949,
           925,  1005,   960,   902, 29783, 29473, 30525, 30661, 29904, 30279,
         29473, 32127, 30072, 30050],
        [    1, 10959, 10445,  1076, 20084,  1065, 23397,  1072, 25348,  7495,
         29493,  1032, 11237, 11550,  1427,  1518,  4625,  1065,  6159,  2035,
          1122,  7167,  1678, 29447,  1263,  2179,  9789,  2427,  1115, 19772,
          1546, 26748, 28363,  3800,  1066, 17759,  1693, 19025, 29491, 15962,
          1070,  1040,  6860,  1070,  7495,  1124,  3207,  9139, 15467,  1070,
         17759,  1693,  8798,  1228,  7184,  9366,  1066, 15849,  1070,  1040,
         22317, 10313,  1070,  1040],
        [    1,  1619,  4598, 21334,  1066, 18569,  1040,  4526,  2212,  3296,
          4705,  3194,  1072, 16986, 11243,  1070, 10856,  1684, 25252,  4335,
         29493,  5539,  1066,  1420,  2602,  1070,  4649, 29491,  1328,  6159,
          2035, 29493,  1504,  1427,  1518,  1164,  8251, 14387,  1124,  1678,
         14933,  6182, 21545,  7760, 29577, 14774,  1507,  3260, 10317,  8985,
          8551,  6446,  1142,  6164,  1072, 24729, 29491,  1183, 11679, 13584,
          6058, 29473, 29518, 29549],
        [    1,  1862, 12677, 29473, 29508, 29555, 29591,  1070,  1309, 15916,
         16013,  1228,  6131,  1163, 17759,  1693, 10878, 29491,  1183, 11893,
          1072, 22317, 19077,  1070,  8460,  1050,  7471, 29516,  1030, 12032,
          1065,  5463, 15052,  1039, 12482,  1117,  7509,  9806,  1330,  1597,
          1115, 13588,  2181,  3698,  1971,  3257,  1293,  1093, 29522,  4152,
         29501,  9698, 29499,  1946,  1245, 15052,  1039, 12482, 29491,  1584,
          2937,  1164,  2333,  3600],
        [    1,  1299, 29473, 30992, 30489, 30368,  1007,   930,   900, 29473,
         30515, 29932, 30499, 29473, 30455, 31478, 29493, 29473, 31099,  1007,
           927,   952, 30004, 30515, 29916, 29473,  1005,   955,   951, 30515,
         32170, 29473, 30279, 30609, 29473, 30735, 30795, 29473, 30992, 30489,
         30368,  1007,   930,   900, 29955, 29473, 30515, 29932, 30499, 29473,
         30455, 31478, 29903, 29473, 32755, 30499, 30616, 30333, 30073, 29473,
         31099,  1007,   927,   952],
        [    1, 19890,  3801,  1070,  1040,  1675, 29501,  1030, 29501, 19297,
          5650,  1056,  1751,  1089,  1364,  1117,  1032,  8044,  4475,  1065,
          8644,  4867,  1072,  7716, 11997,  5432,  5942,  1070,  8644,  6791,
         29491, 18349,  3350,  1065,  1131,  6230,  7961,  2075,  1066,  4001,
          1421, 15961, 10937, 29577, 29481,  8299,  1070,  6791,  1228,  2886,
         29501,  1551,  1306, 29493,  1495, 29501,  1072,  3202, 29501,  1269,
          4865,  1072,  6270,  1080],
        [    1,  1299, 29473, 30368, 29932, 31502, 29473, 30004, 30402, 29493,
         29473,  1007,   929,   909, 29929, 29473, 32021, 30304, 31031, 29473,
         30491, 29473, 30334, 30627, 30782, 29473, 30368, 29932, 31502, 29903,
         29473, 32331, 30543, 30346, 29904, 30333, 30073, 29473,  1007,   956,
           927, 31159, 30421, 30919, 30092, 29473, 31780, 31468, 29919, 31083,
         30351, 29911, 29473, 30325, 30990, 31373, 30279, 29473, 30372, 30338,
         29962, 29811, 29491, 29473],
        [    1,  1183,  1822,  1937,  4744,  1557,  5880,  1031,  2550, 12448,
          1050,  9895,  3181,  1324,  2730, 11973,  1770,  1065,  1032,  6634,
          2212,  1040,  5527,  1072,  1881,  4155, 29491,  8035, 19347, 10386,
          1124,  1040,  7386,  6267,  7017,  1070,  1639,  6740,  1037,  1106,
          1850,  1050, 29501,  4859,  6142,  9662,  1072, 27934,  9662, 29493,
          1458,  1989,  4537,  1047,  3573,  1273,  1480,  1441,  2865, 29493,
          1210, 19464,  1306,  1092]], device='cuda:0')
[2025-05-09 11:45:55] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:55] [INFO] [nan 감지] labels: tensor([1, 1, 0, 1, 0, 0, 1, 0], device='cuda:0')
[2025-05-09 11:45:55] [INFO] [nan 감지] input_ids: tensor([[    1,  1619,  6215, 16658,  1032,  2489, 15736, 12365,  1070,  1040,
         25687,  7219,  1122,  1960, 29501,  6295, 21608,  3396,  1065, 15364,
          6145,  3185,  2241,  1194,  5481, 24611, 29491, 10384,  1164,  3061,
          1070, 23981, 15361,  6330, 29493,  1246, 17267,  1137,  1472, 13760,
          1070,  3915, 29486,  2206, 10608, 29473, 29508, 29716,  1093, 29527,
          7912, 29479, 29508, 29716, 29499,  6550, 18573, 17967,  3001,  1065,
         24938, 14689, 16074, 14579],
        [    1,  1299, 29473, 29904, 30463, 31481, 32347, 29493, 29473, 30194,
         31900, 30325, 31123, 29473, 29783, 30368, 29473,  1008,   907,   943,
         30062, 30062, 29968, 29473, 30512, 30050, 29473, 32452, 31812, 30050,
         29473, 30668, 31663, 29515, 29473, 30325, 31123, 29473,  1007,   962,
           931,  1008,   914,   947, 29473, 29929, 32331, 29473, 29783, 32620,
         30172, 29473, 29904, 30463, 31481, 32347, 30306, 29473, 30194, 31900,
         30325, 31123, 29473,  1008],
        [    1,  1183, 10789,  3408,  1396,  1365, 16295,  6023,  1033,  1128,
          1093, 29521,  2413, 29499,  1117,  5293,  1040,  5782,  5555,  1122,
          4916, 15248,  3759, 17650, 29491, 27954,  1401,  1072,  2876, 29374,
          1263,  6330,  1066,  8852,  4916, 15248,  5165,  1274,  1518,  6970,
         29491,  1183,  7271,  1070,  1581,  4649,  1171,  1066,  8852,  1040,
         23657,  3205,  1070,  1032,  2611, 29501,  1030,  6938,  1263,  9777,
         29232,  5008,  1199,  1340],
        [    1, 14391, 29515, 11228,  3088,  1845,  4488,  9488,  1364,  1030,
         29501,  1282,  1111,  1925,  1249, 18072, 29474, 29473, 29518,  7123,
          1065,  7651,  1163, 21156,  7139,  3473,  8798,  2439,  4222,  4108,
          1070,  4916, 19530, 28908,  8798, 25084, 29515,  1457,  8392,  7139,
          3473,  8798,  1093,  3657, 29525, 29499,  1117,  1032,  4066, 18172,
          6131,  1163,  6632,  3312, 10447,  1240, 29493,  6229,  2813,  1072,
         16008,  7734, 29491,  2155],
        [    1,  1183,  6800,  1070,  1224,  4649,  1171,  1066, 17982,  1040,
          3026,  3932,  1070, 11568,  1068,  2479,  7548,  1375,  1074,  1204,
          2566,  1134,  3097,  2211,  1093, 29558,  4160, 29499,  1792,  8230,
          1178,  1079,  1489,  3719,  9173,  1065,  7651,  1163,  7472, 29496,
          1062,  1072,  4657, 29501,  1973,  2337,  1240, 25400,  2006, 29493,
          1072,  1066,  9819,  5391,  9380,  1122,  1318,  4160, 29481,  1065,
          1224,  1851,  3831,  1070],
        [    1, 29473, 30766, 29943, 29473, 30415,  1006,   909,   923, 29473,
         29783, 29473, 29916, 31701, 29929, 32452, 31812, 30512, 30584, 30652,
         29473, 30652, 29955, 29968, 29473, 30871, 32170, 29904, 30333, 30073,
         29473, 29538, 31502, 29473, 29916, 31701, 29929, 29932, 31303, 30874,
          1008,   925,   912, 29473, 32347, 30279, 31093, 30306, 29473, 30512,
         31747, 29904, 30279, 29473, 30478, 30294, 31357, 29473,  1008,   907,
           943, 30233, 29783, 29811],
        [    1, 29473, 31078, 32109, 29473,  1006,   941,   906, 29473, 30325,
         29473, 30482, 31093, 29473,  1006,   927,   939,  1005,   949,   900,
         30601, 29473,  1006,   921,   931, 30415, 31859, 30279, 29473, 30372,
         29943,  1232, 31761, 30455, 29473, 30894, 30489, 29510, 30468, 29943,
         29473, 30959, 30973, 29493, 29473,  1008,   923,   956, 30004, 29473,
         30707, 30201, 30245,  1007,   904,   939, 30463, 30285, 29572, 29473,
         29783, 29473, 30959, 30973],
        [    1, 29473, 30402, 30547, 30421,  1006,   953,   900, 29473, 31780,
          1007,   927,   932, 32170, 29473, 29783, 29904, 29473, 30992, 30874,
         30992, 30707, 29473, 31547, 30001, 32170, 30083, 29473, 31491, 30627,
         29904, 30004,  1006,   909,   915, 30468, 30279, 29473, 31293, 30279,
         29903, 29473,  1006,   938,   913, 32645, 30351, 30056, 30468, 29473,
         30461, 31518, 30538, 29962, 29811, 29491, 29473, 30455, 30668, 29473,
         32575, 30306, 29473, 30083]], device='cuda:0')
[2025-05-09 11:45:56] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:56] [INFO] [nan 감지] labels: tensor([1, 0, 1, 1, 1, 0, 0, 1], device='cuda:0')
[2025-05-09 11:45:56] [INFO] [nan 감지] input_ids: tensor([[    1,  1619,  4598, 16658,  1032, 13487,  1394, 10402,  4649,  1070,
          1678,  7170, 12879,  1065,  1164, 16345,  6317,  4363, 29501, 18877,
          3758,  6703,  1608,  1632,  3884,  1072,  3577, 24222, 29491, 17180,
          1056,  1124, 13342, 29501,  2124,  3098, 18977, 29493,  4492,  1394,
         10402,  1072,  4873,  6145, 13920,  7058, 29493,  1946,  1274,  1518,
         13237,  1245, 14724,  7170,  7323, 13618,  4455,  1065,  1040,  4732,
          4565,  1070,  6551, 29491],
        [    1,  1133,  5382, 15002, 11701, 29515,  2450,  9408,  1062, 24303,
          2551,  1046,  1117, 16158,  1158,  1164,  1271,  7091,  4460,  1310,
         29508, 29516,  1995, 29518,  4369, 29501,  4045,  1369,  3667, 29491,
          1183,  2937,  4649,  2075,  8585,  2933,  6411,  1066, 10352, 17966,
          6550,  6925,  2212, 28219,  1062, 24303,  2551,  1046,  7651,  1927,
          1072,  1792,  1032,  4286,  1070,  1951,  1483,  1572,  1327,  7091,
          1317,  2761,  9197, 29491],
        [    1,  1183, 20126,  1845,  2798,  1070,  6441, 25434,  8049,  1060,
          8189,  7363,  1149,  1093, 29548,  2151, 29558, 29499,  1065,  1040,
         11786,  1091,  8753, 18677,  1274,  3495,  1164, 20259,  1845,  3468,
          1137,  7716, 26744,  5269, 29491,  1328,  1224,  6215, 29493,  1246,
          3852,  1032, 16081, 23862,  1070,  1040, 25687, 24168, 14932,  1162,
          2151, 29558, 18965,  1072, 12186,  7860,  6595, 14713, 29491, 10859,
          4302,  6411, 29493,  1246],
        [    1,  1328,  1224,  4598, 29493,  1246,  4110,  1040, 28273,  1065,
          3460, 29501,  2500,  3428,  8611, 29477, 29501, 18728,  2457,  7447,
          5832,  1093, 29511,  1411, 29477, 29501,  2862,  1325,  1164,  4577,
          1056, 11876,  1137,  1427,  1684, 29489,  1306,  6632, 14972,  3708,
          1066, 27283,  9438,  2107,  2027,  1158, 17415,  4237,  6907,  1093,
         29528,  2619,  1377,  1328, 21022, 29477, 29501,  2862,  1946, 23779,
         29493,  1133,  2619, 13825],
        [    1,  1299, 29473, 31727, 29783, 30809, 29473,  1007,   958,   943,
         30056, 29493, 29473,  1006,   938,   929,  1007,   905,   915, 29473,
         29783, 32350,  1006,   939,   955, 30072, 29473, 30004, 30515, 29916,
         29473, 30367, 30368, 30515, 29515, 29473,  1008,   928,   919, 30707,
          1006,   937,   951, 31346, 29572, 29473, 31078, 32109, 29473, 31727,
         29783, 30809, 29903, 29473,  1006,   938,   929,  1007,   905,   915,
         29473, 29783, 32350,  1006],
        [    1, 29473, 31310, 29493, 29473, 31744, 32578, 30338, 29962, 29811,
         29491, 29473, 30382,  1006,   930,   947, 30737, 29473, 30895, 30304,
         31780,  1007,   927,   932, 29955, 29473, 30778, 31780,  1007,   927,
           932, 29473, 29932, 30960, 29473, 31038,  1007,   953,   940, 29783,
         30468, 30279, 29473, 29904, 29943, 29473, 30601, 29473, 30588, 31859,
         32578, 30338, 29962, 29811, 29493, 29473, 31780,  1007,   927,   932,
         32170,  1006,   910,   923],
        [    1, 29473, 31104, 31047, 30092, 30394, 30778, 31083, 29916, 29473,
         32727, 29955, 29473,  1007,   933,   899, 29473, 30238, 30056, 32578,
         30338, 29962, 29811, 29491, 29473, 29518, 29502, 29508, 29518, 32247,
         30543, 30433, 29473, 32231, 30880, 31083, 30591, 30092, 29473, 30050,
         31547, 29932,  1007,   907,   931, 30062, 31878, 31205, 30001, 30584,
         29916, 29473, 30782, 30512,  1008,   902,   900, 30616, 30201, 30073,
         29473, 30004, 30862, 30616],
        [    1,  1619,  6215,  6080,  1164,  6411,  1124,  1040,  2165,  1062,
          3194,  1070,  1757,  2349,  8829,  1077,  6145, 15623, 29493,  5324,
          1169, 18168,  1219,  1592,  1193,  6145, 23292, 11981,  1093, 29537,
          5289, 29558, 29499,  1072,  2093,  1839,  2247, 27932,  1513, 23292,
         11981,  1093, 15923, 29558,  1325,  1065,  8269, 11608,  1129,  6145,
          2528,  1066,  1203,  1034,  1770,  1163, 23419, 29501,  1300, 29501,
          1099, 11656,  1124,  1441]], device='cuda:0')
[2025-05-09 11:45:57] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:57] [INFO] [nan 감지] labels: tensor([0, 1, 0, 1, 1, 0, 0, 1], device='cuda:0')
[2025-05-09 11:45:57] [INFO] [nan 감지] input_ids: tensor([[    1, 29473, 29518, 29502, 29508, 29550, 32247, 30367, 30543, 30433,
         29473, 30795, 30279, 31158, 31479, 31780, 29473, 30612, 30001, 30306,
         29473, 31159, 30421, 31780,  1007,   927,   932, 29957, 29473, 30668,
         31210, 30612, 30990, 30351, 29911, 29473, 29929, 30001, 29904, 30279,
         29473, 30372, 30338, 29962, 29811, 29491, 29473, 32526, 31104, 29916,
         30073, 29943, 29473, 32337, 31055, 31780, 30083, 29903, 29473, 29549,
         29502, 30723, 29473, 30001],
        [    1,  1619,  6215, 14734,  1032,  7860,  5199,  1066,  7167,  1040,
          2798,  1070,  7974, 16330,  1124, 24392, 11539,  5762,  2181,  5750,
         28898,  6411, 29491, 12418,  1055, 14296, 19025,  1274,  1518, 28585,
         20207,  3441,  1040, 22657,  1072,  2055,  6548,  1274, 14260,  5398,
         21056, 12876,  1066,  3730,  1935,  1566,  3296, 11142, 29491,  3155,
          1070,  1040,  7026, 11137,  1065,  1224,  2602,  1117, 24457, 16167,
          3370,  9693,  1137,  6034],
        [    1,  1133,  5382, 15002, 11701, 29515,  3155,  5450,  2035,  4352,
         29493, 23751,  2938, 11589,  1422,  1040, 19437,  5686,  1122,  1040,
          4649,  1070,  4867,  1050,  5008,  2929, 29491,  1183, 11137,  1065,
          1080,  3070,  1935,  2938, 11589,  4304,  1070,  1032, 23751,  6355,
         13433, 19514,  1066,  1032,  6907,  5851,  1539,  1124,  1032,  7768,
          2242,  1070,  1347, 29501, 12333,  2997,  5686, 29491,  9595,  8990,
          1228, 10334,  2611, 29501],
        [    1,  1299, 29473, 31113, 30325, 30004, 29493, 29473, 29904,  1007,
           931,   907, 29932, 29473, 31008, 31371, 30334, 32698, 29473, 32041,
         30482, 29473, 32464, 30394, 29473, 31113, 30325, 30004, 29943, 29473,
          1006,   927,   939, 30781, 31321, 29473, 30595, 31075, 29957, 29473,
          1006,   938,   929, 30304, 29473, 29904,  1007,   931,   907, 29932,
         29473, 31824, 30795, 29473, 31008, 31371, 30334, 32698, 29473, 32041,
         30482, 29957, 29473, 32464],
        [    1, 13396,  1040, 18367,  1070, 14018,  1521, 16008,  2355, 29493,
          1146,  1117,  8742,  1066, 12186,  7860, 12876,  1066,  8411, 16008,
          6131,  1065, 24266,  1093, 14075,  3069, 29499,  4120,  1639, 10661,
         29491,  2559,  1224,  1716, 29493,  1032, 20107, 29501, 18013, 16170,
          4649,  1171, 13584,  1065, 29473, 29508, 29508,  1484,  4865,  2424,
          8775,  3441,  7439, 22505,  6419,  3441,  1757, 10013,  1065, 14018,
          1163,  1040,  7271,  1070],
        [    1,  1328,  1224,  4598, 29493,  1246,  2718, 29197,  5486,  1343,
          5124, 11682,  5099,  1122, 28240,  1040,  3191,  5124,  1065,  8517,
          3716,  3698, 25963, 29491,  1183, 28829, 29197,  5486,  1117,  2075,
          1066,  3672,  1040,  2611,  4843,  1290, 23599,  6066,  1040, 29473,
         29538, 29525,  8734, 29491,  1183, 18815,  1070,  1040,  1757,  2349,
         20000,  1093, 29508, 29525,  1072, 29473, 29538, 29525, 29499,  1645,
         23201,  1614,  1032, 20694],
        [    1, 14057,  2362, 12556,  6092, 29515,  3413,  1042,  1240,  1117,
          1032,  5391,  7767,  1122,  6134,  8517, 18782,  1792,  2938,  1971,
         18839,  1120,  1093,  8724, 29521,  1377,  4791,  2192,  1143,  8205,
          1072,  1055,  2668,  1038,  2259, 14069,  1093,  6721, 29503, 29499,
          1117,  5791,  1072,  6413,  1065, 23424, 14413,  1586,  1042,  1240,
         29493,  1040,  4673,  1070,  1119,  9108,  1065,  1971, 18839,  7651,
         11120,  1066,  1451,  6071],
        [    1,  1299, 29473, 31167,  1005,   956,   904, 31293, 30172, 29473,
         30194,  1008,   901,   956, 29473, 30083, 31539, 29515, 29473, 30512,
          1006,   946,   956, 30072, 31043, 29473,  1008,   907,   943, 30062,
         30062, 29473, 30245, 30498, 29955, 29473, 30795, 30285, 30233, 29473,
         31078, 32109, 29473, 31167,  1005,   956,   904, 31293, 30172, 29473,
         30194,  1008,   901,   956, 29473, 30083, 31539, 30306, 29473, 32204,
         30498, 30499,  1008,   914]], device='cuda:0')
[2025-05-09 11:45:58] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:58] [INFO] [nan 감지] labels: tensor([0, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')
[2025-05-09 11:45:58] [INFO] [nan 감지] input_ids: tensor([[    1, 29473, 29508, 29542, 29502, 29502, 32247, 29916, 29473, 30073,
         32526, 29955, 29473, 30092, 30547, 29903, 29473, 29518, 29502, 30525,
         29783, 31359, 30338, 29962, 29811, 29491, 29473, 30382,  1006,   930,
           947, 30737, 29473, 29518, 29502, 29502, 29502, 32247, 29916, 29493,
         29473,  1006,   919,   948, 29473, 29508, 29502, 29502, 32247, 29783,
         29473, 29929, 32644, 29473, 29811, 30649, 29916, 29473, 30073, 32526,
         29955, 29473, 30092, 30547],
        [    1,  1299, 29473, 31859, 31155, 29473, 30973, 30515, 29493, 29473,
         30092, 30880, 29955, 29473,  1007,   905,   915, 32038, 29473, 30731,
         29783, 29473,  1007,   926,   899, 31660, 29783, 29943, 29473, 30735,
         30657,  1113, 31859, 31155, 29473, 30973, 30515, 29473, 30092, 30584,
         29473, 29502, 30723, 29507, 29783, 30468, 29943, 29473,  1006,   901,
           946, 30668, 29473, 30959, 30973, 29783, 29473,  1006,   921,   931,
         30415, 31859, 29943, 29473],
        [    1, 29473, 29550, 29591, 29473, 30735, 30525, 29783, 29929, 30285,
         29572, 29473, 30245, 30056, 31502, 29473,  1006,   900,   918, 30595,
         29473,  1006,   939,   956, 30279, 29473, 30382, 31207, 31346, 29929,
         29473, 29904, 30333, 29473, 29550, 29502, 29591, 29783, 30279, 29493,
         29473, 29550, 29502, 29591, 29955, 29473, 31547, 32231, 30707, 30306,
         29473, 31663, 29957, 29473, 30083,  1006,   939,   956, 31383, 29929,
         29473, 30382,  1006,   931],
        [    1, 29473, 30325, 32441, 30192, 29473, 30512, 30584, 30197, 29962,
         29811, 29491,  1292,  8724, 29473, 30489, 31258, 31371, 29473, 30894,
         30826, 29473, 30478, 30294, 29493, 29473, 30766, 29943, 29473, 30960,
         30238, 30004, 29473, 30894, 30826,  1006,   914,   959, 31031, 29473,
         30665, 29811, 30279, 29473, 30461, 31518, 29904, 30279, 29473, 30679,
         30183, 29473,  1007,   953,   940,  1006,   953,   900, 29473, 31547,
         32170, 29916, 29473, 29903],
        [    1,  1183,  6800,  1070,  1224,  4826,  1117,  1066, 18830,  1421,
         18652,  1273, 29493,  1514,  8614, 29493, 10977,  4935,  1044, 29493,
          1072,  8460,  1050, 10878,  1070,  1080,  6414,  1263, 21608,  3151,
         15467,  1065, 23905, 29493, 20351, 29493,  1344,  2217, 29493,  1052,
         12646, 29493, 12706, 29493, 10308, 29493,  1072, 18865, 29491,  1183,
         15961,  2937,  1465,  1070,  8798,  3708,  1066,  1080,  6414,  1263,
          3207,  1244,  1364,  1228],
        [    1,  7363,  7680,  1274,  1518, 14867,  1451, 19158,  1420, 15079,
          8589,  2006,  2622,  1040,  1675, 20219, 13799, 14291,  1254, 22199,
          5008,  2929, 29501,  6295, 14713, 29491,  1183, 14408,  1070,  1224,
          4598,  1117,  1066, 17982,  1678,  8460,  7680, 19664,  5398, 15341,
         15079, 26766,  1245, 20434,  2706,  3207,  9139, 15467,  1072,  1827,
         10963,  1396, 10305, 29493,  1163,  1032,  4000,  1124,  5959,  1056,
          1475, 10646,  3268,  1065],
        [    1, 29473, 29929, 30083,  1006,   910,   923,  1005,   958,   923,
         30073, 29473, 30382, 31539, 29473, 32252, 29473, 29904, 30004, 29943,
         30737, 29473, 30382, 29811, 30649, 29916, 29473, 30421, 29932, 29473,
          1006,   953,   900, 30543, 30333, 30919, 29955, 29473, 29549, 29550,
         29591, 29903, 29473, 32440, 30083, 29473, 32750, 30294, 29916, 29473,
          1006,   943,   953, 30595, 29473, 30372, 29811, 29493, 29473, 32440,
         30083, 29473, 32750, 30294],
        [    1,  1113,  5715,  1265,  1188,  4283,  3186,  2507,  5103, 10265,
          5636,  2324, 28356, 22060,  1042, 29507,  1619,  4598, 16658,  1032,
          1401, 14374, 22305,  1070, 16028,  4100, 20991,  1137,  7786,  5706,
          1309,  1115, 12234, 17034,  3441,  2349, 19165, 20000,  2439,  6632,
          5088,  1070,  1055,  3331,  2356,  1240, 29491,  4791,  6353,  7961,
          1274,  4625,  6334,  1122,  1040, 10833,  1070,  6766,  1467,  1075,
          6448, 29501,  6295,  7786]], device='cuda:0')
[2025-05-09 11:45:59] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:45:59] [INFO] [nan 감지] labels: tensor([0, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')
[2025-05-09 11:45:59] [INFO] [nan 감지] input_ids: tensor([[    1,  5718,  1513,  2487,  1241, 22809, 15790,  1093,  6926, 29533,
         29558, 29499,  1117,  1164,  3046,  3207,  9139,  1065,  1040,  2487,
          1241,  5547, 29491,  7363,  1064, 23185,  1117, 18054,  1369,  1254,
         24399,  2864,  1894,  1181, 29500, 29480,  3693, 29499,  1072,  1181,
         29518, 29493,  1072,  1181, 29518,  1117,  1944, 11159,  1364,  1290,
         29491, 10384,  1032,  4068, 29501,  4022,  1923,  1107,  1163,  2896,
          1205,  1059,  1181, 29518],
        [    1, 26369,  7572,  1087,  1114,  6600, 29492,  2233,  1070,  4369,
          6207,  1087,  1114, 12685,  5007,  1030,  3708,  1066,  3716, 14264,
          1465,  1070,  1087,  1114,  6600,  3719, 20281,  1323,  7123,  1117,
          5080,  6131,  1163, 19561,  1072,  2192,  1297, 23083,  1070,  9643,
         29493,  7079,  1070,  5903,  1202,  1062,  2021, 21011, 24624, 29491,
          3306,  1339,  1215,  1315,  1186, 29501,  1091,  1067,  3719,  8614,
          1340,  1153,  9023,  3719],
        [    1, 29473, 30285,  1007,   937,   923, 29473, 31663, 29903, 29473,
         30346, 32441, 29783, 29473, 32755, 30194, 30073, 29473,  1006,   947,
           936, 30523, 30367, 29473,  1005,   948,   948, 30001, 29783, 29473,
         30616, 29943, 29473, 31801, 29783, 29473,  1006,   938,   913, 30304,
          1007,   932,   911,  1007,   934,   931, 29491, 29473, 32452, 32677,
         29473, 31123, 30782, 30989, 29473, 29903, 31878, 29783, 29473, 32331,
         31158, 29904, 30333, 30073],
        [    1, 17550,  1263,  6411,  1070,  3207,  1244,  1364, 13866,  3698,
          1072, 13768,  4007, 19025, 15815,  9189, 29493,  3716,  1072, 14277,
         15877,  6330,  1458,  1309,  1115,  8357,  1065,  4100, 25351,  1576,
          9791,  1072,  1065,  2602,  1122, 12703, 22735, 29491, 20806,  2830,
          1050,  5467,  1055,  3574, 29366,  1093, 29526, 14809, 29481, 29499,
          1274,  1518,  7508,  1158, 14767,  7808,  1122,  2027, 14073,  1513,
          9197, 29491,  1328,  1040],
        [    1,  1133,  5382, 15002, 11701, 29515,  9975,  2359,  1062,  1241,
          6519,  1032,  5610,  6469,  1066,  5684,  7386,  3003,  1072, 11066,
          1122, 26744,  8517,  2424, 29369,  1491, 29491,  1328,  1164, 12611,
          6355, 29493,  1146,  1427,  1518,  2075,  1066,  2528,  1675, 10189,
          1172,  1254,  8269, 16197,  1210,  4566, 25648,  1163, 29143,  1206,
         22505,  1072,  1827,  1040,  9645,  1070,  1478, 29484,  5766,  8517,
          2639, 29491,  1619, 27283],
        [    1,  2066,  4945,  1971,  3257,  1323,  1093,  7810, 29499,  1070,
          3698,  5261,  1138,  1341, 29490, 10143, 15790,  1093, 29537,  6810,
         10340, 29508,  9696,  1032,  2713,  4673,  1065,  9524,  1845,  8460,
          1050,  1080,  3227,  1072,  1117,  1164,  3046,  3486,  1122, 11191,
          7659, 29501, 29537,  6810, 11515, 29491,  4257,  4222,  4649,  7410,
          1137,  1757,  5316,  1465,  1093, 29570, 29518, 29555, 29508, 29509,
          1072,  1083, 29518, 29555],
        [    1,  1619,  4598, 16658, 18915,  1245,  1032,  8585,  2933, 29501,
          6295,  1971,  3257,  1293,  6411,  1070,  1040, 21698,  3667,  1066,
         18965,  1070, 13856,  7026,  1049,  1184,  1640,  1062,  9662,  1163,
         17759,  1693, 14921,  1106, 18196, 15790, 29491,  1328,  3561,  1693,
         14921,  1106, 18196,  1117,  1164,  1950,  1521,  8798, 28585, 14163,
         26192,  1364, 29493, 13866,  9280,  2061,  9887,  5653,  1072,  8460,
          1050, 11608,  1129,  6145],
        [    1,  1862,  3493,  1459, 29512,  1081,  1208, 12928, 28737, 20670,
         29493,  1384,  1037,  1153,  3719,  3921,  1170,  1595, 29474,  1030,
          1323,  1093, 21821, 29485, 29499,  1171, 11016,  1245,  1040,  6353,
         14147,  1971,  3257,  1293,  8267,  1070,  2438,  1129,  1121,  2148,
         29493,  1738,  1146,  1171,  3824,  3098,  1065,  1032, 11345,  1489,
          1092,  5947, 28411,  5412, 29491,  4440,  1040,  1098, 29537, 29485,
         15079, 29493,  1032,  2509]], device='cuda:0')
[2025-05-09 11:46:00] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:46:00] [INFO] [nan 감지] labels: tensor([0, 0, 1, 0, 0, 1, 0, 0], device='cuda:0')
[2025-05-09 11:46:00] [INFO] [nan 감지] input_ids: tensor([[    1,  3298,  5533, 12177, 18965, 29493,  2175,  1233, 29501, 24006,
          4537,  1047,  3573,  1273,  1198,  3151,  9826,  1093, 29526,  5158,
         28014, 29499, 23388,  1032, 10904,  3667,  1066,  1080,  2746,  6585,
          4537,  1047,  3573,  1273, 27990, 29515,  4621,  1845,  1032,  3048,
          1106, 29501, 24519, 29473, 30243, 29527, 29488, 29552, 29538, 29516,
          3474, 11946,  1031, 13612, 29473, 29550,  1093, 29564,  3344, 29550,
         29499,  1771,  1118,  4340],
        [    1,  1183,  4916,  2243,  8460,  1121,  1404,  7767,  1070,  3698,
         29501,  2508,  9139,  1062,  6638,  2139, 29476,  1134, 16304, 29484,
          1125,  1062,  1181,  3220,  1031,  1307,  1283,  1758, 29478,  1093,
         29517,  2589, 29511, 29499,  1117,  2063,  8001,  1066, 29512,  1030,
          1093,  1486, 29512,  1325,  1458, 10878, 14413, 10005,  1194,  2381,
          2243, 10536,  1465,  3258,  7139,  3473,  8487,  1254,  3975,  4822,
          1080,  3108,  1716,  1090],
        [    1,  1183, 22136,  1070, 22261,  1562,  1323, 29222,  2066,  2542,
          1093,  4967, 29522, 29499,  1066, 11797,  1421,  1040,  9189, 20219,
          1070,  1176, 15944,  1394, 16727,  1427,  1518, 18561,  1065,  1032,
          7445,  1070, 25687, 10572, 29491,  1619,  4649,  7462,  1770,  1040,
         19077,  1072,  2165,  1062,  3194,  1070,  2349,  1135,  6496, 29501,
          6295, 12876,  1122,  7473,  1056,  1176, 15944,  1394,  1129,  7680,
         29493, 19087,  1124,  2121],
        [    1,  1619,  4598,  4350, 29482,  8905,  6401, 20209,  1122,  1168,
          3844,  1933,  1062,  1072, 28909,  5008,  1170,  7749,  5303,  3837,
          1065,  6875,  5545, 12238, 29501, 24080,  1042,  1093, 29521,  1128,
          1178,  1149,  1032,  1612, 29477, 29499, 14150,  1065,  4426,  6459,
         29304, 29493,  7432, 29491,  4242,  1907,  1422,  2187,  1066,  1115,
         11420,  1163,  2163,  1070,  1567,  1135,  1128,  1178,  1149,  7786,
         29491, 10379,  5450,  1072],
        [    1, 29473, 29783,  1007,   949,   931,  1008,   928,   943, 30197,
         29962, 29811, 29491, 29473, 30766, 29943, 29473, 29518, 29502, 29508,
         29552, 32247, 29916, 29473, 30004, 30549, 30665, 29473, 31637, 31266,
         31491, 29473, 30368, 29473, 31205, 30083, 29473, 30627, 30056, 29473,
         30992, 32376, 30194, 30073, 29473, 32727, 30478, 29957, 29473, 30238,
         30056, 32578, 30338, 29962, 29811, 29491, 29473, 31637, 31266, 31491,
         29473, 30368, 29473, 31205],
        [    1,  1619,  6215, 16658,  1032, 11305, 11679,  1070,  1040,  2636,
          4193,  1072,  5396,  1122,  1040, 20219,  1072,  5247,  2605,  1070,
         23751, 12004,  3493,  1089,  3898, 29494,  1273,  1391,  5415,  2603,
          5868,  3207,  1244,  1364, 29491, 28041,  4018,  1070, 16923, 27520,
          4908,  4589,  1089,  3898, 29494,  1273,  4648,  5415,  1122, 18812,
         11996,  1244,  1364,  6519,  3046, 16932,  1546,  2328,  1198,  1489,
          3719, 20353, 12876,  1066],
        [    1,  7363,  1050,  1065, 24266,  1228,  3419,  1066,  1115,  6131,
          1163, 14413,  1207,  9940, 29494,  1465,  1070, 10365, 29484,  1473,
          1065,  3204, 29491,  1328, 10117, 29493,  1504,  1117,  7284,  1946,
          1137,  8460,  1050,  1065, 24266,  1451,  3424,  1951,  2491, 17910,
          1070, 10365, 29484,  1473,  1137,  3467, 12611,  2424,  1065, 12879,
         29491,  2559,  8423,  1040,  4673,  1070,  8460,  1050,  1065, 24266,
          1065,  1207,  9940, 29494],
        [    1, 29473, 30294, 29903, 29473, 31547, 30001, 32170, 30083, 29903,
         29473, 30004, 30549, 30616, 29932, 29473, 30368, 30543, 30433, 29473,
         29932,  1008,   925,   912, 30782, 30001, 30543, 29916, 29473, 30062,
         30989, 29473, 30285, 30547, 29968, 29473, 31444, 31359, 30338, 29962,
         29811, 29491, 29473, 29538, 29551, 30661, 29955, 29473, 30062, 30989,
         29968, 29473, 30285, 30547, 29968, 29473, 31444, 31359, 29943, 30737,
         29473, 30382, 30795, 29916]], device='cuda:0')
[2025-05-09 11:46:01] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:46:01] [INFO] [nan 감지] labels: tensor([0, 1, 1, 1, 1, 1, 0, 0], device='cuda:0')
[2025-05-09 11:46:01] [INFO] [nan 감지] input_ids: tensor([[    1,  1328,  1224,  4598, 29493, 24392, 11539,  6817,  1163,  1040,
          1109,  3337,  5865,  1064,  2997,  1124,  3921, 24007,  1072, 24170,
         24007, 12935,  1117, 10372, 29491,  4973, 29493,  1040, 24392, 11539,
         14821,  1070,  1040,  4356,  1109,  3337,  5865,  1064,  2997,  1117,
          2846, 29491,  3247,  1040,  1109,  3337,  5865,  1064,  2997,  1163,
          6718,  1072,  3936,  1117,  1603,  5293, 29491,  1183,  1990,  1738,
         17759,  3342,  1117,  1032],
        [    1,  1619,  4649, 15269,  1040,  3764,  1070,  3929,  5103,  8654,
         19602,  3915,  1092,  1068,  1149, 13491, 11162, 12292,  1430,  1089,
          3445,  1093, 29506, 29527,  3023, 29499,  1309, 12234,  8411, 21608,
          1489,  1125,  1283,  1065,  7651,  1163, 26761,  1056, 26005, 29491,
          1152,  1614,  1489,  1125,  1283,  1117,  1032,  7184,  6433,  1077,
         24019,  5412, 29493,  1458,  1309, 11885, 18657,  1040,  4813,  1070,
          2179,  1122,  2163,  1461],
        [    1,  1619,  1990,  3032,  5733,  2829,  1124,  1032, 10732,  5412,
         15739,  1164, 27325,  2333, 28464, 21949, 25400,  1250, 13908,  1254,
          1029,  1067,  1044,  1149, 12914,  1083,  1065,  1164,  1289,  9413,
          1138,  2100,  2270, 29491,  1328,  2513,  1066,  8050,  1040, 21425,
          2929,  1070,  4694,  6142,  1066,  1224,  5412,  1072,  1639, 14888,
         29493,  1246,  4016,  2004,  1032,  4826,  1070,  5970,  6482, 12122,
         29491,  1183, 14408,  1070],
        [    1,  1299, 29473,  1008,   911,   943, 30737,  1006,   946,   956,
         29473, 30004, 30402, 29493, 29473, 30627,  1007,   953,   920, 31663,
         30062, 29473, 30778, 32331,  1006,   938,   928, 29955, 29473, 30795,
         30285, 30233, 30612, 29473, 29550, 30661, 32247, 29473, 30874,  1008,
           925,   912, 29473, 29919, 32519, 29473, 31078, 32109, 29473, 31061,
         29911, 30463, 29508, 29542, 29473,  1008,   911,   943, 30737,  1006,
           946,   956, 29957, 29473],
        [    1, 29473, 31078, 32109, 29473, 30092, 30778, 29929, 31132, 29473,
         29932,  1007,   907,   931, 29955, 29473, 31250, 30368, 30351, 29911,
         29473, 30092, 30194, 29473, 29911,  1006,   951,   906, 29783, 29473,
         31357, 31357, 29473, 31287, 29473, 30973, 30759, 30919, 30351, 29911,
         29493, 29473, 30382, 30056, 30279, 29473, 30455, 30056, 29473,  1007,
           901,   953, 29916, 29473,  1005,   956,   909,  1007,   907,   924,
         29783, 29473, 30072, 32486],
        [    1,  1619,  4598, 16658,  1040, 14888,  1070,  1040, 10344, 10758,
          1070,  1188,  1097, 20939,  1902,  4196,  1065, 17541,  1513,  8710,
          2235,  1093,  2362, 29511,  1921, 29499,  9838, 29493,  1458, 26249,
          1040,  6860,  1070,  3703,  1072, 26627,  3115, 29478,  8607, 13006,
          1124, 17260,  8585,  1902,  4196,  1065,  3958,  2712,  4552,  7651,
         29491,  1183,  6277,  3226, 29493,  2401,  2566, 29501,  2167,  8548,
         29493, 13403,  2839,  4649],
        [    1,  1133,  5382, 15002, 11701, 29515,  4719, 15541, 23859,  1093,
         12509, 29499,  1117,  6284,  1056,  1040,  2527,  1070, 11237,  4100,
         29491, 16875, 29493, 22849,  1163, 19626,  1070,  3243, 27650,  1072,
          7256,  4401,  2017,  2750, 29493,  1117, 13825,  1845,  5865,  1065,
          5788,  2027,  1158,  3420, 24910, 29493, 12027,  3036,  1072, 16079,
         16399,  1501,  7715,  1092, 25125, 29473, 29518, 29502, 29508, 29542,
          5834, 10644,  1088,  1236],
        [    1,  1133,  5382, 15002, 11701, 29515, 24286,  1198,  4974,  1272,
          1062,  5762,  1122,  8445,  1091,  1165,  2241,  1050, 15260, 16304,
         29484,  1233,  1093,  2372, 29537, 29499,  1274,  3253,  1072,  4356,
          4958, 29493, 15117,  1484,  4865,  2424,  5796,  1093,  2372, 29547,
         29499,  3193,  1240, 20000,  3792,  1448, 13908,  6055, 29491,  4895,
          2838, 13254,  6970, 19355, 29547,  3193,  1240, 20000,  1274,  8798,
         29501, 16358,  6276, 29493]], device='cuda:0')
[2025-05-09 11:46:02] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:46:02] [INFO] [nan 감지] labels: tensor([1, 1, 0, 0, 0, 1, 1, 1], device='cuda:0')
[2025-05-09 11:46:02] [INFO] [nan 감지] input_ids: tensor([[    1,  1299, 29473, 30050, 30735, 29473, 30992, 30489,  1008,   923,
           916, 30346, 29493, 29473, 30782, 31217, 31468, 30346, 29473, 29518,
         29518, 29473, 31266, 31123, 30351, 29911, 29473, 30782, 30661, 29515,
         29473, 31761, 29911, 31321, 29473, 30004, 30402, 29955, 29473, 31217,
         31371, 29473,  1008,   923,   916, 30394, 29957, 29473, 32228, 30194,
         29473, 30050, 30735, 29473, 31835, 31547, 30306, 29473, 29518, 29518,
         32247, 29473, 30525, 29916],
        [    1,  1619,  4598,  1391,  5353,  1040,  7550,  1263,  4484,  1240,
          1070,  1040,  7430,  1068,  3894, 12412,  6056,  5252, 23246, 25776,
          1066, 16395,  1164,  4003, 29510, 29481, 22655,  5391,  1507,  7056,
          1056, 17686, 29501,  6295, 14850, 29491,  1183, 19077,  1070,  1224,
          4649,  2614, 29481,  1065, 22122, 10053,  3655,  8059,  1072, 17800,
         10519,  1396, 15447,  1158, 20787,  2274,  5191,  1658,  2236,  1228,
          9402,  3708,  1066, 12027],
        [    1,  1328,  1040,  6130,  3526, 29493,  1183,  9622,  1217,  2017,
         25961,  1265,  1122, 18272, 23246, 25776,  1093,  4245, 29526, 29592,
         29499,  1117, 23764,  3028,  2075,  1065,  8852,  1056,  1776, 29501,
          2144,  7200,  5936, 12876, 29491,  3761, 29493,  1448,  4100,  1117,
          4004,  1066,  3730,  3929,  1146,  1117,  9575,  1122,  6096,  6130,
         29491,  1183,  2717,  1039,  6145,  1072,  9503,  6145,  7767,  6411,
          1422,  2075,  1066,  2137],
        [    1,  1102,  3927,  3236,  3592,  1760,  1234,  1117,  1032,  4066,
          6032,  9230,  1137,  1427,  1518,  5662,  1066,  6032,  1065,  1522,
         29473, 29550, 29502,  2007,  1070,  7651,  3064,  8044, 13982, 29491,
          1619,  3592,  1760,  1234, 12790,  3441,  1032,  3587,  1070, 26766,
          3258,  5269, 29493,  5501, 29493, 10225,  5743, 29493,  1072, 12980,
         21608,  3396, 29491,  1328,  1224,  6215, 29493,  1246,  1390,  4110,
          1040,  3207,  3894,  1614],
        [    1,  1328, 29473, 29518, 29502, 29502, 29555, 29493,  1040,  2294,
         20945,  1040, 29473, 29550, 29502,  1130, 21869,  1070,  1040, 15811,
          1070,  1559,  1410,  1034,  1093,  4834, 29527, 29499,  1254, 18346,
         24184,  1072, 10868,  1037,  7250, 29491,  5846, 24089, 29493,  1040,
         14952, 29527, 29501, 29716, 17966,  1171,  1301, 13648, 29493,  6662,
          6099,  5465,  1072, 14952, 29527, 29501, 29716,  1171,  7531,  1065,
          1080,  1443,  6489,  1208],
        [    1,  1619,  4598,  2717,  4394,  1040, 11137,  1072, 11886, 12876,
          1070, 25411,  5398,  2528,  3976,  1507, 17254,  3286,  1673,  1122,
         27591,  1065,  2179, 29491,  5103,  1453,  1056,  1066,  1384,  1121,
          1130,  1772,  1117,  1032, 13496,  2634, 20896,  2527,  1787,  2439,
          4654, 10426, 29493,  3551,  2055,  3286,  1673,  1163,  1472,  8541,
          3467,  5638, 13851,  1070,  3609,  3226,  2424,  2706,  1224,  8044,
          1495,  3984, 29491,  6568],
        [    1,  1619,  4649,  7462,  1770,  1040,  4673,  1070,  1102,  6496,
         29518,  1065, 21301,  2603, 22920,  1459, 15790,  1093, 20346, 10340,
          1275, 23101,  1249,  1126,  1489,  1050, 18196, 29491, 10384,  1065,
         12549,  1079,  1072, 13501,  5762, 29493,  1246, 17267,  1137,  1040,
          8230,  1261,  1241,  1080, 19701,  1102,  6496, 29518,  1117,  1350,
          2144,  7200,  2706,  1167, 29558, 29501,  1030, 12032,  1072,  9696,
          1032, 25930,  1676,  1254],
        [    1,  1299, 29473, 30368, 29932, 31502, 29473, 31171, 30433, 30056,
         29473, 29783, 30346, 29473, 32170, 29929, 29493, 29473, 30499, 30782,
         29473, 31310, 31008, 29957, 29473, 30512, 30050, 29473, 30004,  1006,
           949,   919, 29473, 30083, 31047, 29473, 30759, 30004, 29473, 31078,
         32109, 29473, 30368, 29932, 31502, 29955, 29473, 30092, 29932, 29903,
         29473,  1006,   905,   917, 30304, 29929, 30333, 30073, 29473, 31171,
         30433, 30056, 29473, 30992]], device='cuda:0')
[2025-05-09 11:46:03] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:46:03] [INFO] [nan 감지] labels: tensor([1, 0, 1, 0, 0, 0, 0, 0], device='cuda:0')
[2025-05-09 11:46:03] [INFO] [nan 감지] input_ids: tensor([[    1,  1183,  7160,  9290,  1070,  7212, 29478,  2929,  1093,  3023,
         29522, 29499, 12831, 13191,  1455, 19615,  6694,  6683,  7531,  2257,
          6013, 19660,  1066,  5684, 18965, 28307,  1072,  3370,  1065, 10039,
         13191,  1455,  4577,  1056, 29491,  1619, 25105,  7019,  1343,  8831,
          1040,  2636,  9064,  1122, 29466,  4813, 29501,  1257,  3098,  4942,
         10203, 29493,  4110,  1042, 12876,  1122,  6341,  1056, 20821, 20611,
         29493,  1072,  6080, 13070],
        [    1, 25961,  1265,  1122,  8644, 15811,  1072,  1080,  4408,  1056,
          1228,  1164, 26744,  1695,  1163,  3884,  1066, 12515, 29501, 29508,
         29542, 29491,  1584,  6970,  1113,  1664,  4052,  1854, 29501, 29518,
         29502, 29518, 29502,  1316,  1032, 11746,  1070,  6367,  5936,  5762,
          1122,  6778,  1845,  2509, 12928, 28737,  7123,  1245, 25687,  5461,
         29493,  1122,  1032,  3587,  1070,  1086,  1855, 29503, 29501,  7938,
         29558, 29501, 29518,  5970],
        [    1,  1183,  1559,  2442,  2212,  3202,  1072, 24566,  1070,  5533,
         12177, 11748,  2071,  1427,  1518, 13343, 10372,  1065, 13967,  1066,
          5995,  3296, 10854, 29491, 10087, 29501, 10614, 10760,  6411,  1117,
         13343,  8357,  1066,  6034,  1040,  5856,  1070, 11748, 10203, 29493,
          1330,  1146,  1309,  1115,  7284,  1507,  4942,  4806,  5398, 22909,
          1210,  5934,  1851,  6719,  7109, 29491,  2559, 24675,  1224,  5059,
          1072,  2641,  3148,  1678],
        [    1,  1183,  4795,  9413,  2570,  8798,  3419,  1158,  6849, 29501,
         29508,  6046,  7659, 29501,  4235,  1216,  4871,  1323, 28249,  1093,
          5584, 29499,  1117, 23868,  1254, 15860,  1845, 22447,  1942,  5882,
          1172,  1066,  2776,  1081,  3719, 29501, 29475, 29522,  4152, 14374,
          4871,  1323,  1093, 29537,  1855, 29503,  1325,  1458,  1761,  1924,
          1032,  4673,  1065,  1736, 14390,  1040,  2611, 29501,  4998, 16334,
          6340,  1070,  1150,  1855],
        [    1, 24651, 12314,  8906, 29515,  2903,  1062,  4429,  7229,  1120,
         15003,  1066, 13139,  2715, 10857,  1117,  8159,  1066,  9718,  1079,
          1489,  1077,  1093,  9365, 29527, 29499, 21608,  2144,  3445, 23868,
          1254, 10165,  1186, 29533, 29501, 29497, 29528, 29501,  2049,  3501,
          1072,  1045,  5243,  1261,  1241,  6550, 29491,  1183, 16330,  1070,
          1186, 29533, 29501, 29497, 29528, 29501, 11143,  1120,  1158,  1930,
          1158,  1639,  1828, 29501],
        [    1,  2760, 29488,  1129,  6145,  1065, 24266,  1228,  4066,  1478,
         14953,  1502,  1066, 10365, 29484,  1473,  1207,  9940, 29494,  1465,
          1065,  3204, 29493,  1330, 25687, 21698, 15683,  1137,  8423,  3929,
          1072,  1678,  1164, 18965, 10878,  1164,  1207,  9940, 29494,  1120,
          1228, 24890,  9756, 29491,  3231,  2181,  5686, 29501,  6473,  4449,
          6411, 29493,  1246,  9819,  1080,  1255,  1300,  3821,  1070,  4369,
          1866,  1971,  3110,  1050],
        [    1, 29473, 30304, 30871, 29473, 30515, 30992, 30367, 29473, 30482,
         29955, 29968, 29473, 29904, 30004, 29943, 30737, 29473, 30382, 30482,
         31093, 29916, 29473, 30201,  1006,   921,   935, 29473, 30183, 29957,
         29473, 29904,  1007,   904,   939, 29943, 29929, 29903, 29473, 32227,
         29493, 29473, 31663,  1006,   932,   931, 29473,  1007,   908,   960,
         29929, 29473, 30872, 29811, 29943, 29473, 31207, 30306, 29473, 31744,
         30279, 29473, 30372, 30338],
        [    1, 29473, 30382, 29473, 29783, 30895, 29943, 29473,  1006,   944,
           915,  1006,   902,   915, 29473, 29904, 30333, 29473, 30668, 31113,
         31547, 30306, 29473, 30368, 29473, 31547, 32231, 29783, 29473, 29811,
         29473, 29903, 30197, 30616, 30201, 29473, 30372, 30279, 29473, 30543,
         30612, 31123, 30351, 29911, 29473, 30194, 30073, 29473, 30382, 29473,
         31547, 32231, 30707, 29473, 29508, 29502, 29502, 29591, 29916, 30601,
         29473, 29811, 29473, 30875]], device='cuda:0')
[2025-05-09 11:46:03] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:46:03] [INFO] [nan 감지] labels: tensor([1, 1, 1, 1, 0, 1, 1, 0], device='cuda:0')
[2025-05-09 11:46:03] [INFO] [nan 감지] input_ids: tensor([[    1,  1619,  4649,  4942,  2071,  1040,  1706,  1070,  1135,  2862,
         29501,  2154,  4577,  1056,  1065,  8852,  1056,  1073,  1850,  1489,
         24624, 29493,  1032,  2839,  1070,  1309, 15916, 15241, 28585,  1040,
          2955, 29577, 29481,  1073,  1850,  1489,  2479,  2355, 29491, 10859,
         12059,  6411,  1070, 29473, 29508, 29518, 13618, 27397,  1163, 23080,
         16128,  1070,  1150,  1118, 29489,  8278,  1072,  9955, 29501, 29537,
          1118, 29489,  8278,  1161],
        [    1,  1619,  6215,  6080,  1164, 23851,  1070,  1678,  1032,  4294,
         24824,  1070,  5556, 29493, 14864,  2899,  1289,  7077,  2061,  3207,
          2272,  1072, 11171,  1124,  7459, 29493,  1597,  1115, 12779,  1827,
         22501, 15965, 10572, 29491,  4243, 29506,  1104,  2946,  1164,  1703,
          7077,  2061,  7790,  1070, 10091, 29577,  4948,  2145, 29473, 29538,
          6008, 29501, 15767, 18039,  1158,  1032,  3593,  1066, 17900,  2636,
         15609,  1065,  1040, 11237],
        [    1,  1619,  6215,  2717,  4394,  1040, 21215,  1070,  2864,  2786,
          1072,  1420, 23126,  9826,  1065,  1040,  3207,  9139, 15467,  1070,
         21156,  1586,  2124,  1263, 10789,  3408,  1396,  8798,  1093,  2766,
          9036,  1377, 17340, 29525,  1117,  1032,  5398,  1072,  3299,  1103,
          7643,  1273,  3312, 10447,  1240,  1163,  3592, 27951,  7659, 29501,
         22023,  1082,  3844,  1463,  3667,  1158,  1639, 23397,  5248, 29491,
         28495,  7961,  1274, 14843],
        [    1,  1619,  4826, 21933,  1124,  1040, 23238,  1070,  1088, 29501,
          5628, 15683,  1254, 13496, 29033,  1072,  8798,  6131,  4795,  1208,
          1361,  1118,  1265,  1137, 13133,  1066,  1045,  5243,  1261,  2071,
         29491, 28495,  6334, 13076,  1137,  1935, 22447,  1118,  1265, 29493,
          2937,  1065, 11357,  1072,  1567, 28222,  1741,  1924,  1032,  8044,
          4673,  1065,  1751, 14390,  1040,  2798,  1039,  6340,  1070,  1088,
         29501,  5628, 29481,  6142],
        [    1, 17325,  1775, 21856,  7168,  1228,  1227, 20318, 12779,  1254,
          6347,  6317,  7765,  1346,  6450, 27631, 29491,  1219,  1585, 29477,
          1056, 17650,  1072,  6179,  1070,  1040,  1055,  3331,  2356,  1240,
          1065,  2163, 27631,  1117,  1164, 14155,  3046, 29493,  1330,  3376,
          4212,  3112,  1210, 24890, 16499, 29493,  1512,  1070,  1040, 21856,
          2527, 29491,  1328,  1224,  4598,  1246,  3730,  1392,  1070,  1040,
          1675,  1072,  1848,  3046],
        [    1,  1299, 29473, 30992, 30489, 29473,  1008,   914,   944,  1008,
           917,   912, 29473, 31341, 29473, 32074, 32644, 30969, 29493, 29473,
         30627, 30172, 31061, 30092, 29473, 29518, 32247, 30880, 29473, 29549,
         29508, 29502, 29591, 29473, 30346, 32441, 30050, 29473, 29783, 30895,
         29473, 31078, 32109, 29473, 31122, 29911,  1006,   949,   911, 29473,
         31099,  1007,   927,   952, 29473, 30004, 30515, 30306, 29473,  1006,
           900,   909, 31288, 30731],
        [    1, 29473,  1006,   951,   903,  1005,   960,   902, 29783, 29473,
         32074, 30201, 30463, 29932, 29473, 30004, 30549, 30050, 29473,  1008,
           921,   906,  1007,   901,   951, 29473, 29903,  1006,   918,   928,
         30050, 29473, 31801, 29493, 29473, 31858, 30062, 29932, 29473, 29904,
          1006,   909,   923, 29783, 29473, 30201, 31377, 31894,  1007,   932,
           911, 29811, 29491, 29473,  1006,   951,   903, 30908, 32158, 29783,
         29473,  1008,   927,   923],
        [    1,  1133,  5382, 15002, 11701, 29515,  3512,  1030,  3964,  4429,
          7229,  1120, 10605,  1163,  8946,  1910,  1283, 29584,  4519, 29487,
          1283,  1093, 20162, 29499,  1207,  9940, 29494,  1770, 12022,  1065,
          1040, 11191,  7155, 29491, 17249, 26102,  1062,  8946,  1807, 29487,
          1283,  1093,  4439, 29499,  1117,  5555,  2424,  1122,  1203,  1034,
          3964,  1249,  1126,  1489,  1050, 24229, 29492, 29513,  4313, 29493,
          1639,  8717,  1065,  4429]], device='cuda:0')
[2025-05-09 11:46:04] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:46:04] [INFO] [nan 감지] labels: tensor([1, 1, 0, 1, 1, 0, 0, 1], device='cuda:0')
[2025-05-09 11:46:04] [INFO] [nan 감지] input_ids: tensor([[    1, 29473, 31078, 32109, 29473, 31250, 30461, 30050, 29473, 31163,
         30402, 31266, 29473, 31104, 31433, 29473, 32074, 30194, 29911, 29473,
         30092, 30194, 29473,  1007,   922,   903, 31362, 32644, 29473, 30092,
         30723, 32074, 30194, 31043, 29473, 30782, 31104, 29473, 32074, 30194,
         29903, 29473, 31250, 30461, 29904, 30333, 30073, 29493, 29473, 30001,
         30543, 29955, 29473, 30919,  1005,   954,   956, 30919, 30092, 29473,
         29929, 30584, 29783, 29473],
        [    1,  1619,  4100,  4598, 22276,  4243, 18323,  3801,  1070,  4589,
          1089,  3898, 29494,  1273, 11418,  3024,  1070,  1032, 18967,  2712,
          4581,  2567,  4010,  1215,  1315,  4440,  8035,  1088,  1698, 29478,
          1396,  4606,  8955, 29577, 21334,  1066,  4649,  1040, 13052,  2212,
          1029,  1698, 29478,  1396,  5461,  1072,  3493,  1089,  3898, 29494,
          1273,  5396,  1065,  8230,  2712, 12220,  1525,  1215,  2694, 29491,
          8049,  1216,  9464,  8230],
        [    1, 29473, 30735, 30183, 29473, 31093, 30245, 29473, 29903, 29783,
         30238, 30468, 30092, 29493, 29473, 29783,  1006,   930,   947, 29473,
         31539, 29473, 30482,  1006,   938,   956, 30001,  1007,   948,   904,
         29955, 29473,  1008,   920,   952, 32755, 29473, 32440, 30083, 30001,
          1007,   948,   904, 29783, 30468, 30279, 29473, 30491, 29473, 29919,
         29473, 30372, 29943, 30737, 29473, 30183, 31303, 29473, 29955, 30652,
         29955, 29473, 32755, 30083],
        [    1,  1328,  6159,  2035, 29493,  1040,  3698,  8585,  7077,  1293,
          1072,  1639, 18367,  1274,  1518, 10372,  1066,  5684,  1581,  7167,
          1070,  3296, 29493,  1163,  3716,  5269,  2846,  1066,  8585,  9742,
          2211, 15751,  1458, 11860, 19690,  1064,  1072,  6089,  1360,  2603,
          3207,  1244,  1364, 29491,  1619,  4649, 21334,  1066,  4397,  3148,
          1935,  5282,  1070, 10159,  1254, 22122, 14062,  1081,  4461,  1029,
         12729,  1093, 29476, 23751],
        [    1,  1619,  4598, 16658,  1032,  4649,  1124,  1040, 22558,  1070,
          7475,  1584,  1361, 29477,  6211,  1072,  1040,  9740,  1845,  9380,
          3678,  1420, 14623,  8985,  3655,  7429,  5150,  1065, 10146,  1066,
          3420,  1067,  2712, 12220,  1093, 17745, 29499, 14850, 29491,  3231,
          4112,  1032,  3698,  4088,  5199, 29493,  1224,  4598, 23857,  3804,
          6411,  1158,  1164,  6413,  4689,  1122,  7167,  5845, 29577, 29481,
          1566,  8150,  5851, 28067],
        [    1,  4647, 29549,  1117,  1032,  2713,  6409,  1070,  1040,  1317,
          1092,  1276,  1090,  2858, 26019,  4449, 29493,  1072,  1639, 15041,
          1079,  2159, 11954,  1163,  1102, 13344, 29518,  1427,  1518,  5662,
          1066,  1115,  1032, 17478,  1208,  1070,  3245,  1040,  1776, 29501,
          1035,  1863,  1050,  1070,  7869,  1411,  9779, 18622,  9662,  1093,
          3023, 29511, 29481, 29499,  1210,  1420,  2349,  7520,  1546,  4824,
          1489,  1366,  4849, 29491],
        [    1,  2370,  5551,  1148,  1072,  8786,  1263, 21698, 15683,  1539,
          1065, 15328,  1066, 11792,  5533, 12177,  8798,  1117,  3551,  1066,
          1115,  6662, 26249,  1065,  1392,  3460,  4649, 29491,  4771, 29493,
          1246, 23303, 29397,  2243, 12482,  1245,  7651,  7324,  2100,  1163,
          1951,  2491,  5533, 12177,  1066,  3148,  1935, 21698, 15683, 29491,
          1584,  3032,  1040, 16330,  1070, 29473, 29508, 29551,  3046, 21698,
          6055, 29493,  5970,  1066],
        [    1,  1619,  4598, 16658,  1032, 11305, 15849,  1070,  4127,  8628,
          1885,  2374,  1070,  1133,  1394,  1241,  7363,  1050,  6984,  3422,
          2892, 11830,  7363,  1149, 29473, 29508,  1093,  4308,  3151, 22243,
         29473, 29508, 29485, 29499,  1065,  1131,  6230, 29491, 10384,  1040,
          1086,  2595, 29501,  1977,  1449, 29501, 10649, 12440, 28373,  1072,
          1040,  2031,  1818,  7010, 24910,  6330, 29493, 16102, 22742,  1054,
          1040,  8754, 10313,  1070]], device='cuda:0')
[2025-05-09 11:46:05] [INFO] [nan 감지] loss: nan, logits: tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=<IndexBackward0>)
[2025-05-09 11:46:05] [INFO] [nan 감지] labels: tensor([0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0')
[2025-05-09 11:46:05] [INFO] [nan 감지] input_ids: tensor([[    1, 29473, 30183, 30543, 29473, 32441, 30334, 29903, 29473, 30372,
         30201, 30285, 29491, 29473, 30183, 30543, 29473, 32441, 30334, 29903,
         29473, 30372,  1005,   955,   951, 29473, 30050, 30737, 29473, 29904,
         30595, 32053, 29473, 29811, 31647, 29473, 31293, 30482,  1007,   930,
           900, 29955, 31043, 29473, 30992, 32376, 30665, 29473, 32441, 30334,
          1007,   927,   939, 29957, 29473, 30245, 30333, 29473, 30402, 30543,
         30985, 29473, 30652, 30083],
        [    1,  1083,  6373, 24229,  1062, 10789,  3408,  1396, 16950,  3505,
          1046,  1093,  3433, 29533, 29499,  1117,  1032, 18558,  1775, 29493,
         21156, 29493,  1072, 15640,  1508,  5680, 21226, 16950,  3505,  1056,
          8798,  6142,  1066,  3936,  1065,  1312,  7651, 11948, 29493,  1072,
          7844, 29533,  1951,  2491,  1207,  9940, 29494,  1465,  1547,  6585,
          1040,  1848, 20126,  1845,  1200,  3227,  2706,  1639, 15961,  3131,
         29491,  7844, 29533,  1207],
        [    1,  1133,  5382, 15002, 11701, 29515, 24991, 29486,  1845,  3529,
         29120,  8607,  3193,  1240,  1117,  8742,  1122,  4003,  4632, 24898,
          1513, 23714,  1986,  1122,  7651,  1163,  1168,  4380,  1170,  1247,
          1866,  2021, 21011,  7711,  1093, 29537,  4788,  1377,  4257,  4222,
          4649,  9046,  1032,  2611, 29501,  1030,  6938,  1263,  2806,  5928,
          3529, 29120,  8607,  3193,  1240, 21078,  1093, 20401, 29499,  1066,
          6198, 25592,  3529, 29120],
        [    1,  1093, 31246, 30346, 30062, 30989, 29968, 29473, 30245, 32486,
         29499, 29473, 29811, 30649, 29473, 30499, 30333, 29957, 29473, 30245,
         30004, 30333, 29473, 29552, 31607, 29473, 31850, 29916, 29473, 30894,
         30279, 31444, 29943, 30737, 29473, 30778, 30482, 30245, 30895, 29473,
         31205, 31544, 29943, 29473, 29508, 29508, 31607, 29473, 29518, 29555,
         30183, 30612, 29473, 29508, 29518, 31607, 29473, 29508, 30183, 29916,
         29473, 30549, 30233, 30665],
        [    1, 29473, 30382, 30601, 29473,  1007,   933,   899, 29473,  1007,
           923,   936, 29955, 29473, 32177, 30092, 30737, 29473, 30382, 29473,
         31055, 30004, 29911, 30073, 29943, 29473, 30538, 29955, 29904, 31478,
         29962, 31346, 29473, 30201,  1007,   940,   919, 29473, 29919, 29473,
         30731, 31359, 30279, 29491, 29473, 30382,  1006,   930,   947, 29473,
         30346, 31301, 29916, 30073, 29473, 29929, 31099, 29473, 30092, 32012,
         30004, 29943, 29473, 30062],
        [    1,  6981,  6329, 29473, 29518, 29502, 29502, 29555, 11596,  1427,
          9072,  1164, 24392, 11539,  8102,  1254,  1457,  1617,  1737,  1138,
          5925, 15790,  1532,  1040,  1675,  3243, 26798, 27392,  1065,  1032,
          5794,  1148, 12027,  3707,  1532,  1163, 11638, 29473, 29508, 29552,
         29508, 25351, 12135,  5237, 25856,  1065,  1757, 22951,  2899, 19764,
          1065,  4732, 29584, 29517, 10998, 11596, 15101,  3747, 29473, 29538,
         29493, 29542, 29552, 29551],
        [    1,  3004, 10255,  1178,  1051,  1050,  4856, 29493,  4001,  3226,
          1254,  3592,  1760,  2107,  1065,  9718,  1079,  1489,  1077,  1072,
          2094, 27488,  1192, 29516,  8270,  1079,  1489,  1233,  1676, 29493,
          1117,  4066,  1065,  3958,  2712,  4552,  7651, 29491,  1183, 19260,
          6305,  1070,  9262,  1121, 27488,  1192, 28883, 29501,  1071,  1089,
         14390,  7767,  1093, 29545, 29501,  6926, 29533, 29499,  1066,  5684,
          1040,  6340,  1070,  2328],
        [    1,  1619,  4598,  4948,  2145,  1032,  1946, 29501, 28137,  5199,
          1066,  7765,  1421, 22581, 10014, 15922,  8770,  1122, 26627,  1157,
          8614,  6825,  1072, 11912,  1081, 25272, 29491, 16925,  4632,  3420,
          1293, 29501,  6473,  2192,  1143,  8205,  2997,  5686, 29493,  1040,
          4649,  4350, 29482,  8905,  1678,  1224,  7981,  1309,  1115,  2075,
          1066,  5684,  1157,  8614, 17927,  1120,  5165,  1065,  4239,  1070,
          5008,  1068,  1257,  5653]], device='cuda:0')
================================================================================
[2025-05-09 11:54:21] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:54:21] [INFO] K-Fold 분할 시작
================================================================================
[2025-05-09 11:54:21] [INFO] 데이터 로드 중...
[2025-05-09 11:54:22] [INFO] 총 3000개 샘플 로드 완료
[2025-05-09 11:54:22] [INFO] 데이터 검증 결과:
[2025-05-09 11:54:22] [INFO] - 총 행 수: 3000
[2025-05-09 11:54:22] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:54:22] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:54:22] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:54:22] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:54:22] [INFO] KFold 비활성화 상태 - 전체 데이터 저장
================================================================================
[2025-05-09 11:54:22] [INFO] train_folds.csv 저장 완료: data/train_folds.csv
================================================================================
================================================================================
[2025-05-09 11:54:26] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:54:26] [INFO] TF-IDF + LightGBM 학습 시작
================================================================================
[2025-05-09 11:54:26] [INFO] 데이터 로드 중...
[2025-05-09 11:54:26] [INFO] 데이터 검증 결과:
[2025-05-09 11:54:26] [INFO] - 총 행 수: 3000
[2025-05-09 11:54:26] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:54:26] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:54:26] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:54:26] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:54:26] [INFO] 데이터 분할 중...
[2025-05-09 11:54:26] [INFO] 학습 샘플: 2400개, 검증 샘플: 600개
[2025-05-09 11:54:26] [INFO] TF-IDF 피처 추출 중 (ngrams=[1, 5], max_features=150000)...
[2025-05-09 11:54:33] [INFO] TF-IDF 피처 추출 완료: 150000 피처 (소요시간: 6초)
[2025-05-09 11:54:33] [INFO] LightGBM 모델 학습 중 (n_estimators=500, lr=0.05)...
[2025-05-09 11:54:40] [INFO] LightGBM 학습 완료 (소요시간: 7초)
[2025-05-09 11:54:41] [INFO] 검증 정확도: 97.50%
[2025-05-09 11:54:41] [INFO] 모델 저장 중...
================================================================================
[2025-05-09 11:54:44] [INFO] TF-IDF 및 LGBM 모델 저장 완료
================================================================================
================================================================================
[2025-05-09 11:54:44] [INFO] 전체 TF-IDF + LGBM 과정 완료 (총 소요시간: 18초)
================================================================================
================================================================================
[2025-05-09 11:54:47] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:54:48] [INFO] KenLM 3-gram 모델 구축 시작
================================================================================
================================================================================
[2025-05-09 11:54:48] [INFO] 이미 KenLM 모델이 존재합니다. 재사용합니다.
================================================================================
================================================================================
[2025-05-09 11:54:48] [INFO] 전체 KenLM 구축 과정 완료 (총 소요시간: 0초)
================================================================================
================================================================================
[2025-05-09 11:54:54] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:54:54] [INFO] Qwen3-4B QLoRA 학습 시작
================================================================================
[2025-05-09 11:54:54] [INFO] 데이터 로드 중...
[2025-05-09 11:54:54] [INFO] 데이터 검증 결과:
[2025-05-09 11:54:54] [INFO] - 총 행 수: 3000
[2025-05-09 11:54:54] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:54:54] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:54:54] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:54:54] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:54:54] [INFO] [라벨 분포] train: {0: 1050, 1: 1050} val: {0: 450, 1: 450}
[2025-05-09 11:54:54] [INFO] 데이터 분할 완료: 학습 2100개, 검증 900개
[2025-05-09 11:54:54] [INFO] Qwen3-4B 모델 및 토크나이저 로드 중...
[2025-05-09 11:56:47] [INFO] LoRA 어댑터 초기화 중...
[2025-05-09 11:56:47] [INFO] 모델 준비 완료 (소요시간: 1분 53초)
[2025-05-09 11:56:47] [INFO] 학습 디바이스: cuda
[2025-05-09 11:56:47] [INFO] 하이퍼파라미터 설정: epochs=3, batch_size=16, lr=1.0e-05
[2025-05-09 11:56:47] [INFO] 데이터로더 초기화 중...
================================================================================
[2025-05-09 11:56:47] [INFO] 학습 시작
================================================================================
================================================================================
[2025-05-09 11:56:47] [INFO] Qwen3-4B QLoRA 학습 시작 (총 3 단계)
================================================================================
[2025-05-09 11:56:48] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 0.9375,  0.4766, -0.6953,  ..., -4.9062, -4.9062, -4.9062],
         [ 0.7305, -0.2451, -3.8438,  ..., -2.1562, -2.1562, -2.1562],
         [ 0.4180,  1.8438, -0.0549,  ..., -3.9375, -3.9375, -3.9375]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.7188,  5.2188,  2.4531,  ..., -4.6562, -4.6562, -4.6562],
         [-3.2500, -4.7812, -2.8750,  ..., -0.8711, -0.8711, -0.8711],
         [ 6.0625,  3.7812,  6.8438,  ..., -3.9531, -3.9531, -3.9531]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 8.3125,  7.7812,  6.7812,  ...,  2.1406,  2.1406,  2.1406],
         [ 5.3438,  6.2500,  3.7812,  ...,  0.0884,  0.0884,  0.0884],
         [ 8.0000,  6.4688,  4.9688,  ..., -4.6250, -4.6250, -4.6250]],

        ...,

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.8125,  5.7188,  4.4375,  ..., -1.2891, -1.2891, -1.2891],
         [ 3.7500,  6.3125,  2.9062,  ..., -2.1875, -2.1875, -2.1875],
         [11.0000, 10.1875,  8.0000,  ...,  3.1562,  3.1562,  3.1562]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 7.6875,  9.0625,  6.2812,  ...,  0.8633,  0.8633,  0.8633],
         [ 4.4062,  4.3438,  1.8750,  ..., -1.1250, -1.1250, -1.1250],
         [ 6.0938,  4.8750,  3.4219,  ..., -0.7344, -0.7344, -0.7344]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 7.1562,  5.1250,  4.8125,  ..., -2.3750, -2.3750, -2.3750],
         [-1.1016, -4.0312, -0.7656,  ..., -3.0938, -3.0938, -3.0938],
         [ 5.5938,  3.7500,  6.2500,  ..., -2.3750, -2.3750, -2.3750]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:48] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,  19969,  95170,  29281],
        [ 12802,  53435,  40853,  ...,  25715,    220,     18],
        [ 12802,  53435,  40853,  ...,   3476,    315,  16254],
        ...,
        [ 12802,  53435,  40853,  ...,  63970,  22077,  29640],
        [ 12802,  53435,  40853,  ...,   1052,    525,  15279],
        [ 12802,  53435,  40853,  ..., 128514,    220,     17]],
       device='cuda:0')
[2025-05-09 11:56:49] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 3.4062,  0.4062,  1.0938,  ...,  0.3164,  0.3164,  0.3164],
         [ 8.9375,  7.7188,  6.0625,  ...,  0.8516,  0.8516,  0.8516],
         [ 9.3750, 10.1875,  9.5000,  ..., -0.3145, -0.3125, -0.3145]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [10.3750, 10.0625,  6.5625,  ...,  2.0469,  2.0469,  2.0469],
         [ 3.6250,  2.3125,  1.5156,  ..., -0.8828, -0.8828, -0.8828],
         [ 2.3594,  4.8438,  0.9141,  ..., -1.5312, -1.5312, -1.5312]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.7188,  6.8125,  2.0156,  ..., -0.7383, -0.7344, -0.7383],
         [ 7.2500,  5.6875,  3.7812,  ...,  0.0306,  0.0308,  0.0308],
         [11.2500, 11.0625,  7.3750,  ...,  2.4844,  2.4844,  2.4844]],

        ...,

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 5.8438,  6.3750,  2.7969,  ...,  0.4590,  0.4590,  0.4590],
         [13.3125,  6.0938,  6.5000,  ..., -3.4219, -3.4219, -3.4219],
         [ 8.9375, 10.1250,  7.4062,  ..., -4.5938, -4.5938, -4.5938]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [13.4375, 11.0000,  9.9375,  ..., -4.0938, -4.0938, -4.0938],
         [10.7500, 10.5000,  8.3125,  ...,  0.4414,  0.4414,  0.4414],
         [12.6875, 11.6875,  6.3438,  ...,  0.6055,  0.6055,  0.6055]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 6.4688,  6.7188,  3.7656,  ..., -2.1406, -2.1406, -2.1406],
         [ 7.1250,  6.4688,  3.8594,  ..., -2.5781, -2.5781, -2.5781],
         [ 8.8750,  8.7500,  6.9688,  ...,  1.8125,  1.8125,  1.8125]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:49] [INFO] [nan 감지] input_ids: tensor([[12802, 53435, 40853,  ...,    13,  2885,   297],
        [12802, 53435, 40853,  ...,  5203,    25,   279],
        [12802, 53435, 40853,  ...,   389, 30106,  2188],
        ...,
        [12802, 53435, 40853,  ...,   323,  6105,   404],
        [12802, 53435, 40853,  ..., 15151, 16827,    32],
        [12802, 53435, 40853,  ...,  1205, 35781,  4961]], device='cuda:0')
[2025-05-09 11:56:50] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 5.1250,  5.1562,  2.7656,  ..., -3.1562, -3.1562, -3.1562],
         [ 5.1250,  7.1250,  4.2812,  ..., -1.6641, -1.6641, -1.6641],
         [ 4.4375,  6.9062,  5.2500,  ..., -2.0938, -2.0938, -2.0938]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 9.7500, 10.4375,  8.2500,  ..., -2.7344, -2.7344, -2.7344],
         [ 8.1250,  8.4375,  6.2188,  ..., -4.3750, -4.3750, -4.3750],
         [ 7.7812,  8.3750,  3.9219,  ...,  0.5000,  0.5000,  0.5000]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.1250,  5.7812,  3.2344,  ..., -4.7188, -4.7188, -4.7188],
         [ 4.4062,  6.3438,  2.4531,  ..., -3.2500, -3.2500, -3.2500],
         [-0.3105,  1.2734, -0.4160,  ..., -5.5312, -5.5312, -5.5312]],

        ...,

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 7.7188,  8.8750,  4.2812,  ..., -0.4609, -0.4609, -0.4609],
         [ 9.8750,  9.1250,  5.9688,  ...,  0.2119,  0.2119,  0.2119],
         [ 7.8125,  7.9062,  5.4375,  ...,  0.9414,  0.9414,  0.9414]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [10.8125, 11.4375,  6.3750,  ...,  1.3750,  1.3750,  1.3750],
         [ 4.1250,  2.1094,  1.1328,  ...,  1.2266,  1.2266,  1.2266],
         [ 7.4375,  8.8750,  6.2812,  ...,  1.4688,  1.4688,  1.4688]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 8.3750,  6.6562,  6.1250,  ..., -0.0532, -0.0530, -0.0532],
         [11.5625, 10.5000, 10.2500,  ..., -2.9219, -2.9219, -2.9219],
         [ 7.1875,  6.4375,  4.5312,  ...,  1.1875,  1.1875,  1.1875]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:50] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,    311,  48486,  20959],
        [ 12802,  53435,  40853,  ...,     72,   8813,   4671],
        [ 12802,  53435,  40853,  ...,  63256, 144495,  64577],
        ...,
        [ 12802,  53435,  40853,  ...,  49023,  29809,   4940],
        [ 12802,  53435,  40853,  ...,    550,     11,   1741],
        [ 12802,  53435,  40853,  ...,   3930,    258,     38]],
       device='cuda:0')
[2025-05-09 11:56:50] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [-1.8906e+00, -1.1797e+00, -2.8594e+00,  ..., -6.6875e+00,
          -6.6875e+00, -6.6875e+00],
         [ 3.1719e+00,  3.6406e+00,  2.8564e-02,  ..., -4.3125e+00,
          -4.3125e+00, -4.3125e+00],
         [-9.1016e-01,  8.2812e-01, -1.9297e+00,  ..., -4.8750e+00,
          -4.8750e+00, -4.8750e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 2.0156e+00,  5.3787e-04, -4.3213e-02,  ..., -5.7812e+00,
          -5.7812e+00, -5.7812e+00],
         [ 4.2383e-01, -4.7266e-01, -8.6719e-01,  ..., -5.9688e+00,
          -5.9688e+00, -5.9688e+00],
         [ 4.5938e+00,  3.0781e+00,  2.4531e+00,  ..., -4.0312e+00,
          -4.0312e+00, -4.0312e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 3.7188e+00,  1.7188e+00,  3.2344e+00,  ...,  1.6484e+00,
           1.6484e+00,  1.6484e+00],
         [ 9.7500e+00,  9.0625e+00,  8.0625e+00,  ...,  1.0156e+00,
           1.0156e+00,  1.0156e+00],
         [ 8.8125e+00,  8.1250e+00,  7.6875e+00,  ...,  3.9673e-03,
           4.0588e-03,  3.9978e-03]],

        ...,

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [-1.9766e+00,  2.2188e+00,  1.5859e+00,  ..., -6.4375e+00,
          -6.4375e+00, -6.4375e+00],
         [ 5.5312e+00,  7.0625e+00,  9.5625e+00,  ..., -5.0625e+00,
          -5.0625e+00, -5.0625e+00],
         [ 8.0078e-01,  7.4688e+00,  3.2344e+00,  ..., -6.5625e+00,
          -6.5625e+00, -6.5625e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 5.8750e+00,  5.7812e+00,  3.0469e+00,  ..., -1.5781e+00,
          -1.5781e+00, -1.5781e+00],
         [ 5.6250e+00,  7.8125e+00,  4.6562e+00,  ..., -6.8359e-01,
          -6.8359e-01, -6.8359e-01],
         [ 4.2500e+00,  5.0625e+00,  3.5625e+00,  ...,  3.2812e-01,
           3.2812e-01,  3.2812e-01]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [-4.0039e-01, -3.7344e+00, -2.1719e+00,  ...,  1.6641e+00,
           1.6641e+00,  1.6641e+00],
         [ 8.8750e+00,  8.5625e+00,  8.1875e+00,  ...,  1.1016e+00,
           1.1016e+00,  1.1016e+00],
         [ 9.6250e+00,  9.5000e+00,  7.9375e+00,  ...,  7.0703e-01,
           7.0703e-01,  7.0703e-01]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:50] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,  23084, 126674,  18411],
        [ 12802,  53435,  40853,  ..., 124546,  56419, 138279],
        [ 12802,  53435,  40853,  ...,    220,     19,     15],
        ...,
        [ 12802,  53435,  40853,  ...,    320,   3902,   6222],
        [ 12802,  53435,  40853,  ...,  28461,  46710,    304],
        [ 12802,  53435,  40853,  ...,    220,     17,     15]],
       device='cuda:0')
[2025-05-09 11:56:51] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 6.1562e+00,  4.8750e+00,  2.4062e+00,  ..., -2.2812e+00,
          -2.2812e+00, -2.2812e+00],
         [ 1.1812e+01,  9.8750e+00,  6.0625e+00,  ...,  1.5859e+00,
           1.5859e+00,  1.5859e+00],
         [ 7.8438e+00,  8.8750e+00,  4.7500e+00,  ..., -9.2163e-03,
          -9.0942e-03, -9.1553e-03]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [-4.2383e-01, -1.3984e+00, -1.6406e+00,  ..., -2.0625e+00,
          -2.0625e+00, -2.0625e+00],
         [ 2.2344e+00,  2.0000e+00,  1.2734e+00,  ..., -3.9219e+00,
          -3.9219e+00, -3.9219e+00],
         [ 5.0000e+00,  1.4609e+00,  3.0781e+00,  ..., -3.2969e+00,
          -3.2969e+00, -3.2969e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 5.8438e+00,  5.5625e+00,  1.8125e+00,  ..., -8.2812e-01,
          -8.2812e-01, -8.2812e-01],
         [ 5.0000e+00,  6.8438e+00,  1.8594e+00,  ..., -4.8750e+00,
          -4.8750e+00, -4.8750e+00],
         [ 1.0562e+01,  1.0688e+01,  9.0625e+00,  ...,  1.8828e+00,
           1.8828e+00,  1.8828e+00]],

        ...,

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 8.7500e+00,  7.4375e+00,  6.2500e+00,  ...,  5.8203e-01,
           5.8203e-01,  5.8203e-01],
         [ 3.5000e+00,  2.8750e+00, -3.9648e-01,  ..., -2.0156e+00,
          -2.0156e+00, -2.0156e+00],
         [ 3.4688e+00,  4.5000e+00,  2.0625e+00,  ..., -6.4062e-01,
          -6.4062e-01, -6.4062e-01]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 5.3125e+00,  6.7188e+00,  3.8906e+00,  ..., -3.6094e+00,
          -3.6094e+00, -3.6094e+00],
         [ 2.6250e+00,  6.6250e+00,  1.8984e+00,  ..., -4.5000e+00,
          -4.5000e+00, -4.5000e+00],
         [ 9.8125e+00,  8.7500e+00,  4.5625e+00,  ..., -5.3438e+00,
          -5.3438e+00, -5.3438e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 7.6562e+00,  8.5000e+00,  2.8594e+00,  ...,  8.9062e-01,
           8.9062e-01,  8.9062e-01],
         [ 5.4062e+00,  7.1875e+00,  3.8086e-01,  ..., -2.2168e-01,
          -2.2168e-01, -2.2168e-01],
         [ 6.0312e+00,  5.6250e+00, -1.0078e+00,  ..., -1.4219e+00,
          -1.4219e+00, -1.4219e+00]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:51] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,  34593,  10535,    481],
        [ 12802,  53435,  40853,  ...,  33509, 126896,  34395],
        [ 12802,  53435,  40853,  ...,    315,    472,  17048],
        ...,
        [ 12802,  53435,  40853,  ...,  28984,    323,  16400],
        [ 12802,  53435,  40853,  ...,   1356,     79,    360],
        [ 12802,  53435,  40853,  ...,  10735,    389,    279]],
       device='cuda:0')
[2025-05-09 11:56:52] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [16.3750, 11.8125,  8.8125,  ..., -1.0078, -1.0078, -1.0078],
         [13.1250, 11.8750,  8.8750,  ..., -2.3438, -2.3438, -2.3438],
         [11.6875,  9.8125,  5.8750,  ...,  0.1191,  0.1191,  0.1191]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.2188,  4.9688,  1.4375,  ..., -4.5000, -4.5000, -4.5000],
         [ 8.1250,  5.6875,  5.8750,  ..., -0.7812, -0.7812, -0.7812],
         [12.2500, 11.2500,  8.0625,  ...,  2.9375,  2.9375,  2.9375]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [10.5000, 11.3750,  6.0625,  ...,  1.7031,  1.7031,  1.7031],
         [ 5.7500,  6.3750,  1.3750,  ...,  0.2754,  0.2754,  0.2754],
         [ 2.6875,  0.7188,  1.4531,  ...,  4.0938,  4.0938,  4.0938]],

        ...,

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 2.6719,  0.7383, -0.7734,  ..., -2.5781, -2.5781, -2.5781],
         [ 5.5938,  2.7188,  0.4199,  ..., -1.4688, -1.4688, -1.4688],
         [ 6.0625,  6.8438,  5.2188,  ..., -1.9375, -1.9375, -1.9375]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 5.0312,  2.8125,  0.8594,  ..., -2.6406, -2.6406, -2.6406],
         [ 4.3438,  2.4375,  0.3027,  ..., -4.1250, -4.1250, -4.1250],
         [ 3.5469, -1.4219,  1.8281,  ..., -5.0625, -5.0625, -5.0625]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 7.7188,  8.3750,  4.6562,  ...,  1.0938,  1.0938,  1.0938],
         [ 7.2188,  7.1562,  3.2188,  ...,  0.8555,  0.8555,  0.8555],
         [ 5.6250,  7.4375,  1.8594,  ..., -0.0591, -0.0591, -0.0591]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:52] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,  33427,     72,   1925],
        [ 12802,  53435,  40853,  ...,  15801,  62312,  95360],
        [ 12802,  53435,  40853,  ...,  15344,  45238,    220],
        ...,
        [ 12802,  53435,  40853,  ...,     13,   1096,   3395],
        [ 12802,  53435,  40853,  ..., 138268, 132537,    508],
        [ 12802,  53435,  40853,  ...,   1473,    429,    419]],
       device='cuda:0')
[2025-05-09 11:56:53] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.7812, -1.3125, -0.2197,  ..., -0.2100, -0.2100, -0.2100],
         [ 4.7812,  4.5312,  2.3281,  ..., -1.1797, -1.1797, -1.1797],
         [ 8.9375,  1.8828,  5.6562,  ..., -6.4375, -6.4375, -6.4375]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.5938,  5.4688,  1.3281,  ..., -2.2656, -2.2656, -2.2656],
         [ 4.7188,  5.6875,  6.8438,  ..., -5.9375, -5.9375, -5.9375],
         [ 6.3750, 11.1875,  9.3750,  ...,  1.4922,  1.4922,  1.4922]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.9688,  5.1875,  6.2188,  ..., -2.5156, -2.5156, -2.5156],
         [ 7.6250,  7.4062,  7.0312,  ..., -4.9375, -4.9375, -4.9375],
         [ 5.5938,  4.9688,  2.9531,  ..., -4.6562, -4.6562, -4.6562]],

        ...,

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 3.1719,  0.9336, -0.4688,  ..., -2.7969, -2.7969, -2.7969],
         [11.5625,  9.2500,  5.7812,  ...,  3.5625,  3.5625,  3.5625],
         [ 0.6016, -0.3477, -2.6406,  ..., -2.0781, -2.0781, -2.0781]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 2.7344,  3.2031,  3.5156,  ..., -5.8125, -5.8125, -5.8125],
         [ 6.7500,  2.1094, -1.1016,  ..., -7.9375, -7.9375, -7.9375],
         [ 0.2988,  0.9492, -0.8398,  ..., -5.4688, -5.4688, -5.4688]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 6.7812,  6.3125,  5.8750,  ..., -1.1797, -1.1797, -1.1797],
         [ 5.5625,  5.3750,  4.7812,  ..., -4.4375, -4.4375, -4.4375],
         [ 8.8125,  7.0938,  5.3750,  ..., -1.4766, -1.4766, -1.4766]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:53] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,      8,    553,   1815],
        [ 12802,  53435,  40853,  ...,    448,    730,   1500],
        [ 12802,  53435,  40853,  ...,     17,     21,  32077],
        ...,
        [ 12802,  53435,  40853,  ...,     13,   4354,     11],
        [ 12802,  53435,  40853,  ...,     15, 127475,  31328],
        [ 12802,  53435,  40853,  ...,  41400,  13621,  48968]],
       device='cuda:0')
[2025-05-09 11:56:54] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 5.3438,  4.3438,  1.6250,  ..., -0.3359, -0.3359, -0.3359],
         [ 7.0625,  6.5312,  2.7031,  ..., -0.7539, -0.7539, -0.7539],
         [ 6.2500,  5.2500,  3.1250,  ..., -1.6953, -1.6953, -1.6953]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 3.2188,  0.6445,  0.7930,  ..., -1.2188, -1.2188, -1.2188],
         [ 6.0000,  1.0469,  2.7344,  ..., -3.5312, -3.5312, -3.5312],
         [ 9.3125, 10.7500,  8.2500,  ...,  2.3750,  2.3750,  2.3750]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 6.5625,  6.2812,  3.0625,  ...,  0.1260,  0.1260,  0.1260],
         [ 5.9688,  6.7500,  1.8828,  ..., -1.5078, -1.5078, -1.5078],
         [ 3.7031,  5.0312,  0.1377,  ..., -1.6406, -1.6406, -1.6406]],

        ...,

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [-3.6562, -2.3438, -0.7539,  ..., -2.4219, -2.4219, -2.4219],
         [ 6.3438,  7.3750,  6.5938,  ..., -3.2188, -3.2188, -3.2188],
         [ 3.9375,  4.7812,  4.5938,  ..., -5.5625, -5.5625, -5.5625]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 3.9219,  5.5938, -0.3066,  ..., -2.3594, -2.3594, -2.3594],
         [ 9.3125,  9.6250,  5.7500,  ...,  1.7188,  1.7188,  1.7188],
         [ 5.2500,  7.0625,  1.2188,  ..., -1.6484, -1.6484, -1.6484]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 9.0000,  5.5000,  4.3125,  ..., -4.2500, -4.2500, -4.2500],
         [ 8.1875,  5.0938,  6.2812,  ..., -2.7656, -2.7656, -2.7656],
         [ 9.0000,  6.4062,  6.2500,  ..., -2.0000, -2.0000, -2.0000]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:54] [INFO] [nan 감지] input_ids: tensor([[12802, 53435, 40853,  ...,  1030, 11941,  4722],
        [12802, 53435, 40853,  ...,    13,   508, 49863],
        [12802, 53435, 40853,  ...,   362, 12126,   315],
        ...,
        [12802, 53435, 40853,  ...,   220,    16,    15],
        [12802, 53435, 40853,  ...,   311, 18873,   448],
        [12802, 53435, 40853,  ..., 65632, 46241,  2524]], device='cuda:0')
[2025-05-09 11:56:54] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 6.5938,  8.4375,  3.0469,  ..., -0.0991, -0.0986, -0.0991],
         [ 6.7500,  8.6250,  1.9766,  ..., -1.4453, -1.4453, -1.4453],
         [ 6.0000,  7.0625,  2.3125,  ..., -1.8906, -1.8906, -1.8906]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [11.6875, 11.2500,  9.7500,  ..., -0.7188, -0.7188, -0.7188],
         [ 7.9062,  6.0312,  4.8750,  ...,  0.2148,  0.2148,  0.2148],
         [11.6250,  6.1562,  8.2500,  ...,  0.7773,  0.7773,  0.7773]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 1.7422,  4.3125,  3.7656,  ..., -6.4062, -6.4062, -6.4062],
         [ 1.2578,  2.1562,  1.4844,  ..., -7.6250, -7.6250, -7.6250],
         [ 0.5391,  0.5000, -1.9688,  ...,  1.1875,  1.1875,  1.1875]],

        ...,

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 6.6562,  5.7500,  3.5156,  ..., -0.4434, -0.4434, -0.4434],
         [ 9.3125, 10.4375,  7.1562,  ..., -1.0625, -1.0625, -1.0625],
         [ 5.5938,  4.9062,  1.7344,  ..., -1.3203, -1.3203, -1.3203]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 5.0938,  2.4531,  2.3750,  ..., -4.6250, -4.6250, -4.6250],
         [ 3.9531,  3.9844, -0.1719,  ..., -3.2344, -3.2344, -3.2344],
         [ 2.8125,  1.3359, -1.0547,  ..., -4.3750, -4.3750, -4.3750]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 1.6094,  2.9219,  1.9688,  ..., -2.5000, -2.5000, -2.5000],
         [ 3.6719,  2.1250,  1.7188,  ..., -3.3594, -3.3594, -3.3594],
         [11.8125, 10.6250,  7.7500,  ..., -2.3750, -2.3750, -2.3750]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:54] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,  20346,    264,   4285],
        [ 12802,  53435,  40853,  ...,    458,     35,   1417],
        [ 12802,  53435,  40853,  ..., 125118, 139274,   8620],
        ...,
        [ 12802,  53435,  40853,  ...,  42710,    360,   6953],
        [ 12802,  53435,  40853,  ...,    226, 132513, 142701],
        [ 12802,  53435,  40853,  ...,  97143,  34395,  90686]],
       device='cuda:0')
[2025-05-09 11:56:55] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.8125,  6.3750,  5.0312,  ..., -3.3125, -3.3125, -3.3125],
         [12.1875, 10.7500,  5.9375,  ...,  1.4062,  1.4062,  1.4062],
         [ 7.0312,  6.5312,  4.1250,  ...,  0.3535,  0.3535,  0.3535]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.0938,  3.0469,  0.0703,  ..., -3.3281, -3.3281, -3.3281],
         [ 4.3125,  3.5312,  1.2734,  ..., -6.4375, -6.4375, -6.4375],
         [ 5.2188,  3.9688,  2.3281,  ..., -4.0000, -4.0000, -4.0000]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 0.4648, -1.5703, -0.6211,  ..., -3.3438, -3.3438, -3.3438],
         [13.3750,  9.2500,  6.3125,  ..., -0.5156, -0.5156, -0.5156],
         [ 1.5938,  0.7617, -0.2412,  ..., -3.9844, -3.9844, -3.9844]],

        ...,

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 6.5938,  6.9375,  2.0312,  ..., -7.8125, -7.8125, -7.8125],
         [ 9.6250, 11.3125,  5.7812,  ...,  0.7188,  0.7188,  0.7188],
         [ 2.0469,  3.0156,  3.4062,  ..., -7.8438, -7.8438, -7.8438]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 8.6250,  3.6406,  4.7500,  ...,  0.9766,  0.9766,  0.9766],
         [11.0625, 11.0000,  8.5000,  ..., -0.6445, -0.6445, -0.6484],
         [ 7.9062,  7.6562,  5.2500,  ...,  0.5391,  0.5391,  0.5391]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 5.0000,  2.2656,  3.7969,  ..., -6.0000, -6.0000, -6.0000],
         [ 1.9688,  2.2969, -0.0187,  ..., -6.4375, -6.4375, -6.4375],
         [ 4.2188,  3.3906,  1.3281,  ..., -5.2500, -5.2500, -5.2500]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:55] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,  37101,  47506,    748],
        [ 12802,  53435,  40853,  ...,  46652,   4212,  36195],
        [ 12802,  53435,  40853,  ..., 144141,  31079,     13],
        ...,
        [ 12802,  53435,  40853,  ...,  64535,   4342,    320],
        [ 12802,  53435,  40853,  ...,     60,   5908,     49],
        [ 12802,  53435,  40853,  ..., 129423,  55054,  54321]],
       device='cuda:0')
[2025-05-09 11:56:56] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 6.7812,  8.1875,  4.8125,  ...,  0.4785,  0.4785,  0.4785],
         [ 5.5312,  7.0312,  4.0938,  ...,  1.0078,  1.0078,  1.0078],
         [ 4.4375,  6.6250,  1.9922,  ..., -1.1484, -1.1484, -1.1484]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [11.6250, 10.1875,  8.5000,  ..., -2.1406, -2.1406, -2.1406],
         [10.1875, 10.9375,  8.2500,  ...,  1.5234,  1.5234,  1.5234],
         [ 6.3438,  3.1875,  2.9062,  ..., -0.2637, -0.2637, -0.2637]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 8.6875,  7.8125,  5.5312,  ...,  0.5391,  0.5391,  0.5391],
         [ 5.8438,  5.6562,  3.2500,  ..., -0.8789, -0.8789, -0.8789],
         [ 6.2812,  6.4062,  1.7344,  ..., -1.4531, -1.4531, -1.4531]],

        ...,

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.2188,  2.9375, -0.3750,  ..., -5.5000, -5.5000, -5.5000],
         [-1.2422,  2.2188, -0.6797,  ..., -5.2812, -5.2812, -5.2812],
         [ 3.7188,  2.7344,  0.4688,  ..., -5.5938, -5.5938, -5.5938]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.4375,  3.3906,  0.8789,  ..., -0.5039, -0.5039, -0.5039],
         [ 6.3750,  4.0938,  3.5781,  ..., -2.0000, -2.0000, -2.0000],
         [11.3125, 10.8750,  8.1875,  ...,  3.1094,  3.1094,  3.1094]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 6.7500,  9.0000,  5.3125,  ...,  0.2754,  0.2754,  0.2754],
         [ 4.2188,  6.4375,  3.7188,  ..., -0.5469, -0.5469, -0.5469],
         [ 5.4375,  6.6562,  2.6562,  ..., -2.0469, -2.0469, -2.0469]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:56] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,   6240,    438,  45238],
        [ 12802,  53435,  40853,  ...,  40092,     74,     60],
        [ 12802,  53435,  40853,  ...,  87786,    374,    264],
        ...,
        [ 12802,  53435,  40853,  ...,  37087,  73523, 129027],
        [ 12802,  53435,  40853,  ...,    279,   3042,    975],
        [ 12802,  53435,  40853,  ...,  45750,    304,    264]],
       device='cuda:0')
[2025-05-09 11:56:57] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 2.3594,  9.1250,  0.9453,  ..., -1.0859, -1.0859, -1.0859],
         [ 5.0625,  9.2500,  3.4219,  ..., -1.2500, -1.2500, -1.2500],
         [ 9.0000, 10.6875,  6.0938,  ..., -1.8438, -1.8438, -1.8438]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 5.2812,  6.8125,  0.4336,  ..., -0.8906, -0.8906, -0.8906],
         [ 1.7500,  5.0312,  5.0938,  ..., -3.4219, -3.4219, -3.4219],
         [ 3.1562,  2.6719,  2.5156,  ..., -1.2266, -1.2266, -1.2266]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 8.6250,  4.9688,  2.3125,  ..., -4.0625, -4.0625, -4.0625],
         [-1.7891,  0.0903, -1.7812,  ...,  0.7617,  0.7617,  0.7617],
         [ 3.2344,  0.1040, -0.3613,  ..., -2.6875, -2.6875, -2.6875]],

        ...,

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 5.9688,  5.6250,  2.0781,  ..., -0.4375, -0.4375, -0.4375],
         [ 5.2188,  5.9062,  1.7109,  ...,  0.5508,  0.5508,  0.5508],
         [ 5.9062,  5.5625,  2.8438,  ...,  0.4844,  0.4844,  0.4844]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 4.3438,  2.0625,  2.5938,  ..., -7.4375, -7.4375, -7.4375],
         [ 3.9375,  1.1250,  0.5352,  ..., -5.9688, -5.9688, -5.9688],
         [-0.0427, -2.5625, -0.8359,  ..., -1.3359, -1.3359, -1.3359]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 6.8750,  3.9688,  5.0312,  ..., -5.9375, -5.9375, -5.9375],
         [ 2.7969,  1.7344, -2.1094,  ..., -3.5625, -3.5625, -3.5625],
         [ 0.5781,  3.5000, -0.2871,  ..., -2.5469, -2.5469, -2.5469]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:57] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,   8606,   8453,   5562],
        [ 12802,  53435,  40853,  ...,    369,    796,  97543],
        [ 12802,  53435,  40853,  ..., 131137,  85413,    222],
        ...,
        [ 12802,  53435,  40853,  ...,  37759,    389,    264],
        [ 12802,  53435,  40853,  ..., 130472, 130427,    220],
        [ 12802,  53435,  40853,  ...,  20401,  18411,  53900]],
       device='cuda:0')
[2025-05-09 11:56:58] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 7.5000e+00,  7.6875e+00,  5.8125e+00,  ...,  2.4609e-01,
           2.4609e-01,  2.4609e-01],
         [ 9.2500e+00,  7.9375e+00,  6.3750e+00,  ...,  8.2422e-01,
           8.2422e-01,  8.2422e-01],
         [ 5.5625e+00,  6.6250e+00,  3.1406e+00,  ...,  3.3594e-01,
           3.3594e-01,  3.3594e-01]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [-7.3438e+00, -5.6875e+00, -5.0938e+00,  ..., -2.5781e+00,
          -2.5781e+00, -2.5781e+00],
         [ 3.8281e+00,  3.1875e+00,  4.5312e+00,  ..., -4.7500e+00,
          -4.7500e+00, -4.7500e+00],
         [ 4.1250e+00,  1.4375e+00, -1.1902e-02,  ..., -7.5312e+00,
          -7.5312e+00, -7.5312e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 9.0625e+00,  6.5000e+00,  3.5781e+00,  ..., -4.2480e-02,
          -4.2480e-02, -4.2480e-02],
         [ 1.2750e+01,  1.0250e+01,  6.9062e+00,  ...,  1.8594e+00,
           1.8594e+00,  1.8594e+00],
         [ 6.9375e+00,  6.3750e+00,  2.8906e+00,  ...,  5.7812e-01,
           5.7812e-01,  5.7812e-01]],

        ...,

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [-5.8984e-01, -1.7109e+00,  2.9844e+00,  ..., -5.8750e+00,
          -5.8750e+00, -5.8750e+00],
         [ 1.5391e+00,  2.2969e+00,  4.7188e+00,  ..., -6.3438e+00,
          -6.3438e+00, -6.3438e+00],
         [ 4.9375e+00,  3.9375e+00,  9.7656e-01,  ..., -7.9375e+00,
          -7.9375e+00, -7.9375e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 4.1250e+00, -6.2109e-01,  3.4531e+00,  ..., -4.9375e+00,
          -4.9375e+00, -4.9375e+00],
         [ 3.3750e+00,  4.0312e+00,  4.1250e+00,  ..., -5.9062e+00,
          -5.9062e+00, -5.9062e+00],
         [ 3.0000e+00,  4.7500e+00,  3.6719e-01,  ..., -6.7188e+00,
          -6.7188e+00, -6.7188e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 4.3438e+00,  6.2188e+00,  2.2344e+00,  ...,  7.5391e-01,
           7.5391e-01,  7.5391e-01],
         [ 6.0000e+00,  6.0625e+00,  2.0469e+00,  ..., -1.3184e-01,
          -1.3184e-01, -1.3184e-01],
         [ 2.3750e+00,  1.2344e+00, -5.9814e-02,  ..., -5.9082e-02,
          -5.9082e-02, -5.9082e-02]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:58] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,    304,     51,  59537],
        [ 12802,  53435,  40853,  ...,    220,     20,  63089],
        [ 12802,  53435,  40853,  ...,  11705,   8315,    702],
        ...,
        [ 12802,  53435,  40853,  ...,    364,  37087,  17380],
        [ 12802,  53435,  40853,  ...,     60, 142337,  18411],
        [ 12802,  53435,  40853,  ...,   1752,    419,     11]],
       device='cuda:0')
[2025-05-09 11:56:58] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 3.2031e+00,  3.3750e+00,  9.8828e-01,  ..., -1.1719e+00,
          -1.1719e+00, -1.1719e+00],
         [ 8.3750e+00,  7.6562e+00,  5.3125e+00,  ...,  2.4414e-01,
           2.4414e-01,  2.4414e-01],
         [ 7.4688e+00,  7.7812e+00,  7.5625e+00,  ..., -7.5781e-01,
          -7.5781e-01, -7.5781e-01]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 1.2109e+00,  1.4258e-01,  1.3672e-01,  ..., -5.0938e+00,
          -5.0938e+00, -5.0938e+00],
         [ 5.4688e-01,  6.2500e-01, -1.7969e-01,  ..., -5.5000e+00,
          -5.5000e+00, -5.5000e+00],
         [-4.3750e+00, -1.0078e+00, -3.6875e+00,  ..., -3.3789e-01,
          -3.3789e-01, -3.3789e-01]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 1.9609e+00,  3.8281e+00,  1.8750e+00,  ..., -4.6875e+00,
          -4.6875e+00, -4.6875e+00],
         [-2.4844e+00, -6.0547e-01, -1.2969e+00,  ..., -5.0312e+00,
          -5.0312e+00, -5.0312e+00],
         [-1.7700e-03,  1.9844e+00, -5.8203e-01,  ..., -6.0000e+00,
          -6.0000e+00, -6.0000e+00]],

        ...,

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 3.4219e+00,  2.9531e+00,  1.0469e+00,  ..., -5.2188e+00,
          -5.2188e+00, -5.2188e+00],
         [-1.2158e-01, -7.8516e-01, -9.8047e-01,  ..., -4.8750e+00,
          -4.8750e+00, -4.8750e+00],
         [ 1.0156e+00,  2.6406e+00,  3.1445e-01,  ..., -1.1562e+00,
          -1.1562e+00, -1.1562e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [-7.6172e-01,  5.5078e-01,  9.6484e-01,  ..., -6.0312e+00,
          -6.0312e+00, -6.0312e+00],
         [ 5.4688e-01, -1.5469e+00,  5.2734e-01,  ..., -6.6875e+00,
          -6.6875e+00, -6.6875e+00],
         [ 6.9688e+00,  4.9688e+00,  1.8516e+00,  ..., -5.0000e+00,
          -5.0000e+00, -5.0000e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 5.0625e+00,  2.9844e+00,  1.3438e+00,  ..., -3.2422e-01,
          -3.2422e-01, -3.2422e-01],
         [ 4.8438e+00,  3.5312e+00,  2.6094e+00,  ..., -8.5547e-01,
          -8.5547e-01, -8.5547e-01],
         [ 9.0000e+00,  9.7500e+00,  6.6875e+00,  ...,  1.6953e+00,
           1.6953e+00,  1.6953e+00]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:58] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,    369,  28456,   8720],
        [ 12802,  53435,  40853,  ...,  19391, 140084, 129419],
        [ 12802,  53435,  40853,  ..., 136115,     11, 139465],
        ...,
        [ 12802,  53435,  40853,  ...,  80901,     11,  64805],
        [ 12802,  53435,  40853,  ..., 128605, 136361, 138520],
        [ 12802,  53435,  40853,  ...,   1012,  14576,  53752]],
       device='cuda:0')
[2025-05-09 11:56:59] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 3.3906e+00,  3.0781e+00, -4.3359e-01,  ..., -4.9688e+00,
          -4.9688e+00, -4.9688e+00],
         [ 3.2188e+00,  3.6406e+00,  3.5742e-01,  ..., -4.8750e+00,
          -4.8750e+00, -4.8750e+00],
         [ 3.5000e+00, -1.0547e+00, -6.9922e-01,  ..., -5.0625e+00,
          -5.0625e+00, -5.0625e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 7.5312e+00, -1.2598e-01,  3.9844e+00,  ..., -2.1250e+00,
          -2.1250e+00, -2.1250e+00],
         [ 6.0312e+00, -1.6211e-01,  2.7500e+00,  ..., -5.0938e+00,
          -5.0938e+00, -5.0938e+00],
         [ 1.0688e+01,  1.0750e+01,  8.4375e+00,  ...,  3.1719e+00,
           3.1719e+00,  3.1719e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 2.1250e+00,  3.3281e+00,  1.5312e+00,  ..., -6.2188e+00,
          -6.2188e+00, -6.2188e+00],
         [-1.3281e+00, -7.1716e-03, -5.6250e-01,  ..., -5.1562e+00,
          -5.1562e+00, -5.1562e+00],
         [-1.8672e+00, -3.3281e+00,  3.2812e+00,  ..., -5.7812e+00,
          -5.7812e+00, -5.7812e+00]],

        ...,

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 8.8672e-01,  2.5625e+00,  3.6914e-01,  ..., -7.4062e+00,
          -7.4062e+00, -7.4062e+00],
         [ 3.2188e+00,  2.6250e+00,  5.9375e-01,  ..., -4.6875e+00,
          -4.6875e+00, -4.6875e+00],
         [ 1.9062e+00,  2.1562e+00,  1.7656e+00,  ..., -6.5938e+00,
          -6.5938e+00, -6.5938e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [-1.2969e+00,  2.0938e+00, -3.0664e-01,  ...,  1.0938e+00,
           1.0938e+00,  1.0938e+00],
         [-3.6523e-01,  1.0625e+00, -3.3906e+00,  ..., -2.9062e+00,
          -2.9062e+00, -2.9062e+00],
         [ 3.2344e+00, -1.3125e+00,  7.3438e-01,  ..., -3.5156e+00,
          -3.5156e+00, -3.5156e+00]],

        [[ 3.8281e+00,  3.9219e+00,  1.5234e+00,  ..., -3.1875e+00,
          -3.1875e+00, -3.1875e+00],
         [ 1.7109e+00,  2.4375e+00,  3.3750e+00,  ..., -6.1875e+00,
          -6.1875e+00, -6.1875e+00],
         [ 5.6562e+00,  6.6562e+00,  3.0312e+00,  ..., -4.0938e+00,
          -4.0938e+00, -4.0938e+00],
         ...,
         [ 5.9688e+00,  7.6562e+00,  6.8750e+00,  ..., -4.7070e-01,
          -4.7070e-01, -4.7070e-01],
         [ 5.3438e+00,  1.4766e+00,  2.3125e+00,  ..., -2.8750e+00,
          -2.8750e+00, -2.8750e+00],
         [ 7.9688e-01,  1.4453e+00, -7.7344e-01,  ..., -6.9531e-01,
          -6.9531e-01, -6.9531e-01]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:56:59] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ..., 126551,  47985, 126254],
        [ 12802,  53435,  40853,  ...,     60,    508,  49863],
        [ 12802,  53435,  40853,  ..., 136115,     11,    364],
        ...,
        [ 12802,  53435,  40853,  ..., 130092,  36055,  63089],
        [ 12802,  53435,  40853,  ...,  34143,    105,  52959],
        [ 12802,  53435,  40853,  ...,  49863,     60,  63332]],
       device='cuda:0')
[2025-05-09 11:57:00] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [11.1250, 11.5625,  7.4062,  ...,  0.4590,  0.4590,  0.4590],
         [ 6.5625,  6.1562,  2.8125,  ...,  1.0859,  1.0859,  1.0859],
         [ 8.5625,  9.4375,  7.3750,  ...,  1.2734,  1.2734,  1.2734]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 6.9688,  7.4062,  4.4062,  ..., -0.7852, -0.7852, -0.7852],
         [ 6.8438,  7.9375,  5.3750,  ..., -0.0742, -0.0742, -0.0742],
         [ 3.5469,  6.3750,  3.3281,  ..., -0.2910, -0.2910, -0.2910]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 6.9062,  5.2812,  1.1484,  ..., -1.7266, -1.7266, -1.7266],
         [11.8125,  8.5625,  4.7188,  ...,  1.2578,  1.2578,  1.2578],
         [ 4.5000,  4.2188, -0.0396,  ..., -2.4219, -2.4219, -2.4219]],

        ...,

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 2.9219,  3.3906,  1.6250,  ..., -6.0312, -6.0312, -6.0312],
         [-1.4453, -1.2969, -3.5000,  ..., -6.5625, -6.5625, -6.5625],
         [ 0.4609,  2.7500, -0.5312,  ..., -7.6875, -7.6875, -7.6875]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 3.0938,  1.5781,  0.9609,  ...,  0.7773,  0.7773,  0.7773],
         [ 4.7188,  5.4062,  2.2344,  ..., -1.4922, -1.4922, -1.4922],
         [ 5.0938,  4.7500,  2.7656,  ..., -0.5703, -0.5703, -0.5703]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [10.1250,  9.0625,  5.3750,  ...,  0.6133,  0.6172,  0.6172],
         [ 2.1094,  1.1172, -0.3457,  ..., -2.2188, -2.2031, -2.2031],
         [12.4375,  9.5000,  5.6562,  ...,  4.1250,  4.1250,  4.1250]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:57:00] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,    367,   1667,    362],
        [ 12802,  53435,  40853,  ...,  10895,   5048,    518],
        [ 12802,  53435,  40853,  ...,  16052,  29803,    892],
        ...,
        [ 12802,  53435,  40853,  ..., 138373,  53680,  20401],
        [ 12802,  53435,  40853,  ...,     11,    892,    572],
        [ 12802,  53435,  40853,  ...,  46065,     13,   4354]],
       device='cuda:0')
[2025-05-09 11:57:01] [INFO] [nan 감지] loss: nan, logits: tensor([[[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 5.5625,  5.5938,  3.6250,  ..., -1.6562, -1.6562, -1.6562],
         [ 4.7500,  4.5312, -0.9414,  ..., -1.8047, -1.8047, -1.8047],
         [ 4.1562,  2.9375, -0.3379,  ..., -3.0156, -3.0156, -3.0156]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 7.9688,  6.5312,  1.3359,  ..., -4.6562, -4.6562, -4.6562],
         [ 4.8125,  2.4375, -3.6094,  ..., -6.4688, -6.4688, -6.4375],
         [-2.9531,  3.8125, -0.5898,  ..., -1.1406, -1.1406, -1.1406]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 7.9688,  7.5625,  3.8281,  ...,  0.1768,  0.1768,  0.1768],
         [11.1250, 12.5000,  6.6562,  ...,  1.4766,  1.4766,  1.4766],
         [ 2.6562,  0.2539,  0.6211,  ..., -0.8945, -0.8945, -0.8945]],

        ...,

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 9.7500,  5.7500,  3.6250,  ..., -4.0312, -4.0312, -4.0312],
         [11.6875, 10.5000,  7.4688,  ...,  2.1250,  2.1250,  2.1250],
         [ 5.5312,  4.6875,  1.9609,  ..., -0.7539, -0.7539, -0.7539]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 3.1562,  2.9844,  2.0625,  ..., -4.6562, -4.6562, -4.6562],
         [ 3.3750,  1.9062,  0.1182,  ..., -4.9062, -4.9062, -4.9062],
         [ 0.0154, -1.6016,  3.7969,  ..., -1.1016, -1.1016, -1.1016]],

        [[ 3.8281,  3.9219,  1.5234,  ..., -3.1875, -3.1875, -3.1875],
         [ 1.7109,  2.4375,  3.3750,  ..., -6.1875, -6.1875, -6.1875],
         [ 5.6562,  6.6562,  3.0312,  ..., -4.0938, -4.0938, -4.0938],
         ...,
         [ 3.2188, -0.8672,  0.1118,  ..., -3.1094, -3.1094, -3.1094],
         [11.6875,  7.5625,  3.1719,  ..., -4.9688, -4.9688, -4.9688],
         [ 3.4688, -0.3828,  2.1875,  ..., -4.5938, -4.5938, -4.5938]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)
[2025-05-09 11:57:01] [INFO] [nan 감지] input_ids: tensor([[ 12802,  53435,  40853,  ...,    432,    702,   1012],
        [ 12802,  53435,  40853,  ...,  66845, 138669,  22042],
        [ 12802,  53435,  40853,  ...,  40194,   7822,     13],
        ...,
        [ 12802,  53435,  40853,  ...,     15,  16375,    323],
        [ 12802,  53435,  40853,  ...,  19391, 130869,   3315],
        [ 12802,  53435,  40853,  ...,     60, 128900,    508]],
       device='cuda:0')
================================================================================
[2025-05-09 11:57:32] [INFO] 로그 파일 열림: log.txt
================================================================================
================================================================================
[2025-05-09 11:57:32] [INFO] K-Fold 분할 시작
================================================================================
[2025-05-09 11:57:32] [INFO] 데이터 로드 중...
[2025-05-09 11:57:32] [INFO] 총 3000개 샘플 로드 완료
[2025-05-09 11:57:32] [INFO] 데이터 검증 결과:
[2025-05-09 11:57:32] [INFO] - 총 행 수: 3000
[2025-05-09 11:57:32] [INFO] - NULL 텍스트: 0 (0.00%)
[2025-05-09 11:57:32] [INFO] - NULL 라벨: 0 (0.00%)
[2025-05-09 11:57:32] [INFO] - 중복 행: 0 (0.00%)
[2025-05-09 11:57:33] [INFO] - 텍스트 정제를 적용했습니다.
[2025-05-09 11:57:33] [INFO] KFold 비활성화 상태 - 전체 데이터 저장
================================================================================
[2025-05-09 11:57:33] [INFO] train_folds.csv 저장 완료: data/train_folds.csv
================================================================================
