---

## ⚙️ config.yaml

# 경로
data_dir: data  # 상대 경로 사용
checkpoint_dir: checkpoint
result_csv: submission.csv  # 상대 경로로 변경

# 공통
seed: 42
max_length: 256  # nan 방지: 너무 길지 않게
batch_size:
  mistral: 1
  mistral_infer: 16 # Mistral/Qwen 추론용 배치 크기
  deberta: 1
  deberta_infer: 16 # DeBERTa 추론용 배치 크기

# 진행 상황 및 로깅
logging:
  save_log: true
  log_file: log.txt
  verbose: true
  show_progress: true

# 데이터 처리
data_preprocessing:
  handle_null: true
  clean_text: true
  remove_duplicates: false

# Mistral‑7B QLoRA
mistral:
  model_id: Qwen/Qwen3-4B
  lora_r: 32
  lora_alpha: 64
  lora_dropout: 0.05
  lr: 1.0e-4   # nan 방지: 더 낮은 학습률
  epochs: 3
  quant:
    load_in_4bit: true
    quant_type: nf4
    double_quant: false

# DeBERTa‑v3‑Large LoRA
deberta:
  model_id: microsoft/deberta-v3-large
  lora_r: 8
  lora_alpha: 16
  lr: 2.0e-5
  epochs: 3
  # DeBERTa LoRA 학습 시 타겟 모듈 (모델 구조 확인 후 정확히 명시)
  lora_target_modules: ["query_proj", "key_proj", "value_proj", "intermediate.dense", "output.dense", "attention.output.dense"] # 예시 값입니다. 실제 모델에 맞게 수정하세요.

# TF‑IDF / LightGBM
tfidf:
  ngram: [1, 5]
  max_features: 150000
lgbm:
  n_estimators: 500
  learning_rate: 0.05

# Noise augmentation
augment:
  enable: false   # nan 방지: 증강 비활성화
  p_noise: 0.3
  mask_ratio: 0.15
  swapcase_ratio: 0.10

# KFOLD & Optuna
kfold:
  enable: false
  n_splits: 5
optuna:
  enable: false
  trials: 20

# (선택) 외부 서비스/민감 정보 관리 예시
# 실제 키는 환경변수(예: HUGGINGFACE_TOKEN, WANDB_API_KEY)로 관리하고,
# 아래는 참고용/로컬 테스트용으로만 사용하세요.
external:
  huggingface_token: ${HUGGINGFACE_TOKEN}  # 또는 직접 입력하지 말고 환경변수로 관리
  wandb_api_key: ${WANDB_API_KEY}
